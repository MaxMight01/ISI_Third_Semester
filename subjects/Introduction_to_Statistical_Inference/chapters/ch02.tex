\chapter{POINT ESTIMATION}
\textit{August 8th.}

We begin with a definition.

\begin{definition}
    A \eax{point estimator} is a function $W$ of the sample, mapping into the parameter space $\Theta$ of the parameter $\theta$ of interest. The value $W(X)$ is called a \eax{point estimate} of $\theta$.
\end{definition}

Various methods of point estimation are discussed in this chapter, including the method of moments, maximum likelihood estimation, and Bayes estimation.

\section{Estimators}
\subsection{Method of Moments}
Let $X_{1},\ldots,X_{n}$ be a sample from a population with a probability distribution function and probability density function. The \eax{method of moments} estimators are found by equating the first $k$ sample moments to the corresponding $k$ population moments and solving the resulting system of simultaneous equations. Here, the $k^{\text{th}}$ sample moment and the $k^{\text{th}}$ population moment are given as
\begin{align}
    m_{k} = \frac{1}{n} \sum_{j=1}^{n} X_{j}^{k} \qquad \text{and} \qquad \mu_{k}' = E[X^{k}]
\end{align}
respectively. For $\mu_{j}' = \mu_{j}'(\theta_{1},\ldots,\theta_{i})$, with $1 \leq j \leq k$, the estimators are obtained by solving the equations
\begin{align}
    m_{j} = \mu_{j}'(\theta_{1},\ldots,\theta_{i}) \qquad \text{for } 1 \leq j \leq k.
\end{align}

\subsection{Maximum Likelihood Estimators}

Let $X_{1},\ldots,X_{n}$ be an independent and identically distributed sample from a population with a probability distribution/mass function $f(\underline{x} \mid \theta_{1},\ldots,\theta_{k})$. The \eax{likelihood function} is defin as
\begin{align}
    L(\underline{\theta} \mid \underline{x}) = L(\theta_{1},\ldots,\theta_{k} \mid x_{1},\ldots,x_{n}) \defeq \prod_{j=1}^{n} f(x_{j} \mid \theta_{1},\ldots,\theta_{k}).
\end{align}

\begin{definition}
    For each sample points, let $\overline{\theta}(\underline{x})$ be a parameter value at which $L(\underline{\theta} \mid \underline{x})$ attains its maxima as a function of $\underline{\theta}$, with $\underline{x}$ held fixed. A \eax{maximum likelihood estimator} (MLE) of the parameter $\theta$ based on a sample $\underline{X}$ is $\hat{\theta}(\underline{X})$.
\end{definition}

\begin{remark}
    \begin{enumerate}
        \item By its contraction, the ranges of the MLE concludes with the range of the parameter.
        \item The MLE is the parameter points for which the observation sample is most likely.
    \end{enumerate}
\end{remark}

If the likelihood function is differentiable in $\theta_{i}$, then possible candidates for the MLE are the values of $(\theta_{1},\ldots,\theta_{k})$ that solve $\frac{\partial}{\partial \theta_{i}}L(\underline{\theta} \mid \underline{x}) = 0$ for $1 \leq i \leq k$. Moreover, we must have $\frac{\partial^{2}}{\partial^{2}\theta_{i}}L(\underline{\theta} \mid \underline{x}) < 0$ for the solution to be a maximum.

\begin{remark}
    \begin{enumerate}
        \item The solutions to the above equation are the only possible candidates for the MLE since the first derivative being zero is only a necessary condition for the maximum and not sufficient.
        \item The zeroes of the first derivatives only locate extreme points in the interior of the domain of the function.
        \item If the extrema occurs on te boundary of the domain, then the first derivative may not be zero. Thus the boundary must be checked separately.
        \item The points where the first derivatives are zero may be local/global maxima or minima. 
    \end{enumerate}
\end{remark}


\begin{example}
    Let us take the example of the binomial distribution, $X \sim \Bin (n,p)$. The likelihood function is given by $L(p \mid x) = \binom{n}{x} p^{x} (1-p)^{n-x}$, where $0 < p < 1$. We have
    \begin{align}
        \frac{\partial}{\partial p} L(p \mid x) = \frac{dL}{dp}(p \mid x) = 0.
    \end{align}
    This derivative is hard to compute directly. So we take the logarithm (an increasing function) of the likelihood function, and then differentiate.
    \begin{align}
        \implies \frac{d}{dp} \log L(p \mid x) = \frac{x}{p} - \frac{n-x}{1-p} = 0.
    \end{align}
    This gives us $p = \frac{x}{n}$, with the second derivative less than zero, confirming that this is a maximum. Thus, the MLE of $p$ is $\hat{p} = \frac{X}{n}$.
\end{example}

The method in this example is known as \eax{log likelihood estimation}. It is often easier to work with the log likelihood function, especially when dealing with products.

\begin{remark}
    \begin{enumerate}
        \item The MLE may not exist at all or may not be unique.
        \item If $\hat{\theta}$ is the MLE of $\theta$, then $\rho(\hat{\theta})$ is the MLE of $\rho(\theta)$ for any function $\rho$.
    \end{enumerate}
\end{remark}

The following is a vital and important result.
\begin{theorem}
    The MLE depends on $\underline{x}$ only through the sufficient statistic $T(\underline{x})$.
\end{theorem}
\begin{proof}
    We have $L(\theta \mid \underline{x}) = f(\underline{x} \mid \theta) = g(T(\underline{x}),\theta) h(\underline{x})$. Therefore, we have
    \begin{align}
        L(\hat{\theta}(\underline{x}) \mid \underline{x}) = \max_{\theta} g(T(\underline{x}),\theta) h(\underline{x}).
    \end{align}
    Since $h(\underline{x}) > 0$, and does not depend on $\theta$, we must have
    \begin{align}
        L(\hat{\theta}(\underline{x}) \mid \underline{x}) = h(\underline{x}) \max_{\theta} g(T(\underline{x}),\theta)
    \end{align}
    where the maximization is on the part that involves $\underline{x}$ through $T(\underline{x})$ only.
\end{proof}

\subsection{One Parameter Exponential Family in Natural Form}
Recall that the usual density form of the exponential family was given as
\begin{align}
    f(x \mid \theta) = \exp(c(\theta)T(x) + d(\theta) + s(x))\1_{A}(x).
\end{align}
Define $\eta = c(\theta)$ for $\theta \in \Theta$, and let $\Gamma = \{\eta \mid \eta = c(\theta), \theta \in \Theta\}$. We then have
\begin{align}
    f^{\ast}(x \mid \eta) = \exp(\eta T(x) + d_{0}(\eta) + s(x))\1_{A}(x)
\end{align}
where $d_{0}(\eta) = d(c^{-1}(\eta))$ if $c$ is one-one. Moreover,
\begin{align}
    1 &= \int_{A}f^{\ast}(x \mid \eta)dx = \int_{A} \exp(\eta T(x) + d_{0}(\eta) + s(x))dx = \exp(d_{0}(\eta)) \int_{A} \exp(\eta T(x)+s(x))dx \\
    \implies d_{0}(\eta) &= -\log\left( \int_{A} \exp(\eta T(x) + s(x))dx\right). 
\end{align}

\begin{theorem}
    If $X$ has density of the form $f(x \mid \eta) = \exp(\eta T(x) + d_{0}(\eta) + s(x))\1_{A}(x)$ and $\eta$ is an interior points of $H \defeq \{\eta \mid \abs{d_{0}(\eta)} < \infty\}$, then the moment generating function of $T(X)$ exists and is given by
    \begin{align}
        \varphi(s) = E[\exp(sT(X))].
    \end{align}
    Also,
    \begin{align}
        E[T(X)] = -\frac{d}{d\eta} d_{0}(\eta), \qquad \text{ and } \qquad \Var(T(X)) = -\frac{d^{2}}{d\eta^{2}} d_{0}(\eta).
    \end{align}
\end{theorem}

\begin{theorem}
    Let $\{P_{\theta} \mid \theta \in \Theta\}$ be a one parameter exponential family with density $f(x \mid \theta) = \exp(c(\theta)T(x) + d(\theta) + s(x))\1_{A}(x)$ and let $c$ be the interior of $C = \{c(\theta) \mid \theta \in \Theta\}$. Also suppose $\theta \mapsto c(\theta)$ is one-one. If the equation
    \begin{align}
        E_{\theta}[T(X)] = T(x)
    \end{align}
    has a solution $\hat{\theta}(x)$ for which $c(\hat{\theta}(x)) \in C$, then $\hat{\theta}(x)$ is the unique MLE of $\theta$.
\end{theorem}
\begin{proof}
    Since $\theta \mapsto c(\theta)$ is one-one, maximizing the likelihood over $\theta$ is maximizing over $\eta = c(\theta)$. Hence, consider the natural parametrization
    \begin{align}
        f(x \mid \eta) = \exp(\eta T(x) + d_{0}(\eta) + s(x))\1_{A}(x) \text{ for } \eta \in H.
    \end{align}
    $L(\eta \mid x) = \eta T(x) + d_{0}(\eta) + s(x)$, if $x \in A$, is the log likelihood function. Also,
    \begin{align}
        \frac{\partial}{\partial \eta} L(\eta \mid x) = T(x) + d_{0}'(\eta) \text{ and } \frac{\partial^{2}}{\partial \eta^{2}} L(n \mid x) = d_{0}''(\eta).
    \end{align}
    Therefore, we get $\frac{\partial}{\partial \eta} L(\eta \mid x) = T(x) - E_{\eta}[T(x)] = 0$ implying that $E_{\eta}[T(X)] = T(x)$. Now, $\frac{\partial^{2}}{\partial \eta^{2}} L(\eta \mid x) < 0$ so that $L$ is strictly concave. Then we get a unique maxima at $\hat{\eta}(x)$ for which $E_{\eta}[T(X)]_{\eta = \hat{\eta}(x)} = T(x)$.
\end{proof}

\noindent \textit{August 22nd.}

The \eax{loss function} $L(q(\theta),T(X))$ measures the discrepancy between the true parameter $q(\theta)$ and the estimate $T(X)$. A common choice for $L$ is the squared error loss, defined as
\begin{align}
    L(q(\theta),T(X)) = (q(\theta) - T(X))^{2}.
\end{align}
One may also choose the absolute error loss, defined as
\begin{align}
    L(q(\theta),T(X)) = \abs{q(\theta) - T(X)}.
\end{align}


\begin{theorem}[The \eax{Rao-Blackwell theorem}]
    Let $\underline{X}$ be a random vector with distribution $P_{\theta}$, $\theta \in \Theta$, and let $T$ be sufficient for $\theta$. Moreover, let $\delta(\underline{X})$ be an estimator of $\theta$ and $\delta^{\ast}(t) = E[\delta(X) \mid T = t]$. Also let $L(\theta,d)$ be a strictly convex loss function (in $d$) and $R(\theta,d) = E[L(\theta,d(\underline{X}))]$. Then if $R(\theta,\delta) = E[L(\theta,\delta(X))] < \infty$, we obtain $R(\theta,\delta^{\ast}) < R(\theta,\delta)$ for all $\theta$ unless $\delta(x) = \delta^{\ast}(T(x))$ with probability 1.
\end{theorem}

Another form is

\begin{theorem}
    If $T$ is an unbiased estimate of $\tau(\theta)$ and $S$ is a sufficient statistic, then $T' = E_{\theta}[T \mid S]$ is also unbiased for $\tau(\theta)$ and
    \begin{align}
        \Var_{\theta}(T') \leq \Var_{\theta}(T) \text{ for all } \theta.
    \end{align}
\end{theorem}

\begin{proof}
    We simply have $E_{\theta}[T'] = E_{\theta}[E[T \mid S]] = \tau(\theta) = E_{\theta}[T] = \tau(\theta)$, where we have used the property $E[E[X \mid Y]] = E[X]$. For the next part, we have
    \begin{align}
        \Var_{\theta}(T) &= E[T-E[T]]^{2} = E[T-\tau(\theta)]^{2} = E[T-T'+T'-\tau(\theta)]^{2} \notag \\ &= E[T-T']^{2} + E[T'-\tau(\theta)]^{2} + 2E[(T-T')(T'-\tau(\theta))].
    \end{align}
    The last term can be worked upon as
    \begin{align}
        E[(T-T')(T'-\tau(\theta))] = E[E[(T-T')(T'-\tau(\theta)) \mid S]] = E[(T-\tau(\theta))E[(T-T') \mid S]]
    \end{align}
    where we have used the fact that $E[AB \mid S] = AE[B \mid S]$ when $B$ is any random variable and $A$ is measurable with respect ot $S$. But note that the inner term, $E[(T-T') \mid S]$ expands as
    \begin{align}
        E[(T-T') \mid S] = E[T \mid S] - E[T' \mid S] = 0
    \end{align}
    giving us
    \begin{align}
        \Var_{\theta}(T) = E[T-T']^{2} + E[T'-\tau(\theta)]^{2} \geq E[T'-\tau(\theta)]^{2} = \Var_{\theta}(T').
    \end{align}
\end{proof}

\begin{theorem}
    Let $X$ have distribution $P_{\theta}$, $\theta \in \Theta$, and let $T = T(\underline{X})$ be complete and sufficient for $\theta$ (or $P_{\theta}$, $\theta \in \Theta)$. Then every function $h(T)$ is the unique unbiased estimator of its own expected value; that is, for any $h$, if $g(\theta) = E_{\theta}[h(T)]$ holds, then $h(T)$ is the only unbiased estimate available for $g(\theta)$.
\end{theorem}

\begin{proof}
    $T$ is complete for $\theta$, so for any function $h$, $E[h(T(X))] = 0$ for all $\theta$ implies that $h \equiv 0$. Let $h_{1}(T)$ and $h_{2}(T)$ be unbiased, so that $E[h_{1}(T)] = \theta = E[h_{2}(T)]$. Set $h(t) = h_{1}(t) - h_{2}(t)$. Then $E[h(T)] = 0$ for all $\theta$, which implies that $h \equiv 0$, or $h_{1}(t) = h_{2}(t)$.
\end{proof}

\begin{theorem}[The \eax{Lehmann-Scheffe theorem}]
    Suppose $T = T(\underline{X})$ is complete sufficient for $P_{\theta}$, $\theta \in \Theta$, and $S = S(\underline{X})$ is any unbiased estimate of $q(\theta)$. Then $S^{\ast} = E[S(\underline{X}) \mid T]$ is the unique minimum variance unbiased estimator (UMVUE) of $q(\theta)$ is $\Var_{\theta}(S^{\ast}(\underline{X})) < \infty$ for all $\theta$.
\end{theorem}

\begin{proof}
    Both $S$ and $S^{\ast}$ are unbiased, and the mean squared error is the variance. By Rao-Blackwell theorem, $\Var_{\theta}(S^{\ast}) \leq \Var_{\theta}(S)$ for all $\theta$. Let $S_{1}$ and $S_{2}$ be two such that $g_{1}(T) = E[S_{1} \mid T]$ and $g_{2}(T) = E[S_{2} \mid T]$ with $E[g_{1}(T)] = \theta = E[g_{2}(T)]$. Then $E[h(T)] = E[g_{1}(T)-g_{2}(T)] = 0$ showing $h \equiv 0$.
\end{proof}

\begin{remark}
    \begin{enumerate}
        \item Give any $S(X)$ unbiased for $q(\theta)$, the UMVUE is found by obtaining $S^{\ast}(X) = E[S(X) \mid T(X)]$, where $T$ is a complete sufficient statistic for $\theta$.
        \item If we already have $h(T)$ unbiased for $q(\theta)$ and $T$ complete sufficient, then $h(T)$ is the UMVUE for $q(\theta)$ since $S^{\ast} = E[h(T) \mid T] = h(T)$.
    \end{enumerate}
\end{remark}

We work with some problems to familiarize.

\begin{example}
    Let $X_{1},\ldots,X_{n} \sim N(\mu,\sigma^{2})$ be independent and identically distributed random variables. Setting $s^{2} = \frac{1}{n-1} \sum_{i} (X_{i}-\overline{X})^{2}$, one can show that $s^{2}$ is unbiased for $\sigma^{2}$.
\end{example}

\begin{example}
    For $X_{1},\ldots,X_{n} \sim N(\theta,\theta^{2})$, one can show that $(\sum X_{i}, \sum X_{i}^{2})$ is sufficient but not complete.
\end{example}

\begin{theorem}[\eax{Basu's theorem}]
    Suppose $T$ is complete sufficient for $\{P_{\theta} \mid \theta \in \Theta\}$. Let $S$ be any ancillary statistic. Then $T$ and $S$ are indpendent for all $\theta$.
\end{theorem}

\begin{proof}
    We have
    \begin{align}
        \int_{-\infty}^{\infty} [F_{S}(s) - F_{S \mid T = t}(s)]f_{T}(t) dt = 0 \text{ for all } \theta.
    \end{align}
\end{proof}

\begin{theorem}
    Let $\{P_{\theta} \mid \theta \in \Theta\}$ be a $k$-parameter exponential family with density
    \begin{align}
        f(\underline{x} \mid \theta) = \exp \left( \sum_{j=1}^{k} c_{j}(\theta) T_{j}(\underline{x}) + d(\theta) + S(\underline{x}) \right)\1_{A}(\underline{x}).
    \end{align}
    Suppose $\{\underline{c}(\theta) = (c_{1}(\theta),\ldots,c_{n}(\theta)) \mid \theta \in \Theta\}$ contains an open set $g(e)$ in $\R^{k}$. Then $\underline{T}(\underline{X}) = (T_{1},\ldots,T_{k})$ is complete sufficient.
\end{theorem}


\begin{theorem}
    A (bounded) complete sufficient statistic is minimal sufficient, assuming that the minimal sufficient statistic exists.
\end{theorem}

\begin{proof}
    Let $T$ be minimal sufficient and $U$ complete sufficient.Then $T = h(U)$ for some function $h$. We need to show that $T$ and $U$ are equivalent statistics (produce the same partition). It is enough t show that for all integrable $\varphi$, $E[\varphi(U) \mid T] = \varphi(U)$. Suppose $E[\varphi(U) \mid T] \neq \varphi(U)$ for some $\varphi$. Define $K(U) = \varphi(U) - E[\varphi(U) \mid h(U)]$. Then
    \begin{align}
        E[K(U)] = E[\varphi(U)] - E[E[\varphi(U) \mid h(U)]] = E[\varphi(U)] - E[\varphi(U)] = 0.
    \end{align}
    But $U$ is complete, so $K(U) \equiv 0$, or $\varphi(U) = E[\varphi(U) \mid h(U)]$ for all integrable $\varphi$. This shows that $T$ and $U$ are equivalent statistics.
\end{proof}


\section{Information Number}

Let $\{P_{\theta}:\theta \in \Theta\}$ be a family of probability distributions satisfying the following mathematical regularity conditions.
\begin{enumerate}
    \item[(A)] $A = \{x : f(x) > 0\}$ does not depend on $\theta$.
\end{enumerate}
For all $x \in A$, $\theta \in \Theta$, the \eax{score function}
\begin{align}
    S(x) = \frac{\partial}{\partial \theta} \log f(x \mid \theta) = \frac{\frac{\partial}{\partial \theta} f(x \mid \theta)}{f(x \mid \theta)}
\end{align}
exists and is finite. $S(x)$ measures the relative rate at which $f(x \mid \theta)$ changes at $x$. Since $X$ is random, this needs averaging as
\begin{align}
    I(\theta) = E_{\theta}[S(X)^{2}] = \int \left( \frac{\partial}{\partial \theta} \log f(x \mid \theta) \right)^{2} f(x \mid \theta) dx.
\end{align}
$I(\theta)$ is called the \eax{Fisher information number} about $\theta$ contained in the observation $X$. It measures the amount of information that the observable random variable $X$ carries about the unknown parameter $\theta$.
The second regularity condition is
\begin{enumerate}
    \item[(B)] The derivative, with respect to $\theta$, of $\int f(x \mid \theta) dx$ can be obtained by differentiating under the integral sign.
\end{enumerate}

\begin{theorem}
    If both (A) and (B) hold then
    \begin{enumerate}
        \item $E_{\theta} [S(X)] = 0$,
        \item $I(\theta) = \Var_{\theta}(S(X))$.
    \end{enumerate}
    In addition, if the second derivative (w.r.t.~$\theta$) of $\log f(x \mid \theta)$ for all $x$ and $\theta$, and the second derivative of $\int f(x \mid \theta) dx$ can be obtained by differentiating under the integral sign, then
    \begin{enumerate}
        \item[3.] $I(\theta) = -E_{\theta}[\frac{\partial^{2}}{\partial \theta^{2}} \log f(X \mid \theta)]$. 
    \end{enumerate}
\end{theorem}

\begin{theorem}
    Let $X,Y$ be two independent random variables with densities $f_{1}(x \mid \theta)$ and $f_{2}(y \mid \theta)$ respectively. If $I(\theta)$, $I_{1}(\theta)$, $I_{2}(\theta)$ are the Fisher information numbers of $(X,Y)$, $X$, $Y$ respectively, then $I(\theta) = I_{1}(\theta) + I_{2}(\theta)$.
\end{theorem}

The above theory works for single parameters; for multiple parameters, the \eax{fisher information matrix} is defined as
\begin{align}
    I(\theta) = [I_{ij}(\theta)] \text{ where } I_{ij}(\theta) = E \left[ \frac{\partial}{\partial \theta_{i}} \log f(x \mid \theta) \cdot \frac{\partial}{\partial \theta_{j}} \log f(x \mid \theta) \right].
\end{align}
The matrix is symmetric.

\textit{August 29th.}

\begin{theorem}
    Let $X$ have one parameter exponential family density
    \begin{align}
        f(x \mid \theta) = \exp(c(\theta)T(x) + d(\theta) + s(x))\1_{A}(x).
    \end{align}
    Consider the mean value parametrization $\delta(\theta) = E_{\theta}[T(X)]$. Then $I(\delta) = \frac{1}{\Var T}$.
\end{theorem}
\begin{proof}
    The natural parametrization $\eta = c(\theta)$ gives
    \begin{align}
        f^{\ast}(x \mid \eta) = \exp(\eta T(x) + d_{0}(\theta)+s(x))\1_{A}(x).
    \end{align}
    Log gives us
    \begin{align}
        \log f^{\ast}(x \mid \eta) = \eta T(x) + d_{0}(\theta)+s(x) \implies \frac{\partial}{\partial \eta} \log f^{\ast}(x \mid \eta) = T(x)+d_{0}'(\eta) = T(x)-E_{\eta}(T).
    \end{align}
    Here $E_{\eta}(T) = -d_{0}'(\eta)$ and $\Var_{\eta}(T) = -d_{0}''(\eta)$. Therefore, we have
    \begin{align}
        I^{\ast}(\eta) = E_{\eta} [\frac{\partial}{\partial \eta} \log f^{\ast}(X \mid \eta)] = E_{\eta}[T(X)-E_{\eta}(T)] = \Var_{\eta}(T).
    \end{align}
    Here $\delta(\theta) = E_{\theta}[T] = -d_{0}'(\eta) = h(\eta)$.
    \begin{align}
        \frac{d \eta}{d \delta} = \left( \frac{d \delta}{d \eta} \right)^{-1} = (-d_{0}''(\eta))^{-1} = \frac{1}{\Var_{\eta}(T)} \implies I(\delta) = I^{\ast}(\eta) \left( \frac{d \eta}{d \delta} \right)^{2}\big|_{\eta = h^{-1}(\delta)} = \frac{1}{\Var_{\eta}(T)}.
    \end{align}
\end{proof}

\subsection{Information Inequality}
Suppose the conditions (A) and (B) (above) hold, and $0 < I(\theta) < \infty$. Let $T(X)$ be any statistic with $\Var T < \infty$ and such that the derivative, with respect to $\theta$, of
\begin{align}
    E_{\theta}(T) = \int T(x)f(x \mid \theta) dx
\end{align}
exists and can be obtained by differentiating under the integral sign. Then
\begin{align}
    \Var_{\theta}(T(X)) \geq \frac{\frac{d}{d\theta}[E_{\theta}[T]]^{2}}{I(\theta)}.
\end{align}
This is known as the \eax{Cram\'er-Rao lower bound}. To see this inequality, note that
\begin{align}
    \frac{d}{d \theta} E_{\theta}[T] = \int T(x) \frac{d}{d\theta}f(x \mid \theta) dx = \int T(x) S(x) f(x \mid \theta) dx = E_{\theta}[T(X)S(X)].
\end{align}
$E[S(X)] = 0$, so the covariance $\Cov(T(X),S(X)) = E[T(X)S(X)] -E[T(X)]E[S(X)] = E[T(X)S(X)] \leq \sqrt{\Var(T(X)) \Var(S(X))}$. $\Var(S(X))$ is nothing but $I(\theta)$. The inequality follows.\\

For the class of all unbiased estimators of $\theta$ we have
\begin{align}
    \Var_{\theta}(T(X)) \geq \frac{\frac{d}{d\theta}[E_{\theta}[T]]^{2}}{I(\theta)} = \frac{1}{I(\theta)}.
\end{align}
This lower bound is independent of any particular $T$. If there exists an unbiased estimator that attains this lower bound, then that estimator has to be the UMVUE.

\begin{remark}
    Let $X_{1},\ldots,X_{n}$ be i.i.d.~from a location parameter family with probability distribution function $f_{X}(x \mid \theta) = f(x-\theta)$ with $\theta \in \R$. Then $R = X_{(n)}-X_{(1)}$ is ancillary.

    If they come from a scale parameter family with pdf $f_{X}(x \mid \theta) = \frac{1}{\theta}f\left(\frac{x}{\theta}\right)$, then $R = X_{(n)}-X_{(1)}$ is also ancillary.
\end{remark}