\chapter{HYPOTHESIS TESTING AND INTERVAL ESTIMATION}

\section{Hypothesis Testing}

Recall that given a null hypothesis $H_{0}$, then the \eax{significance level} $\alpha$ is the probability of a Type I error; that is,
\begin{align}
    \alpha = P(\text{reject } H_{0} \mid H_{0} \text{ is true}).
\end{align}
$\beta$ is defiend as the probability of a Type II error; that is,
\begin{align}
    \beta = P(\text{accept } H_{0} \mid H_{0} \text{ is false}).
\end{align}


We start with an example.

\begin{example}
    A pack of a certain brand of cigarettes displays the statement, `1.5 mg nicotine on average per cigarette'. Let $\mu$ be the actual nictone content per cigarette for this brand. It is required to test if the actual average is higher than what is claimed. Suppose a sample of cigarettes is selected and the nicotine content per cigarette is measured. Let $X_{1}, X_{2}, \ldots, X_{n}$ be the nicotine content of the $n$ cigarettes in the sample. We assume that $X_{1}, X_{2}, \ldots, X_{n}$ are i.i.d. $N(\mu, \sigma^{2})$. We consider the null hypothesis $H_{0}:\mu = 1.5$ and the alternate $H_{1}:\mu > 1.5$. In this case, we consider the test statistic
    \begin{align}
        T = \frac{\overline{X} - 1.5}{s/\sqrt{n}}
    \end{align}
    If $H_{0}$ is true, then $T \sim t_{n-1}$. We reject $H_{0}$ if $T > t_{n-1,\alpha}$, where $t_{n-1,\alpha}$ is the $100(1-\alpha)$ percentile of the $t$-distribution with $n-1$ degrees of freedom.
\end{example}

\subsection{Neyman Pearson Theory of Testing}

Let $X \sim P_{\theta}$, $\theta \in \Theta$. Let $\cX$ be the sample space of $X$, that is, $\cX = X(\Omega)$. We wish to test $H_{0}:\theta \in \Theta_{0}$ against $H_{1}:\theta \in \Theta_{1}$, where $\Theta_{0},\Theta_{1} \subseteq \Theta$ and $\Theta_{0} \cap \Theta_{1} = \emptyset$. A simple hypothesis is one where $\Theta_{0}$ is a singleton.

In a non randomized test, find a subset of $S$ of $\cX$ and reject $H_{0}$ of the observed value $x \in S$. $S \subset \cX$ is called the critical region or the rejection region. The test function $\phi$ for randomized tests is defined as
$\phi(x) = \1_{S}(x)$. For a level of $\alpha$ test, we require that
\begin{align}
    \sup_{\theta \in \Theta_{0}} P_{\theta}(X \in S) \leq \alpha.
\end{align}
If $\Theta_{0} = \{\theta_{0}\}$, then we simply require $P_{\theta_{0}}(X \in S) \leq \alpha$. For a randomized test, any $\phi$ with $0 \leq \phi(x) \leq 1$ for all $x \in \cX$ and at any $x$, $\phi(x)$ is the probability of rejecting $H_{0}$ when $X = x$. A non randomized test is a subset of a randomized test. The power function of the test is defined as
\begin{align}
    P_{\theta}(\text{reject} H_{0}) = E_{\theta}(P(\text{reject } H_{0} \mid X)) = E_{\theta}[\phi(X)] = \int_{\cX} \phi(x) f(x \mid \theta) dx.
\end{align}
The problem is to find $\phi$ such that $E_{\theta}[\phi(X)]$ is maximized when $\theta \in \Theta_{1}$ subject to the condition that $\sup_{\theta \in \Theta_{0}} E_{\theta}[\phi(X)] \leq \alpha$. Such a test, if it exists, is called the \eax{uniformly most powerful} (UMP) test of level $\alpha$.

\begin{lemma}
    \eax{Neyman-Pearson lemma}. Suppose $\Theta_{0} = \{\theta_{0}\}$ and $\Theta_{1} = \{\theta_{1}\}$. Let $P_{\theta_{1}}$ and $P_{\theta_{2}}$ be the respective densities. Then
    \begin{enumerate}
        \item there exists a test $\phi$ and a constant $\alpha \geq 0$ such that $E_{\theta}[\phi(X)] = \alpha$.
        \item If a test satisfies the above and below:
        \begin{align}
            \phi(x) = \begin{cases}
                1 &\text{if } P_{\theta_{1}}(x) > k P_{\theta_{0}}(x), \\
                0 &\text{if } P_{\theta_{1}}(x) < k P_{\theta_{0}}(x).
            \end{cases}
        \end{align}
        for some $k \geq 0$, then it is the most powerful for testing $H_{0}:P_{\theta} = P_{\theta_{0}}$ against $H_{1}:P_{\theta} = P_{\theta_{1}}$ at level $\alpha$.

        \item Conversely, if $\phi$ is a most powerful test at level $\alpha$ for testing, then it satisfies the above for some $k \geq 0$.
    \end{enumerate}
\end{lemma}
\begin{proof}
    Pg no.~80 of Mohan Delampady's notes.
\end{proof}

\subsection{Likelihood Ratio Test}

\begin{definition}
    $P_{\theta}$, $\theta \in \Theta \subseteq \R$ with density $f(x \mid \theta)$ is said to have \eax{monotone likelihood ratio} (MLR) if there exists a real valued function $T(x)$ such that for any $\theta < \theta'$, $P_{\theta} \neq P_{\theta'}$, and the likelihood ratio $\frac{f(x \mid \theta')}{f(x \mid \theta)}$ is a non decreasing function of $T(x)$, that is,
    \begin{align}
        \frac{f(x \mid \theta')}{f(x \mid \theta)} = h_{\theta,\theta'}(T(x)), \text{ where } h_{\theta,\theta'} \text{ is non decreasing}.
    \end{align}
\end{definition}

\textit{October 17th.}


\begin{theorem}
    Let $\theta$ be a real parameter and let $X$ have density $f(x \mid \theta)$ with MLR in $T(x)$. Then
    \begin{enumerate}
        \item for every $H_{0}:\theta \leq \theta_{0}$ and $H_{1}:\theta > \theta_{0}$, there exists a UMP test of level $\alpha$ given by
        \begin{align}
            \phi(x) = \begin{cases}
                1 &\text{ if } T(x) > c,\\
                \gamma &\text{ if } T(x) = c,\\
                0 &\text{ if } T(x) < c,
            \end{cases}
        \end{align}
        where $c$ and $\gamma$ are determined by $E_{\theta_{0}}[\phi(X)] = \alpha$.

        \item The power function $E_{\theta}[\phi(X)]$ of this test is strictly increasing for all $\theta$ satisfying $E_{\theta}[\phi(X)] < 1$.
        \item For all $\theta'$, the test given by part 1.~is the UMP for testing $H_{0}:\theta \leq \theta'$ and $H_{1}:\theta > \theta'$ at level $\alpha' = E_{\theta'}[\phi(X)]$.
        \item For any $\theta < \theta_{0}$, the test given by part 1.~minimizes $E_{\theta}(\phi(X))$ among all test satisfying $E_{\theta_{0}}[\phi(X)] = \alpha$.
    \end{enumerate}
\end{theorem}

\begin{example}
    We have $X_{1},\ldots,X_{n} \sim N(\mu,\sigma^{2})$, where $\sigma^{2}$ is known. We wish to test $H_{0}:\mu \leq \mu_{0}$ against $H_{1}:\mu > \mu_{0}$. With $T(X) = \overline{X}$, we have MLR. Thus, the UMP test of level $\alpha$ is given by
    \begin{align}
        \phi(X) = \begin{cases}
            1 &\text{ if } \overline{X} > c,\\
            \gamma &\text{ if } \overline{X} = c,\\
            0 &\text{ if } \overline{X} < c,
        \end{cases}
    \end{align}
    where $\gamma$ does not matter since $P(\overline{X} = c) = 0$. To find $c$, we require that $E_{\mu_{0}}[\phi(X)] = \alpha$ when $\mu = \mu_{0}$. Thus,
    \begin{align}
        \alpha = E_{\theta}[\phi(X)] = P(\overline{X} > c) = P\left(Z > \frac{c - \mu_{0}}{\sigma/\sqrt{n}}\right) = z_{1-\alpha}.
    \end{align}
    Thus, $c = \mu_{0} + z_{1-\alpha} \frac{\sigma}{\sqrt{n}}$. We now wish to test $H_{0}:\sigma^{2} \leq \sigma_{0}^{2}$ against $H_{1}:\sigma^{2} > \sigma_{0}^{2}$. With $T(X) = \sum_{i=1}^{n}(X_{i} - \mu_{0})^{2}$, we have MLR. Thus, the UMP test of level $\alpha$ is given by $\phi(X) = \1_{\{T(X) > c\}}$, where $c$ is determined by a chi-squared distribution.
\end{example}

\subsection{Generalized Likelihood Ratio Test}

Let $X \sim P_{\theta}$, $\theta \in \Theta$, have ddensity $f(x \mid \theta)$. Consider the test $H_{0}:\theta \in \Theta_{0}$ against $H_{1}:\theta \in \Theta_{1}$, where $\Theta_{0},\Theta_{1} \subseteq \Theta$ and $\Theta_{0} \cap \Theta_{1} = \emptyset$. The \eax{generalized likelihood ratio} (GLR) is defined as
\begin{align}
    L(x) = \frac{\sup_{\theta \in \Theta_{1}}f(x \mid \theta)}{\sup_{\theta \in \Theta_{0}}f(x \mid \theta)}.
\end{align}
Consider
\begin{align}
    \lambda(x) = \frac{\sup_{\theta \in \Theta}f(x \mid \theta)}{\sup_{\theta \in \Theta_{0}}f(x \mid \theta)}.
\end{align}
Note that $\lambda(x) = \max\{1, L(x)\}$. The GLR test rejects $H_{0}$ if $L(x) > k$ for some $k \geq 0$, or equivalently, if $\lambda(x) > c$ for some $c \geq 1$.

\begin{example}
    $X_{i} \sim N(\mu,\sigma^{2})$, with $\mu$ and $\sigma^{2}$ unknown. We wish to test $H_{0}:\mu = 0$ against $H_{1}:\mu \neq 0$. We have $\Theta_{0} = \{(0,\sigma^{2}):\sigma^{2} > 0\}$ and $\Theta_{1} = \{(\mu,\sigma^{2}):\mu \neq 0, \sigma^{2} > 0\}$. The MLE are needed to compute the GLR statistic. We have
    \begin{align}
        \lambda(\underline{x}) = \frac{f(x \mid \hat{\theta}_{1})}{f(x \mid \hat{\theta}_{0})} = \frac{(2\pi)^{-n/2} (-\frac{1}{n} \sum (x_{i}-\overline{x})^{2})^{-n/2} \exp(-\frac{1}{2\hat{\sigma_{1}}^{2}}(\sum (x_{i}-\overline{x}^{2})+n(\overline{x}-\hat{\mu}_{1}^{2})))}{(2\pi)^{-n/2} (-\frac{1}{n} \sum (x_{i}-\overline{x})^{2})^{-n/2} \exp(-\frac{1}{2\hat{\sigma_{0}^{2}}}(\sum (x_{i}-\overline{x})^{2}+n(\overline{x}-0)^{2}))}.
    \end{align}
\end{example}

\subsection{Confidence Set and Interval Estimation}

For a \eax{confidence set}, we want $S(X) \subseteq \Theta$ such that $P(\theta \in S(X)) \geq 1 - \alpha$ for all $\theta \in \Theta$. $S(X)$ is then said to be a $100(1-\alpha)\%$ confidence set for $\theta$. Here we test $H_{0}:\theta = \theta'$ against $H_{1}:\theta \neq \theta'$ for any $\theta' \in \Theta$. Let $A(\theta') \subseteq \cX$ be the acceptance region of level $\alpha$ test for this hypothesis. Define $S(X) = \{\theta' \in \Theta \mid X \in A(\theta')\}$.

\begin{example}
    Suppose $X_{1},\ldots,X_{n} \sim N(\mu,\sigma^{2})$, where $\sigma^{2}$ is known. $\overline{X}$ here is sufficient for $\mu$. We wish to find a $100(1-\alpha)\%$ confidence interval for $\mu$. For any $\mu' \in \R$, we test $H_{0}:\mu = \mu'$ against $H_{1}:\mu \neq \mu'$. The level $\alpha$ test has acceptance region $A(\mu') = \{x : |\overline{x} - \mu'| < z_{\alpha/2} \frac{\sigma}{\sqrt{n}}\}$. Thus the confidence set (interval in our case) is
    \begin{align}
        S(\overline{X}) = \left[ \overline{X} - z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}}, \overline{X} + z_{1-\alpha/2} \frac{\sigma}{\sqrt{n}} \right].
    \end{align}
\end{example}

Let $X \sim P_{\theta}$, $\theta \in \Theta$.

\begin{definition}
    An interval $(\underline{T}(x),\overline{T}(x))$, with $\underline{T}(x) < \overline{T}(x)$ for all $x \in \cX$, is said to be a \eax{confidence interval} for $q(\theta)$ if
    \begin{align}
        \inf_{\theta \in \Theta} P_{\theta}(\underline{T}(X) < q(\theta) < \overline{T}(X)) \geq 1 - \alpha.
    \end{align}
    A closed interval may also be termed so.
\end{definition}

To define confidence intervals, we use the concept of \eax{pivotal quantity}. It is simply a real valued function $T(\underline{X},\theta)$ such that the distribution of $T(\underline{X},\theta)$ does not depend on $\theta$. Suppose we choose two numbers $t_{1}$ and $t_{2}$ such that $P_{\theta}(t_{1} \leq T(\underline{x},\theta) \leq t_{2}) = 1-\alpha$. If, for each $\underline{x}$, $T(\underline{x},\theta)$ is monotone in $\theta$, then we can solve the inequalities $t_{1} \leq T(\underline{x},\theta) \leq t_{2}$ for $\theta$ to get the confidence interval.


For example, if $X_{1},\ldots,X_{n} \sim N(\mu,\sigma^{2})$, with both parameters unknown, then $T(\underline{X}) = \frac{\sum (X_{i}-\overline{X})^{2}}{\sigma^{2}} \sim \chi^{2}_{n-1}$ is a pivotal quantity for $\sigma^{2}$. Choosing $t_{1}$ and $t_{2}$ such that $P(t_{1} \leq \chi^{2}_{n-1} \leq t_{2}) = 1-\alpha$, we have as desired.


\section{Bayesian Inference}
\textit{October 24th.}


Model: $X \mid \theta$ has density $f(x \mid \theta)$, $\theta \in \Theta$. The \eax{prior}: $\theta$ has density $\pi(\theta)$. The \eax{posterior}: the density is
\begin{align}
    \pi(\theta \mid \underline{x}) = \frac{f(\underline{x} \mid \theta) \pi(\theta)}{m(\underline{x})}
\end{align}
where $m(\underline{x}) = \int_{\Theta} f(\underline{x} \mid \theta) \pi(\theta) d\theta$ is the marginal density of $\underline{x}$. Given all the ingredients, the Bayesian calculates the conditional probability density of $\theta$ given $\underline{X} = \underline{x}$. The joint density of $x$ and $\theta$ is
\begin{align}
    h(x,\theta) = f(x \mid \theta) \pi(\theta).
\end{align}

\begin{theorem}
    Suppose $T = T(\underline{X})$ is sufficient for $\theta$. Then the posterior distribution of $\theta$ given $X = x$ depends on $x$ only through $T(x)$.
\end{theorem}

We have $f(x \mid \theta) = g(T(x),\theta) h(x)$, thus for $T(x) = t$,
\begin{align}
    \pi(\theta \mid \underline{x}) = \frac{f(x \mid \theta)\pi(\theta)}{\int f(x \mid u)\pi(u) du} = \frac{g(T(x),\theta)\pi(\theta)}{\int g(T(x),u)\pi(u) du} = \frac{g(t,\theta)\pi(\theta)}{\int g(t,u)\pi(u) du}.
\end{align}

\begin{example}
    Consider an urn with $Np$ red and $N(1-p)$ black balls. $p$ is unknown but $N$ is a known large number. Balls are drawn at random one by one with replacement, and the selection is stopped after $n$ draws. For $i = 1,2,\ldots,n$, let $Y_{i} = 1$ if the $i$-th ball drawn is red, and $0$ otherwise. Then the $Y_{i}$ are i.i.d.~$\Ber(p)$. The likelihood function is $p^{\sum y_{i}} (1-p)^{n - \sum y_{i}}$. Now $X = \sum Y_{i}$ is sufficient for $p$. Suppose the prior distribution of $p$ is $\mathrm{Beta}(\alpha,\beta)$, with density
    \begin{align}
        \pi(p) = \frac{1}{B(\alpha,\beta)} p^{\alpha - 1} (1-p)^{\beta - 1}, \quad 0 < p < 1.
    \end{align}
    Here, $h(x \mid \theta) = f(x \mid \theta) \pi(\theta)$
    \begin{align}
        = \binom{n}{x} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} p^{x + \alpha - 1} (1-p)^{n - x + \beta - 1}.
    \end{align}
    Integration gives
    \begin{align}
        \int_{0}^{1} h(x \mid p) dp = \binom{n}{x} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)} \int_{0}^{1} p^{x + \alpha - 1} (1-p)^{n - x + \beta - 1} dp = \binom{n}{x} \frac{\Gamma(\alpha + \beta)}{\Gamma(\alpha)\Gamma(\beta)}\frac{\Gamma(\alpha + x) \Gamma(\beta + n - x)}{\Gamma(\alpha + \beta + n)}.
    \end{align}
    Thus the posterior density is
    \begin{align}
        \pi(p \mid x) = \frac{h(x \mid p)}{\int_{0}^{1} h(x \mid p) dp} = \frac{\Gamma(\alpha + \beta + n)}{\Gamma(\alpha + x) \Gamma(\beta + n - x)} p^{x + \alpha - 1} (1-p)^{n - x + \beta - 1}, \quad 0 < p < 1.
    \end{align}
    Thus, $p \mid X = x \sim \mathrm{Beta}(\alpha + x, \beta + n - x)$.
\end{example}


The HPD, \eax{highest posterior density estimate} $\hat{p}_{\text{hpd}}$ is the value of $p$ that maximizes the posterior density $\pi(p \mid x)$. In the example above, we have
\begin{align}
    \hat{p}_{\text{hpd}} = \frac{\alpha + x - 1}{\alpha + \beta + n - 2}.
\end{align}
The $\hat{p}_{\text{hpd}}$ is most probably the correct model, while the MLE is the parameter which most likely produced the data.


\begin{definition}
    Let $\cF$ denote a class of density functions $f(x \mid \theta)$. A class $\cP$ of prior distributions is said to be conjugate for $\cF$ if $\pi( \cdot \mid x) \in \cP$ for all $f \in \cF$ and $\pi \in \cP$.
\end{definition}

We discsuss \eax{Jeffreys' prior}; let $f(x \mid \theta)$ be the model of $X \mid \theta$ for which $I(\theta)$ is the Fisher information. Then the Jeffreys' prior is defined as
\begin{align}
    \pi(\theta) = (I(\theta))^{1/2} = \abs{I(\theta)}^{1/2}.
\end{align}

\begin{definition}
    For $0 < \alpha < 1$, a $100(1-\alpha)\%$ \eax{credible set} for $\theta$ is a subset $C \subseteq \Theta$ such that
    \begin{align}
        P(C \mid X = x) = 1-\alpha.
    \end{align}
    For the discrete case, the condition is relaxed to $P(C \mid X = x) \geq 1-\alpha$.
\end{definition}


\subsection{Prediction of Future Observations}

Suppose the data are $x_{1},\ldots,x_{n}$ when $X_{1},\ldots,X_{n}$ are i.i.d.~$f(x \mid \theta)$, for example, $N(\mu,\sigma^{2})$ with $\sigma^{2}$ known. We wish to predict the unobserved value of $X_{n+1}$.

Prediction by a single number $t(x_{1},\ldots,x_{n})$ amounts to considering prediction loss---
\begin{align}
    E[(X_{n+1}-t)^{2} \mid \underline{x}] = E[(X_{n+1}-E[X_{n+1} \mid \underline{x}])^{2} \mid \underline{x}] + (t-E[X_{n+1} \mid \underline{x}])^{2}
\end{align}
which is minimum at $t = E[X_{n+1} \mid \underline{x}]$. To calculate the predictor we need to calculate the predictive distribution.
\begin{align}
    \pi(x_{n+1} \mid \underline{x}) = \int_{\Theta} \pi(x_{n+1} \mid \underline{x},\theta) \pi(\theta \mid \underline{x}) d\theta = \int f(x_{n+1} \mid \theta) \pi(\theta \mid \underline{x}) d\theta.
\end{align}
Let $\mu(\theta) = \int_{-\infty}^{\infty} x f(x \mid \theta) dx$. It can be shown that
\begin{align}
    E[X_{n+1} \mid \underline{x}] = E[\mu(\theta) \mid \underline{x}] = \int_{\Theta} \mu(\theta) \pi(\theta \mid \underline{x}) d\theta.
\end{align}