\chapter{INTRODUCTION TO $\R^{n}$}

\section{Translation into Higher Dimensions}
\textit{July 21st.}

We begin with a definition.
\begin{definition}
    The space $\R^{n}$ is defined as $\R \times \R \times \cdots \times \R$ ($n$ times) = $\{(x_{1},x_{2},\ldots,x_{n}) : x_{i} \in \R\}$.
\end{definition}
In the context of analysis, we will talk about open sets, closed sets, sequences, compact sets, and connected sets. In contrast, algebra considers $\R^{n}$ as a vector space with the operators $+$ and $\cdot$. Combining both these aspects results in the study of analysis of several variables. In this course, we will mainly focus on dealing with functions of the form $f: \R^{n} \to \R^{m}$, and talk about their properties such as continuity, differentiability, and integrability.

\subsection{Algebraic and Analytic Structure}
We note that $\R^{n}$ is also an inner product space with the following properties:
\begin{itemize}
    \item $\ip{x,y} = \sum_{i=1}^{n} x_{i}y_{i}$ for all $x,y \in \R^{n}$.
    \item The set $\{e_{i}\}_{i=1}^{n}$ consisting of the unit vectors is an orthonormal basis for $\R^{n}$.
    \item The simplest maps from $\R^{n}$ to $\R^{m}$ are linear maps that send lines to lines.
\end{itemize}

\begin{example}
    Suppose the function $f$ is a linear map from $\R$ to $\R$. This implies that $f(x) = xf(1)$ for all $x \in \R$. Thus, $f(x) = cx$ for all $x \in \R$, where $c$ is a constant. Conversely, if $c \in \R$, then $x \mapsto cx$ is a linear map. Therefore, we conclude that $\{f: \R \to \R, \text{linear}\} \leftrightarrow \R$, with a possible bijection given by $f \mapsto f(1)$.
\end{example}
In the above example, we note that 1 is not special; we could simply fix any $\alpha \in \R\setminus\{0\}$, and notice that $f(x) = \frac{x}{\alpha} f(\alpha)$ for all $x \in \R$. Here, replacing $f(1)$ by $f(\alpha)$ is a kind of `change of variable'.
\begin{remark}
    Let $L:\R^{n} \to \R^{m}$ be a linear map. Then, $Le_{j} = \sum_{i=1}^{m} a_{ij} e_{i}$ for all $j = 1,2,\ldots,n$. We may write $L$ as $(a_{ij})_{m \times n} \in M_{m \times n}(\R)$, the set of all $m \times n$ matrices with real entries.
\end{remark}

Coming to the analysis side, there is a need for defining a distance between points in $\R^{n}$. Previously, we have seen that the \eax{norm} may be given as $\norm{x} = \sqrt{\sum_{i=1}^{n} x_{i}^{2}}$ for all $x \in \R^{n}$. We can use this norm to define our required distance function.

\begin{definition}
    The \eax{distance} function between two points $d:\R^{n} \times \R^{n} \to \R_{\geq 0}$ is defined as $d(x,y) = \norm{x-y} = \left( \sum_{i=1}^{n} (x_{i}-y_{i})^{2} \right)^{1/2}$ for all $x,y \in \R^{n}$.
\end{definition}
For $n = 1$, we note that $d(x,y) = \abs{x-y}$, from the previous analysis courses. $\R^{n}$ equipped with the function $d$ is called a \eax{metric space}. Coming to the properties of the inner product, we have
\begin{itemize}
    \item $\norm{x} = \ip{x,x}^{1/2}$ for all $x \in \R^{n}$.
    \item $\ip{x,y} = \ip{y,x}$ for all $x,y \in \R^{n}$.
    \item The function $\ip{,}$ is linear with respect to the first and second arguments.
\end{itemize}

We also have the important Cauchy-Schwarz inequality.
\begin{theorem}[\eax{Cauchy-Schwarz inequality}]
    For all $x,y \in \R^{n}$, we have $\abs{\ip{x,y}} \leq \norm{x} \norm{y}$.
\end{theorem}
\begin{proof}
    Note that 
    
    \begin{align}
        0 \leq \sum_{i=1}^{n} \sum_{j=1}^{n} (x_{i}y_{j}-x_{j}y_{i})^{2} &= 2\left(\sum_{i,j} x_{i}^{2}y_{j}^{2}- \sum_{i,j} x_{i}x_{j}y_{i}y_{j}\right) = 2 \left( \norm{x}^{2}\norm{y}^{2} - \ip{x,y}^{2} \right) \\ \implies \abs{\ip{x,y}} &\leq \norm{x}\norm{y}.
    \end{align}
\end{proof}
We note that equality occurs if and only if the first quantity in the above equation is zero, \textit{i.e.}, if and only if $x_{i}y_{j} = x_{j}y_{i}$ for all $i,j$, or $\frac{x_{i}}{y_{i}} = \frac{x_{j}}{y_{j}}$ for all $i,j$ showing that $x$ and $y$ are linearly dependent.
\begin{corollary}[\eax{Triangle inequality}]
    For all $x,y \in \R^{n}$, we have $\norm{x+y} \leq \norm{x} + \norm{y}$.
\end{corollary}
\begin{proof}
    We have
    \begin{align}
        \norm{x+y}^{2} = \ip{x+y,x+y} = \norm{x}^{2} + 2\ip{x,y} + \norm{y}^{2} \leq \norm{x}^{2} + 2\norm{x}\norm{y} + \norm{y}^{2} = (\norm{x}+\norm{y})^{2}
    \end{align}
    where the inequality follows from Cauchy-Schwarz.
\end{proof}
The following will prove to be an important result.
\begin{theorem}
    Let $L:\R^{n} \to \R^{m}$ be a linear map. Then, there exists a $M > 0$ such that $\norm{Lx} \leq M \norm{x}$ for all $x \in \R^{n}$.
\end{theorem}
\begin{proof}
    Rewriting $x$ as $x = \sum_{i=1}^{n} x_{i} e_{i}$, we have
    \begin{align}
        Lx &= \sum_{i=1}^{n} x_{i} Le_{i} \notag \\
        \implies \norm{Lx} &= \norm{\sum_{i=1}^{n} x_{i} Le_{i}} \leq \sum_{i=1}^{n} \abs{x_{i}} \norm{Le_{i}} \leq \norm{x} \sum_{i=1}^{n} \norm{Le_{i}} = \norm{x} M.
    \end{align}
    The first inequality follows from the triangle inequality, and the second from Cauchy-Schwarz. In the last step, $M$ is set to be $\sum_{i=1}^{n} \norm{Le_{i}}$, which is a constant.
\end{proof}
We also term $(\R^{n},d)$ as a Euclidean metric space. There is now a need to define open sets in $\R^{n}$ to talk more about the analysis of several variables.
\begin{definition}
    For $a \in \R^{n}$ and $r > 0$, the \eax{open ball} centred at $a$ of radius $r$ is $B_{r}(a) \defeq \{x \in \R^{n} : d(x,a) < r\}$, the set of all points in $\R^{n}$ that are at a distance less than $r$ from $a$.
\end{definition}
From the notion of open balls, we can define open sets.
\begin{definition}
    A set $S \subseteq \R^{n}$ is said to be an \eax{open set} if for all $x \in S$, there exists an $r > 0$ such that $B_{r}(x) \subseteq S$.
\end{definition}
We now bring the notion of convergence of sequences.
\begin{definition}
    Let $\{x_{m}\} \subseteq \R^{n}$ be a sequence and $x \in \R^{n}$. We say that $\{x_{m}\}$ \eax{converges} to $x$ if for every $\varepsilon > 0$, there exists a natural $N$ such that $\norm{x_{m}-x} < \varepsilon$ for all $m \geq N$.
\end{definition}

\textit{July 23rd.}

\begin{definition}
    Let $S \subseteq \R^{n}$ and $a \in \R^{n}$. We say that $a$ is a \eax{limit point} of $S$ if $S \cap (B_{r}(a)\setminus\{a\})$ is non-empty for all $r > 0$.
\end{definition}

We introduce more notation; for all $i = 1,2,\ldots,n$, the mapping $\Pi_{i}:\R^{n} \to \R$ is called the \eax{$i^{\text{th}}$ projection} where $\Pi_{i}(x) = x_{i}$. Note that $x = (x_{1},x_{2},\ldots,x_{n}) = (\Pi_{1}(x),\Pi_{2}(x),\ldots,\Pi_{n}(x))$. This notation allows us to formulate the following useful fact a little more neatly.

\begin{theorem}
    Let $\{x_{m}\} \subseteq \R^{n}$ be a sequence and $x \in \R^{n}$. Then $x_{m} \to x$ if and only if $\Pi_{i}(x_{m}) \to \Pi_{i}(x)$ for all $i = 1,2,\ldots,n$.
\end{theorem}
\begin{proof}
    Suppose $x_{m} \to x$. Then for all $\varepsilon > 0$, there exists a natural $N$ such that $\norm{x_{m}-x} < \varepsilon$ for all $N \geq N$. Restating, we have
    \begin{align}
        \sum_{i=1}^{n} (\Pi_{i}(x_{m})-\Pi_{i}(x))^{2} &< \varepsilon^{2} \text{ for all } n \geq N \\
        \implies \text{For all $i$, } \abs{\Pi_{i}(x_{m})-\Pi_{i}(x)} &< \varepsilon^{2} \text{ for all } n \geq N.
    \end{align}
    For the converse, we simply work backwards with $\varepsilon/\sqrt{n}$ as our choice of epsilon.
\end{proof}

For example, the sequence $\{(\frac{1}{n},\frac{1}{2n+3})\}_{n=1}^{\infty}$ converges to $(0,0)$. However, the sequence $\{(\frac{1}{n},n^{2})\}_{n=1}^{\infty}$ does not.

\begin{definition}
    Let $S \subseteq \R^{n}$. $a \in S$ is termed an \eax{interior point} of $S$ if for some $r > 0$, $B_{r}(a) \subseteq S$ holds. Thus, a set $S$ is open if $a$ is an interior point for all $a \in S$. The \eax{interior} of set $S$ is defined as $\intr S \defeq \{a \in S \mid a \text{ is an interior point.}\}$.

    If $a \in \intr(S^{c})$, then $a$ is termed an \eax{exterior point} of $S$. $a$ is termed a \eax{boundary point} if $B_{r}(a)$ meets both $S$ and $S^{c}$ for all $r > 0$. The set of \eax{boundary points} of $S$ is denoted as $\partial S$.
\end{definition}

We also term a set $S \subseteq \R^{n}$ as a \eax{closed set} if $\R^{n} \setminus S$ is open. The following facts will only be stated and will be left as an exercise to the reader:
\begin{itemize}
    \item A set $C \subseteq \R^{n}$ is closed if and only if for all sequences $\{x_{m}\}_{m=1}^{\infty} \subseteq C$ that converge to $x$ implies $x \in C$.
    \item The open ball $B_{r}(a)$ is an open set.
    \item The intersection of an arbitrary collection of closed sets is closed; likewise, the union of an arbitrary collection of open sets is open.
    \item The set $S \subseteq \R^{n}$ is open if and only if $S = \intr S$.
\end{itemize}
Fix $O \subseteq \R^{n}$.
\begin{itemize}
    \item $O$ is open if and only if $O \cap \partial O = \emptyset$.
    \item $O$ is closed if and only if $\partial O \subseteq O$.
\end{itemize}
For $S \subseteq \R^{n}$, we define the \eax{closure} of set $S$ as $\overline{S} = \intr S \cup \partial S$.
\begin{itemize}
    \item $S \subseteq \R^{n}$ is closed if and only if $\overline{S} = S$.
    \item Let $C_{i} \subseteq R$ be closed sets and $O_{i} \subseteq \R$ be open sets, for $i = 1,2,\ldots,n$. Then $C_{1} \times C_{2} \times \cdots \times C_{n} \subseteq \R^{n}$ is a closed set, and $O_{1} \times O_{2} \times \cdots \times O_{n} \subseteq \R^{n}$ is an open set.
    \item The \eax{$n$ dimensional unit sphere} $S^{n-1} \defeq \{x \in \R^{n} : \norm{x} = 1\}$ is closed in $\R^{n}$.
\end{itemize}

\begin{definition}
    For $S \subseteq \R^{n}$ and $a \in \R^{n}$, $a$ is termed an \eax{isolated point} if $a$ is \textit{not} a limit points; that there exists an $r > 0$ such that $S \cap (B_{r}(a) \setminus \{a\}) = \emptyset$.
\end{definition}

With the pesky definitions and translation of one dimensional concept into being defined over several variables, we come to limits and continuity.

\section{Limits and Continuity}
Recall that given $f:(a,b)\setminus\{c\} \to \R$, we say that $\lim_{x \to c}f(x) = b$ if for every $\varepsilon  0$, there exists a $\delta > 0$ such that $\abs{f(x)-b} < \varepsilon$ for all $x$ satisfying $0 < \abs{x-c} < \delta$. Note that in this definition of the limit, we have $f(x) \in B_{\varepsilon}(b)$ and $x \in B_{\delta}(c) \setminus \{c\}$; this can easily be rewritten as $f(B_{\delta}(c)\setminus\{c\}) \subseteq B_{\varepsilon}(b)$. However, for our definition we would not require $f$ to be defined on an open set. We defind it over any arbitrary set.

\begin{definition}
    Let $a \in S \subseteq \R^{n}$ be a limit point of $S$ and let $f:S\setminus\{a\} \to \R^{m}$ be a function and $b \in \R^{m}$. We say $\lim_{x \to a} f(x) = b$ if for every $\varepsilon > 0$, there exists a $\delta > 0$ such that $f((B_{\delta}(a)\setminus\{a\}) \cap S) \subseteq B_{\varepsilon}(b)$. Again, this is equivalent to saying that $\norm{f(x)-b} < \varepsilon$ for all $x \in S\setminus\{a\}$ satisfying $\norm{x-a} < \delta$.
\end{definition}

It is important to get accustomed to the definition that works with open balls.
\begin{remark}
    In the above definition, if we instead write $x - a = h$, then $\lim_{x \to a} f(x) = b$ is equivalent to saying that for every $\varepsilon > 0$, there exists a $\delta > 0$ such that $\norm{f(a+h)-b} < \varepsilon$ for all $\norm{h} < \delta$. We can further rewrite to get the usual notation of
    \begin{align}
        \lim_{\norm{h} \to 0} \norm{f(a+h)-b} = 0.
    \end{align}
    Note that the above limit is in the real numbers, making it eaiser to deal with.
\end{remark}

A notion of continuity also comes in handy.
\begin{definition}
    For $S \subseteq \R^{n}$, let $f:S \to R^{m}$ with $a \in S$. We say $f$ is \eax{continuous} at $a$ if $\lim_{x \to a} f(x) = f(a)$. In other words, for every $\varepsilon > 0$, there exists a $\delta > 0$ such that
    \begin{align}
        \norm{f(x)-f(a)} < \varepsilon \text{ for all } x  \in S \text{ satisfying } \norm{x-a} < \delta
    \end{align}
    or
    \begin{align}
        f(B_{\delta}(a) \cap S) \subseteq B_{\varepsilon}(f(a)).
    \end{align}
\end{definition}
Note that if $a$ is an isolated point of $S$, then any $f:S \to \R^{m}$ is continuous at $a$ since $f(\{a\}) \subseteq B_{\varepsilon}(f(a))$ holds true, trivially.

\begin{remark}
    Similar to the previous remark, $f$ is continuous at $a$ if and only if
    \begin{align}
        \lim_{\norm{h} \to 0} \norm{f(a+h)-f(a)} = 0.
    \end{align}
\end{remark}

Functions defined on $S \subseteq \R^{n}$ can be broken down into components; given $f :S \to R^{m}$, define $f_{j} \defeq \Pi_{j} \circ f$ for all $j = 1,2,\ldots,m$. Thus, $f$ can be rewritten as $(f_{1},f_{2},\ldots,f_{m})$. We can conclude that $f$ is continuous at $a \in S$ if and only if $f_{j}:S \to \R$ is continuous at $a$ for all $j = 1,2,\ldots,m$. The proof of this observation is left as an exercise to the reader.

\begin{theorem}
    Let $a \in \R^{n}$ be a limit point of a set $S \subseteq \R^{n}$, with $b \in \R^{m}$ and $f:S \to R^{m}$ a function. Then, the following are equivalent---
    \begin{enumerate}
        \item $\lim_{x \to a}f(x) = b$.
        \item If $\{x_{p}\} \subseteq S\setminus\{a\}$ and $x_{p} \to a$, then $f(x_{p}) \to b$.
        \item $\lim_{x \to a} \norm{f(x)-b} = 0$.
    \end{enumerate}
\end{theorem}
The proof of this theorem is left as an exercise to the reader.

\begin{definition}
    For a set $S \subseteq \R^{n}$, a function $f:S \to \R^{m}$ is termed a continuous function if $f$ is continuous at $a$ for all $a \in S$.
\end{definition}

\begin{theorem}
    Let $f:S \to \R^{m}$ be a function, where $S \subseteq \R^{n}$. The following are, then, equivalent---
    \begin{enumerate}
        \item $f$ is continuous.
        \item For all $a \in S$ and $\{x_{n}\} \subseteq S$ with $x_{n} \to a$, we have $f(x_{n}) \to f(a)$.
        \item For all open sets $O \subseteq \R^{m}$, the set $f^{-1}(O) \subseteq S$ is also open.
        \item For all closed sets $C \subseteq \R^{m}$, the set $f^{-1}(C) \subseteq S$ is also closed.
    \end{enumerate}
\end{theorem}
\begin{proof}
    For 1.~implies 3.~, let $O \subseteq \R^{m}$ be open. Pick some $a \in f^{-1}(O)$. Then, since $f(a) \in O$, there exists $r > 0$ such that $B_{r}(f(a)) \subseteq O$. Also, $f$ is continuous at $a$; for $\frac{r}{2} > 0$, there exists $\delta > 0$ such that 
    \begin{align}
        f(B_{\delta}(a)) \subseteq B_{\frac{r}{2}}(f(a)) \subseteq B_{r}(f(a)) \implies a \in B_{\delta}(a) \subseteq f^{-1}(B_{r}(f(a))) \subseteq f^{-1}(O).
    \end{align}
    Thus, $f^{-1}(O)$ is open. For 3.~implies 1.~, let $a \in S$. Fix $\varepsilon > 0$. Then the set $f^{-1}(B_{\varepsilon}(f(a)))$ is open; there exists a $\delta > 0$ such that $B_{\delta}(a) \subseteq f^{-1}(B_{\varepsilon}(f(a)))$.
\end{proof}
\noindent\textit{July 28th.}

We look at a few examples.
\begin{example}
    Let $f:\R^{2}\setminus\{(0,0)\} \to \R$ be defind as $f(x,y) = \frac{2xy}{x^{2}+y^{2}}$. We find the limit $\lim_{(x,y) \to (0,0)} f(x,y)$. Let us approach from different directions, starting with the line $L_{1}$ defined as $y = 0$ with $x > 0$. Then $\lim_{(x,y) \to (0,0);(x,y) \in L_{1}} f(x,y) = \lim_{(x,y) \to (0,0)} 0 = 0$. However, along the line $L_{2}$ defned as $\{(x,y) \mid x = y\; x,y > 0\}$, we have $\lim_{(x,y) \to (0,0);(x,y) \in L_{2}} f(x,y) = 1$. We conclude that this limit cannot exist. Going along the line $y = mx$ gives several possible values for the limit.
\end{example}
The above method is good only for showing that the limit does not exist; if the limit does exist, we need to use theory.

\begin{example}
    We compute the limit $\lim_{(x,y) \to (0,0)} \frac{x^{3}}{x^{2}+y^{2}}$. Here, we can prove that the limit exists as follows:
    \begin{align}
        \abs{\frac{x^{3}}{x^{2}+y^{2}}} \leq \abs{\frac{x^{3}}{x^{2}}} = \abs{x} \to 0 \implies \lim_{(x,y)\to(0,0)} \frac{x^{3}}{x^{2}+y^{2}} = 0.
    \end{align}
\end{example}

\begin{example}
    We solve the limit $\lim_{(x,y) \to (0,0)} \frac{\sin(x^{2}+y^{2})}{x^{2}+y^{2}}$. Simply rewriting $z = x^{2}+y^{2}$ gives us $z \to 0$ as $(x,y) \to (0,0)$, so $\lim_{z \to 0} \frac{\sin z}{z} = 1$.
\end{example}

Before the next example, we write down a few properties. Let $S \subseteq \R^{n}$, let $f,g:S \to \R$ be functions, and let $a \in \R^{n}$ be a limit point of $S$. Suppose $\lim_{x \to a} f(x) = \alpha$ and $\lim_{x \to b} g(x) = \beta$. Then,
\begin{enumerate}
    \item $\lim_{x \to a} (cf(x)+g(x)) = c \alpha + \beta$ for all $c \in \R$,
    \item $\lim_{x \to a}f(x)g(x) = \alpha\beta$,
    \item $\lim_{x \to a} \frac{f(x)}{g(x)} = \frac{\alpha}{\beta}$, provided that $\beta \neq 0$,
    \item if $f(x) \leq h(x) \leq g(x)$ for all $x \in S$ and if $\alpha = \beta$, then $\lim_{x \to a} h(x)$ exists and equals $\alpha$.
\end{enumerate}

A similar set of corresponding statements also hold true for continuous functions. Note that the function $\Pi_{i}:\R^{n} \to \R$ is also continuous.

\begin{example}
    The function $f(x,y) = \frac{\sin(x^{2}+y^{2})}{x^{2}+y^{2}}$ for $(x,y) \neq (0,0)$ and $f(x,y) = 1$ otherwise is a continuous function since it has been assigned its limit at $(x,y) = (0,0)$. However, the function $f(x,y) = \frac{2xy}{x^{2}+y^{2}}$ for $(x,y) \neq (0,0)$ and $f(x,y) = \alpha$ otherwise is continuous only at $\R^{2}\setminus\{(0,0)\}$ for all $\alpha \in \R$.
\end{example}

\begin{example}
    We look at the continuity of the function
    \begin{align}
        f(x,y) = \begin{cases}
            \frac{xy}{\sqrt{x^{2}+y^{2}}} &\text{ if } (x,y) \neq (0,0),\\
            0 &\text{ if } (x,y) = (0,0).
        \end{cases}
    \end{align}
    Then, we have
    \begin{align}
        \abs{\frac{xy}{\sqrt{x^{2}+y^{2}}}} \leq \frac{1}{2} \cdot \frac{x^{2}+y^{2}}{\sqrt{x^{2}+y^{2}}} = \frac{1}{2}\norm{(x,y)}
    \end{align}
    which shows that $\lim_{(x,y) \to (0,0)} f(x,y) = 0 = f(0,0)$. Thus, $f$ is continuous on $\R^{2}$.
\end{example}

\begin{example}
    Set $\cD = \{(x,y) \in \R^{2} \mid y \neq 0\}$. Since $\cD = (\Pi_{2}^{-1}(\{0\}))^{c}$, $\cD$ is an open set. Define $f:\cD \to \R$ by $f(x,y) = x \sin \frac{1}{y}$. Here, we simply work as
    \begin{align}
        \abs{f(x,y)} = \abs{x \sin \frac{1}{y}} \leq \abs{x} \text{ on } \cD.
    \end{align}
    Thus, the limit becomes $f(x,y)$.
\end{example}

Hereforth, $O_{n}$ denotes an open subset of $\R^{n}$.
\begin{remark}
    Let $(a,b) \in \R^{2}$ be a limit point of $O_{2}$. Suppose $\lim_{(x,y)\to(a,b)} f(x,y)$ exists and equals $\alpha \in \R$. It is natural to ask whether
    \begin{align}
        \alpha = \lim_{y \to b} \lim_{x \to a} f(x,y) = \lim_{x \to a} \lim_{y \to b} f(x,y).
    \end{align}
    For someone looking at multivariable limits for the first time, it is tempting to believe this holds true always. We leave this question unanswered for now, and come back to it later.
\end{remark}

A notion of uniform continuity may also be explored.
\begin{definition}
    A function $f:S \to \R$, for $S \subseteq \R^{n}$, is said to be a \eax{uniformly continuous} function if for every $\varepsilon > 0$ there exists $\delta > 0$ such that
    \begin{align}
        \norm{f(x)-f(y)} < \varepsilon \text{ for all } \norm{x-y} < \delta \text{ in } S.
    \end{align}
\end{definition}

We urge the reader to compute examples for uniformly continuous functions. The exercise of uniform contiuity implying continuity but not the other way around is left as an exercise to the reader.

\section{Differentiability}

As a little convention, for any $f:O_{n} \to \R^{m}$, we prefer to rewrite it as $f = (f_{1},\ldots,f_{n})$ where $f_{j} = \Pi_{j} f$. We now ask the question of derivatives; what does it mean for the derivative of a function $f:O_{n} \to \R^{m}$? What about $f'(a)$ for some $a \in O_{n}$?

For the case of $n=m=1$, we recall that $f$ is termed differentiable at $a$ if and only if $\lim_{h \to 0} \frac{f(a+h)-f(a)}{h}$ exists. If the limit is $\lambda$, then this limit exists if and only if $\lim_{h \to 0} \frac{f(a+h)-f(a)-h\lambda}{h}$, which is really a function of $h$. Thus, the function $h \mapsto \lambda h$ matters the most, that is, $L:\R \to \R$ where $Lh = \lambda h$. So, we can twist our words a little and say that $f$ is differentiable at $a$ if and only if there exists a linear map $L:\R \to \R$ such that 
\begin{align}
    \lim_{h \to 0} \frac{f(a+h) - f(a) - Lh}{h} = 0.
\end{align}
In this case, $f'(a) = L1 = \lambda$. We translate this exact idea into higher dimensions.

\begin{definition}
    Let $f:O_{n} \to \R^{m}$. We say that $f$ is \eax{differentiable} at $a \in O_{n}$ if there exists a linear map $L:\R^{n} \to \R^{m}$, that depends on $a$, such that
    \begin{align}
        \lim_{h \to 0} \frac{1}{\norm{h}}\left( f(a+h) - f(a) - Lh \right) = 0.
    \end{align}
    In this case, we write $Df(a) = L$ and call it the \text{total derivative} of $f$ at $a$. We say $f$ is differentiable on $O_{n}$ if $f$ is differentiable at $a$ for all $a \in O_{n}$.
\end{definition}
Observe that the above limit is equivalent to saying that $\lim_{h \to 0} \frac{1}{\norm{h}}\norm{f(a+h)-f(a)-Lh} = 0$. Note that $Df(a) = L$ is unique. To show this, suppose there exists another linear map $\tilde{L}:\R^{n} \to \R^{m}$ such that $\lim_{h \to 0} \frac{1}{\norm{h}} \norm{f(a+h)-f(a)-\tilde{L}h} = 0$. Let there exist $h_{0} \in \R^{n}$ such that $Lh_{0} \neq \tilde{L}h_{0}$ and $\norm{h_{0}} = 1$. Define $h:\R\to\R^{n}$ by $ht = th_{0}$. Then as $t \to 0$, $ht \to 0$. Therefore,
\begin{align}
    \norm{L(h(t))-\tilde{L}(h(t))} &\leq \norm{f(a+h)-f(a)-Lh(t)} + \norm{f(a+h)-f(a)-\tilde{L}h(t)} \\
    \implies \lim_{t \to 0} \frac{\norm{Lh(t) - \tilde{L}h(t)}}{\norm{h(t)}} &= 0 \implies \lim_{t \to 0} \frac{1}{\abs{t}} \abs{t} \cdot \norm{Lh_{0}-\tilde{L}h_{0}} = 0 \implies Lh_{0} = \tilde{L}h_{0}
\end{align}
which is a contradiction.

\begin{example}
    Let $f:\R^{n} \to \R^{m}$ be a linear map. In this case, $\frac{f(a+h)-f(a)-f(h)}{\norm{h}} \to 0$ as $\norm{h} \to 0$. Thus, $f$ is differentiable at $a$ and $Df(a) = f$ for all $a \in \R^{n}$.
\end{example}

\begin{example}
    Suppose $f:\R^{n} \to \R^{m}$ be defined as $f(x) = c$ for all $x \in \R^{n}$. Then, we simply have $Df(a) = 0$, the null linear mapping.
\end{example}

We now truly ask how to compute $Df(a)$. Observe that $Df(a):\R^{n}\to\R^{m}$ is a linear map. Then, one can represent it as a matrix $Df(a) \in M_{m \times n}(R)$.\\ \\
\textit{July 30th.}

\begin{theorem}
    Let $f:O_{n} \to \R^{m}$ be a function. Then $f$ is differentiable at $a \in O_{n}$ if and only if $f_{i}:O_{n} \to \R$ is differentiable at $a$ for all $i = 1,2,\ldots,n$. Moreover, in this case, 
    \begin{align}
        [Df(a)]_{m \times n} = \begin{bmatrix}
            [Df_{1}(a)]_{1 \times n} \\ \vdots \\ [Df_{m}(a)]_{1 \times n}
        \end{bmatrix}_{m \times n}.
    \end{align}
\end{theorem}

From the above theorem it is clear that $D\Pi_{i}f = \Pi_{i}Df$. We now provide a proof.
\begin{proof}
    For the forward implication, let $f$ be differentiable at $a \in O_{n}$. Set $L \defeq Df(a)$ and $L_{i} \defeq \Pi_{i}Df(a)$. Note that $L_{i}:\R^{n} \to \R$ since $Df(a):\R^{n}\to\R^{m}$ and $\Pi_{i}:\R^{m} \to \R$. Observe
    \begin{align}
        f(a+h) - f(a) - Df(a)h = (\tilde{f}_{1}(h),\tilde{f}_{2}(h),\ldots,\tilde{f}_{m}(h))
    \end{align}
    where $\tilde{f}_{i}(h) = f_{i}(a+h)-f_{i}(a)-L_{i}h$ for $i = 1,2,\ldots,m$. Thus, for all $i = 1,2,\ldots,m$,
    \begin{align}
        \abs{\tilde{f}_{i}(h)} \leq \left( \sum_{j=1}^{m} \abs{\tilde{f}_{j}(h)}^{2} \right)^{1/2} = \norm{f(a+h)-f(a)-Lh}.
    \end{align}
    Dividing by $\norm{h}$ and taking $h \to 0$, we have
    \begin{align}
        \lim_{h \to 0} \frac{\abs{\tilde{f}_{i}(h)}}{\norm{h}} = 0
    \end{align}
    which shows that $f_{i}$ is differentiable with $Df_{i}(a) = L_{i}$ $(=\Pi_{i}Df(a))$.

    For the converse, let $f_{i}$ be differentiable at $a$ for all $i = 1,2,\ldots,m$ and set $L_{i} = Df_{i}(a):\R^{n}\to\R$. Set $L = \begin{bmatrix}
        L_{1} \\ \vdots \\ L_{m}
    \end{bmatrix}:\R^{n} \to \R^{m}$, a linear map. Therefore,
    \begin{align}
        \frac{1}{\norm{h}} \norm{f(a+h)-f(a)-Lh} = \frac{1}{\norm{h}} \left( \sum_{j=1}^{m} \abs{\tilde{f}_{i}(h)}^{2} \right)^{1/2} \to 0.
    \end{align}
    where $\tilde{f}_{i}(h) = f_{i}(a+h) - f_{i}(a) - L_{i}h$ for all $i$.
\end{proof}

\begin{corollary}
    Let $f:O_{1} \to \R^{m}$ be a function. $f$ is, then, differentiable at $a in O_{1}$ if and only if $f_{i}$ is differentiable at $a$ for all $1 \leq i \leq m$. Moreover, in this case,
    \begin{align}
        Df(a) = \begin{bmatrix}
            f_{1}'(a) \\ \cdots \\ f_{m}'(a)
        \end{bmatrix}.
    \end{align}
\end{corollary}
This is just a special case when $n = 1$.

\begin{remark}
    Let $f:O_{n} \to \R^{m}$ be differentiable at $a$. Then $f$ is continuous at $a$.
\end{remark}
\begin{proof}
    We have
    \begin{align}
        \norm{f(x)-f(a)} &\leq \norm{f(x)-f(a)-(Df(a))(x-a)} + \norm{(Df(a))(x-a)} \notag \\
        &\leq \frac{1}{\norm{x-a}} \norm{f(x)-f(a)-(Df(a))(x-a)} \cdot \norm{x-a} + M\norm{x-a} \to 0
    \end{align}
    as $x \to a$. Note that such an $M > 0$ exists because $Df(a)$ is a linear map.
\end{proof}

\subsection{Chain Rule}

To simplify our study of derivatives in highder dimensions, we look at the so called chain rule. This will prove to be a very important tool to study the differentiability of any function of several variables.

\begin{theorem}[The \eax{chain rule}]
    Let $f:O_{n} \to O_{m}$ be differentiable at $a \in O_{n}$, and $g:O_{m} \to \R^{p}$ be differentiable at $b = f(a) \in O_{m}$. Then $g \circ f:O_{n} \to \R^{p}$ is differentiable at $a \in O_{n}$, and
    \begin{align}
        (D g \circ f)(a) = Dg(f(a)) \circ Df(a).
    \end{align}
\end{theorem}

For the proof, we will denote $A \defeq Df(a)$ and $B \defeq Dg(f(a))$, and $b = f(a)$. Moreover, we will write $r_{f}(x) \defeq f(x)-f(a)+Df(a)(x-a)$.

\begin{proof}
    There exists $r_{f}$ in the neighbourhood of $a$ and $r_{g}$ in the neighbourhood of $b$ such that $r_{f}(x) = f(x)-f(a)-A(x-a)$ and $r_{g}(y) = g(y)-g(b)-B(y-b)$. Now set
    \begin{align}
        r(x) = g(f(x)) - g(b) - BA(x-a).
    \end{align}
    We claim that $\lim_{x \to a} \frac{r(x)}{\norm{x-a}} = 0$. We know that $\lim_{x \to a} \frac{\norm{r_{f}(x)}}{\norm{x-a}} = 0 = \lim_{y \to b} \frac{\norm{r_{g}(y)}}{\norm{y-b}}$. Now,
    \begin{align}
        r(x) &= g(f(x))-g(b)-B(A(x-a)) = g(f(x)) - g(b) + B(r_{f}(x)-f(x)+f(a)) \notag\\
        &= \left( g(f(x)) - g(f(a)) - B(f(x)-f(a)) \right) + Br_{f}(x) \notag = r_{g}(f(x)) + Br_{f}(x).
    \end{align}
    We show that both terms on the right hand side, when divided by $\norm{x-a}$, tend to zero. Now,
    \begin{align}
        \frac{\norm{Br_{f}(x)}}{\norm{x-a}} \leq M\frac{\norm{r_{f}(x)}}{\norm{x-a}} \to 0.
    \end{align}
    For the remaining term, we work as follows: for $\varepsilon > 0$ fixed, there exists a $\delta > 0$ such that $\norm{r_{g}(y)} < \varepsilon\norm{y-b}$ for all $0 < \norm{y-b} < \delta$. By continuity of $f$ at $a$, there exists a $\tilde{\delta} > 0$ such that $\norm{f(x)-f(a)} < \delta$ for all $\norm{x-a} < \tilde{\delta}$. Thus, for all $0 < \norm{x-a} < \tilde{\delta}$, we have
    \begin{align}
        \norm{r_{g}(f(x))} &< \varepsilon \norm{f(x)-f(a)} < \varepsilon \norm{r_{f}(x)+A(x-a)} \\
        \implies \norm{r_{g}(f(x))} &< \varepsilon \left( \norm{r_{f}(x)} + \norm{A(x-a)} \right) < \varepsilon \left( \norm{r_{f}(x)} + \tilde{M} \norm{x-a} \right) \notag \\
        \implies \frac{\norm{r_{g}(f(x))}}{\norm{x-a}} &\to 0 \text{ as } x \to a.
    \end{align}
\end{proof}
\noindent \textit{August 4th.}

The derivative also satisfies nice properties in higher dimensions.

\begin{proposition}
    Let $f,g: O_{n} \to \R^{m}$ be differentiable at $a \in O_{n}$. Then
    \begin{enumerate}
        \item $D(\alpha f + g)(a) = \alpha Df(a) + Dg(a)$ for all $\alpha \in \R$.
        \item If $m = 1$, then $(f \times g)'(a) = f(a)g'(a) + f'(a)g(a)$.
        \item If $m = 1$, then $\left( \frac{f}{g} \right)'(a) = \frac{1}{(g(a))^{2}} (f'(a)g(a) - f(a)g'(a))$.
    \end{enumerate}
\end{proposition}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}

\section{Partial Derivatives}

Given $a \in O_{n}$ and $f:O_{n} \to \R$, define $\eta_{i} : (-\varepsilon,\varepsilon) \to \R$ for $i = 1,2,\ldots,n$ by
\begin{align}
    \eta_{i}(t) = f(a+te_{i}).
\end{align}
We define $\frac{\partial f}{\partial x_{i}}(a) = \frac{d\eta_{i}}{dt}(0)$, if the latter term is defined. This is called the \eax{partial derivative} of $f$ in the direction, or with respect to, $x_{i}$ at $a$. Therefore,
\begin{align}
    \frac{\partial f}{\partial x_{i}}(a) = \lim_{t \to 0} \frac{f(a+te_{i}) - f(a)}{t}.
\end{align}

\begin{remark}
    \begin{enumerate}
        \item Note that $\frac{\partial f}{\partial x_{i}}$ is easy to compute since we are essentially holding all other $x_{j}$'s with $j \neq i$ as constant and just differentiating $f$ with respect to $x_{i}$.
        \item $\frac{\partial f}{\partial x_{i}}$ is the total derivative of $f$ with the limit taken in the $x_{i}$-direction.
    \end{enumerate}
\end{remark}

We want $Df(a)$ for a function $f:O_{n} \to \R$. We show further that the idea of partial derivatives solves this issues of computing $Df(a)$.

\begin{definition}
    $f:O_{n} \to \R$ is termed a function in $C^{1}(O_{n})$ if $\frac{\partial f}{\partial x_{i}}$ for all $i = 1,2,\ldots,n$ exists and $x \mapsto \frac{\partial f}{\partial x_{i}}(x)$ is continuous on $O_{n}$.
\end{definition}

We discuss some examples.

\begin{example}
    Let $f(x,y) = x^{3}+y^{4}+\sin(xy)$ on $\R^{2}$. Taking $y$ as a constant, $x \mapsto f(x,y)$ is differentiable. Thus,
    \begin{align}
        \frac{\partial f}{\partial x} = 3x^{2} + y \cos (xy).
    \end{align}
    Similarly,
    \begin{align}
        \frac{\partial f}{\partial y} = 4y^{3} + x \cos(xy).
    \end{align}
    The above two partial derivatives are continuous functions. Thus, $f \in C^{1}(\R^{2})$.
\end{example}

\begin{example}
    Recall the function
    \begin{align}
        f(x,y) = \begin{cases}
            \frac{xy}{x^{2}+y^{2}} &\text{ if } (x,y) \neq (0,0),\\
            0 &\text{ if } (x,y) = (0,0)
        \end{cases}
    \end{align}
    which was discontinuous at the origin. At the origin, we have
    \begin{align}
        \frac{\partial f}{\partial x}(0,0) = \lim_{t \to 0} \frac{f(t,0)-f(0,0)}{t} = 0.
    \end{align}
    Similarly,
    \begin{align}
        \frac{\partial f}{\partial y}(0,0) = 0.
    \end{align}
    Thus we conclude that the partial derivatives exist at $(0,0)$. But $f$ is not continuous at $(0,0)$. Thus, we conclude that the existence of partial derivatives does \textit{not} imply the existence of the total derivative.
\end{example}


\subsection{Higher Order Partials}
Hereforth, we will denote $f_{x_{i}} \defeq \frac{\partial f}{\partial x_{i}}$. Let us assume that the partials $f_{x_{i}}$ exist for all $i = 1,2,\ldots,n$. Therefore, $f_{x_{i}} : O_{n} \to \R$ are functions. Define
\begin{align}
    \frac{\partial^{2} f}{\partial x_{j} \partial x_{i}} \defeq \frac{\partial}{\partial x_{j}} \left( \frac{\partial f}{\partial x_{i}} \right) = \frac{\partial}{\partial x_{j}} \left( f_{x_{i}} \right) \text{ for all } j = 1,2,\ldots,n.
\end{align}
This is known as the second order partial derivative. We may write this as $f_{x_{i}x_{j}} = (f_{x_{i}})_{x_{j}}$. Similarly, $\frac{\partial^{3} f}{\partial x_{i} \partial x_{j} \partial x_{k}}$ may be defined as $\frac{\partial}{\partial x_{i}} (f_{x_{j}x_{k}})$. This idea can be extended even further into higher dimensions.

\begin{example}
    Define $f(x,y) = \sin x + e^{y} + xy$. Then $f_{x} = \cos x + y$ and $f_{y} = e^{y}+x$. From here, we further have $f_{xy} = 1$ and $f_{yx} = 1$. Coincidentally, we have $f_{xy} = f_{yx}$. Thus, we question whether the order of the variables even matters.
\end{example}
The following example shows that the order of the variables does matter.
\begin{example}
    Define
    \begin{align}
        f(x,y) = \begin{cases}
            \frac{xy(x^{2}-y^{2})}{x^{2}+y^{2}} &\text{ if } (x,y) \neq (0,0),\\
            0 &\text{ if } (x,y) = (0,0).
        \end{cases}
    \end{align}
    In this case, we get $f_{xy}(0,0) = 1 \neq f_{yx}(0,0) = -1$. Partial differentiation is not commutative.
\end{example}

The following result shows that with a little more constraints, the commutativity does hold.

\begin{theorem}[\eax{Clairaut's theorem}]
    Let $(a,b) \in O_{2}$, $f:O_{2} \to \R$, and assume that $f_{xy}$ and $f_{yx}$ exist on $O_{2}$. Also suppose that $f_{xy}$ is continuous at $(a,b)$. Then $f_{xy}(a,b) = f_{yx}(a,b)$.
\end{theorem}
The result does hold in higher dimensions too, but we only show for two dimensions. The rest of the theorem and proof are left as exercises for the reader.
\begin{proof}
    Without the loss of generality, let $(a,b) = (0,0)$ and $O_{2} = B_{1}(0,0)$. Choose $h,k > 0$ such that $[0,h] \times [0,k] \subseteq B_{1}(0,0)$. Then,
    \begin{align}
        \frac{\partial^{2}f}{\partial y \partial x} &= \lim_{k \to 0} \frac{1}{k} \left( \frac{\partial f}{\partial x}(x,y+k) - \frac{\partial f}{\partial x} (x,y) \right) \\
        &= \lim_{k \to 0} \frac{1}{k} \lim_{h \to 0} \frac{1}{h} (f(x+h,y+k) - f(x,y+k) - f(x+h,y) + f(x,y)) \notag\\
        &= \lim_{k \to 0} \lim_{h \to 0} \frac{1}{hk} (f(x+h,y+k) - f(x,y+k) - f(x+h,y) + f(x,y)) \\
        \implies \frac{\partial^{2}f}{\partial y \partial x}(0,0) &= \lim_{k \to 0} \lim_{h \to 0} \frac{1}{hk} (f(h,k) - f(0,k) - f(h,0) + f(0,0)) \defeq \lim_{k \to 0} \lim_{h \to 0} \frac{1}{hk}F(h,k).
    \end{align}
    Similarly,
    \begin{align}
        \frac{\partial^{2}f}{\partial x \partial y}(0,0) = \lim_{h \to 0} \lim_{k \to 0} \frac{1}{hk}F(h,k).
    \end{align}
    Fix $k$ and $h$ for a moment. Set $f_{1}(x) = f(x,k) - f(x,0)$ for all $x \in [0,h]$. Then $f_{1}$ is continuous on $[0,h]$, since $f_{x}$ exists, and is differentiable on $(0,h)$. By the mean value theorem, there exists $c_{1} \in (0,h)$ such that $f_{1}(h) - f_{1}(0) = f_{1}'(c_{1}) \times h_{1}$
    \begin{align}
        \implies \frac{1}{h}F(h,k) = \left( \frac{\partial f}{\partial x}(c_{1},k) - \frac{\partial f}{\partial x}(c_{1},0) \right).
    \end{align}
    Next, consider $f_{2}(y) = \frac{\partial f}{\partial x} (c_{1},y)$ for all $y \in [0,k]$ which is continuous on $[0,k]$ and differentiable on $(0,k)$. Again, by the mean value theorem, there exists $c_{2} \in (0,k)$ such that $f_{2}(k) - f_{2}(0) = f_{2}'(c_{2}) \times k$
    \begin{align}
        \implies \frac{1}{hk} F(h,k) = \frac{\partial^{2} f}{\partial y \partial x} (c_{1},c_{2})
    \end{align}
    with $0 < c_{1} < h$ and $0 < c_{2} < k$. Similarly, if we had redefined $f_{1}$ and $f_{2}$, we would have received
    \begin{align}
        \frac{1}{hk} F(h,k) = \frac{\partial^{2} f}{\partial x \partial y} (\tilde{c}_{1},\tilde{c}_{2})
    \end{align}
    with $0 < \tilde{c}_{1} < h$ and $0 < \tilde{c}_{2} < k$. Thus,
    \begin{align}
        \frac{\partial^{2} f}{\partial y \partial x} (c_{1},c_{2}) = \frac{\partial^{2} f}{\partial x \partial y} (\tilde{c}_{1},\tilde{c}_{2}).
    \end{align}
    As $(h,k) \to (0,0)$, $f_{xy}(0,0) = f_{yx}(0,0)$.
\end{proof}

\noindent \textit{August 6th.}

\begin{theorem}[\eax{Schwarz theorem}]
    Let $f:O_{2} \to \R$ be a function such that $(0,0) \in O_{2}$. Also suppose that $f_{x},f_{y},f_{xy}$ exist on $O_{2}$ and $f_{xy}$ is continuous on $O_{2}$. Then $f_{yx}(0,0)$ exists and $f_{yx}(0,0) = f_{xy}(0,0)$.
\end{theorem}

\begin{proof}
    As $f_{xy}$ is continuous at $(0,0)$, for $\varepsilon > 0$, there exists $\delta > 0$ such that
    \begin{align}
        \abs{f_{xy}(s,t) - f_{xy}(0,0)} < \varepsilon \text{ for all } \sqrt{s^{2}+t^{2}} < \delta.
    \end{align}
    We already know $F(h,k) = f_{xy}(c_{1},c_{2})$ with $0 < c_{1} < h$ and $0 < c_{2} < k$. Choose $h,k$ small enough such that $\sqrt{h^{2}+k^{2}} < \delta$. Therefore, for $\sqrt{c_{1}^{2} + c_{2}^{2}} < \delta$,
    \begin{align}
        \abs{f_{xy}(c_{1},c_{2})-f_{xy}(0,0)} < \varepsilon \implies \abs{F(h,k) - f_{xy}(0,0)} < \varepsilon \text{ for } \sqrt{h^{2}+k^{2}} < \varepsilon.
    \end{align}
    From the above, we infer $-\varepsilon+f_{xy}(0,0) < F(h,k) < \varepsilon + f_{xy}(0,0)$. Rewriting the middle term,
    \begin{align}
        F(h,k) = \frac{1}{h} \left( \frac{f(h,k)-f(h,0)}{k} - \frac{f(0,k)-f(0,0)}{k} \right) \xrightarrow{k \to 0} \frac{1}{h} (f_{y}(h,0)-f_{y}(0,0))
    \end{align}
    Rebounding gives us
    \begin{align}
        \abs{\frac{1}{h}(f_{y}(h,0)-f_{y}(0,)) - f_{xy}(0,0)} \leq \varepsilon \implies \lim_{h \to 0} \frac{1}{h} (f_{y}(h,0)-f_{y}(0,0)) = f_{xy}(0,0).
    \end{align}
\end{proof}

Note that in the above theorem, $(0,0)$ was chosen for the sake of simplifying the proof. Any $(\alpha,\beta) \in O_{2}$ would have worked. We move our focus back to the total derivative.

\begin{theorem}
    Let $f:O_{n} \to \R^{m}$ be differentiable at $a \in O_{n}$. Then $\frac{\partial f_{i}}{\partial x_{j}}$ exists at $a$ for all $i = 1,2,\ldots,m$ and $j = 1,2,\ldots,n$. Moreover,
    \begin{align}
        \begin{bmatrix}
            Df(a)
        \end{bmatrix}_{m \times n} = \begin{pmatrix}
            \dfrac{\partial f_{i}}{\partial x_{j}}(a)
        \end{pmatrix}_{m \times n}.
    \end{align}
\end{theorem}

\begin{proof}
    Let $m = 1$, and fix $j \in \{1,2,\ldots,n\}$. Suppose $a = (a_{1},\ldots,a_{j},\ldots,a_{n})$. Consider the mapping $\eta_{j}:(a_{j}-\varepsilon,a_{j}+\varepsilon) \to \R^{n}$ defined as $x \mapsto (a_{1},\ldots,a_{j-1},x,a_{j+1},\ldots,a_{n})$. Since this image is in the neighbourhood of $a$, we can apply $f$ on it to get an image in $\R$; the mapping maps $x$ to $f(a_{1},\ldots,a_{j-1},x,a_{j+1},\ldots,a_{n})$.

    As $x \to a_{1}$, $x \to a_{2}$, $\ldots$, $x \to x$, $x \to a_{j+1}$, $\ldots$ differentiable on $(a_{j}-\varepsilon,a_{j}+\varepsilon)$, $\eta_{j}$ is differentiable on $a_{j}$ and $\eta_{j}'(a_{j}) = e_{j}$. Thus, $f \circ n_{j}$ is differentiable at $a_{j}$ by the chain rule.
    \begin{align}
        (Df \circ n_{j}) (a_{j}) = \frac{d}{dx} (f \circ n_{j})(a_{j}) = \lim_{h \to 0} \frac{f(\eta_{j}(a_{j}+h)) - f(a)}{h} = \lim_{h \to 0} \frac{f(a_{1},\ldots,a_{j},\ldots,a_{n})-f(a)}{h} = \frac{\partial f}{\partial x_{j}}(a).
    \end{align}
    The chain rule implies that $(Df \circ n_{j})(a_{j}) = Df(\eta_{j}(a_{j})) D\eta_{j}(a_{j}) \implies \frac{\partial f}{\partial x_{j}}(a) = Df(a) e_{j}$. Thus, we must have
    \begin{align}
        \begin{pmatrix}
            Df(a)
        \end{pmatrix} = \begin{pmatrix}
            f_{x_{1}(a)} & \cdots & f_{x_{n}}(a)
        \end{pmatrix}.
    \end{align}
    We now work the case for a general $m$; write $f$ as $(f_{1},\ldots,f_{m})$. $f$ is differentiable at $a$ implies that
    \begin{align}
        \begin{bmatrix}
            Df(a)
        \end{bmatrix} = \begin{bmatrix}
            Df_{1}(a) \\ \vdots \\ Df_{m}(a)
        \end{bmatrix} = \begin{bmatrix}
            \frac{\partial f_{1}}{\partial x_{1}}(a) & \cdots & \frac{\partial f_{1}}{\partial x_{n}}(a) \\
            \vdots & \ddots & \vdots \\
            \frac{\partial f_{m}}{\partial x_{1}}(a) & \cdots & \frac{\partial f_{m}}{\partial x_{n}}(a)
        \end{bmatrix}.
    \end{align}
\end{proof}

\begin{definition}
    Let $f:O_{n} \to \R^{m}$ be differentiable at $a$. The matrix representation $\left( \frac{\partial f_{i}}{\partial x_{j}}(a) \right)_{m \times n}$ of the total derivative $Df(a)$ is termed the \eax{Jacobian} of $f$ at $a$. We prefer to write it as $J_{f}(a)$.
\end{definition}

Since we have a matrix to deal with now, it is only natural to ask questions regarding its nature. For instance, what does the rank of the Jacobian tell us? What about its determinant?

\begin{theorem}
    Let $f:O_{n} \to \R^{m}$ be a function with $a \in O_{n}$. Suppose $f$ is a $C^{1}$ function in the neighbourhood of $a$. Then $f$ is differentiable at $a$.
\end{theorem}

There is a \textit{gap} between this theorem and the previous one; we have the extra requirement of continuity of the partial derivatives here.\\ \\
\textit{August 11th.}
\begin{example}
    Consider the function
    \begin{align}
        f(x,y) = \begin{cases}
            (x^{2}+y^{2}) \sin \frac{1}{\sqrt{x^{2}+y^{2}}} &\text{ if } (x,y) \neq (0,0),\\
            0 &\text{ if } (x,y) = (0,0).
        \end{cases}
    \end{align}
    Then $f$ is continuous at $(0,0)$. Moreover, one can show that $f$ is also differentiable at $(0,0)$ with $Df(0,0) = \begin{bmatrix}
        0 & 0
    \end{bmatrix}$. However, both $f_{x}$ and $f_{y}$ are not continuous at $(0,0)$.
\end{example}

We now provide the proof of the above theorem, setting $a = 0$ without the loss of generality.
\begin{proof}
    Let $m = 1$; the general case will be handled later. We claim that
    \begin{align}
        \lim_{h \to 0} \frac{1}{\norm{h}} \abs{f(h)-f(0) - \sum_{i=1}^{n} \frac{\partial f}{\partial x_{i}}(0)h_{i}} = 0.
    \end{align}
    For $h \in \R^{n}$, with $\norm{h}$ sufficiently small, we write $\hat{h}_{i} = (h_{1},h_{2},\ldots,h_{i},0,\ldots,0)$ with $(n-i)$ zeroes at the end, for all $i = 1,2,\ldots,n$, and $\hat{h}_{0} = (0,\ldots,0)$. Therefore,
    \begin{align}
        f(h)-f(0) &= f(\hat{h}_{n}) - f(\hat{h}_{0}) = (f(\hat{h}_{1})-f(\hat{h}_{0})) + (f(\hat{h}_{2})-f(\hat{h}_{1})) + \cdots + (f(\hat{h}_{n})-f(\hat{h}_{n-1})) \notag \\ &= \sum_{i=1}^{n} (f(\hat{h}_{i})-f(\hat{h}_{i-1})).
    \end{align}
    Define $\eta_{i}(t) = f(h_{1},\ldots,h_{i-1},t,0,\ldots,0)$ for $t \in [0,h_{i}]$. Thus, each $\eta_{i}$ is a single variable function and the chain rule tells us $\eta_{i}:[0,h_{i}] \to \R$ is a $C^{1}$-function. The mean value theorem then tell us that there exists $c_{i} \in (0,h_{i})$ such that
    \begin{align}
        \eta_{i}(h_{i})-\eta_{i}(0) &= h_{i} \eta_{i}'(c_{i}) \\
        \implies f(\hat{h}_{i})-f(\hat{h}_{i-1}) &= h_{i} \frac{\partial f}{\partial x_{i}}(h_{1},\ldots,h_{i-1},c_{i},0,\ldots,0).
    \end{align}
    Therefore,
    \begin{align}
        \frac{1}{\norm{h}} \abs{f(h)-f(0)-\sum_{i=1}^{n} f_{x_{i}}(0)h_{i}} &= \frac{1}{\norm{h}} \abs{\sum_{i=1}^{n} h_{i} \left( \frac{\partial f}{\partial x_{i}}(h_{1},\ldots,h_{i-1},c_{i},0,\ldots,0) - f_{x_{i}}(0) \right)} \notag \\
        &\leq \sum_{i=1}^{n} \frac{\abs{h_{i}}}{\norm{h}} \abs{f_{x_{i}}(h_{1},\ldots,h_{i},c_{i},0,\ldots,0) - f_{x_{i}}(0)} \xrightarrow{h \to 0} 0.
    \end{align}
    Hence, the function $f$ is differentiable at $0$.
\end{proof}

\begin{example}
    We compute the total derivative of $f(x,y,z) = (x+2y+3z,xyz,\cos x, \sin x)$. Clearly, $f$ is a $C^{1}$ function. Thus,
    \begin{align}
        J_{f}(x,y,z) = \begin{bmatrix}
            \frac{\partial f_{1}}{\partial x} & \frac{\partial f_{1}}{\partial y} & \frac{\partial f_{1}}{\partial z} \\
            \frac{\partial f_{2}}{\partial x} & \frac{\partial f_{2}}{\partial y} & \frac{\partial f_{2}}{\partial z} \\
            \frac{\partial f_{3}}{\partial x} & \frac{\partial f_{3}}{\partial y} & \frac{\partial f_{3}}{\partial z} \\
            \frac{\partial f_{4}}{\partial x} & \frac{\partial f_{4}}{\partial y} & \frac{\partial f_{4}}{\partial z}
        \end{bmatrix} = \begin{bmatrix}
            1 & 2 & 3 \\
            yz & xz & xy \\
            -\sin x & 0 & 0 \\
            \cos x & 0 & 0
        \end{bmatrix}.
    \end{align}
\end{example}

\section{Gradient, Divergence, and Curl}

We first extend the notion of the derivative along the coordinate directions to the derivative along \textit{any} direction.

\begin{definition}
    Let $u \in \R^{n}$ be a unit vector, that is, $\norm{u} = 1$. Also let $f:O_{n} \to \R$ be a function. The \eax{directional derivative} of $f$ at $x \in O_{n}$ in the direction of $u$ is defined as
    \begin{align}
        D_{u}f(x) = \lim_{t \to 0} \frac{f(x+tu)-f(x)}{t}, \text{ if exists.}
    \end{align}
\end{definition}

The reader may verify that $D_{e_{i}}f(x) = f_{x_{i}}(x)$. If we denote $t \mapsto f(x+tu)$ as $\eta(t)$, then we directional derivative is simply $D_{u}f(x) = \eta'(0)$. If we additionally assume that $f$ is differentiable at $x$, then the chain rule gives us
\begin{align}
    D_{u}f(x) = \eta'(0) = Df(x) \circ u.
\end{align}
Notice that $u$ can be thought of as a linear map from $\R$ to $\R^{n}$ and $Df(x)$ is a linear map from $\R^{n}$ to $\R$.

\begin{theorem}
    Let $f:O_{n} \to \R$ be differentiable at $x \in O_{n}$ and let $u$ be a unit vector in $\R^{n}$. Then the directional derivative $D_{u}f(x)$ exists and is given by
    \begin{align}
        (D_{u}f)(x) = Df(x)u.
    \end{align}
\end{theorem}

The idea of the gradient is introduced.

\begin{definition}
    Let $f:O_{n} \to \R$ be a function with $a \in O_{n}$. Also suppose $f_{x_{i}}(a)$ exists for all $i = 1,2,\ldots,n$. Then
    \begin{align}
        (\nabla f)(a) = (f_{x_{1}}(a),\ldots,f_{x_{n}}(a))
    \end{align}
    is called the \eax{gradient} of $f$ at $a$. Therefore,
    \begin{align}
        \nabla:\{f:\R^{n}\to\R \mid f_{x_{i}} \text{ exists}\} \to \R^{n}.
    \end{align}
\end{definition}

\begin{corollary}
    Let $f:O_{n} \to \R$ be differentiable at $x \in O_{n}$ and let $u$ be a unit vector. Then, $(D_{u}f)(x) = (\nabla f)(x) \cdot u$.
\end{corollary}

\begin{remark}
    Suppose $f:O_{n} \to \R$ is a function whose partial derivatives exist at $x \in O_{n}$. Then
    \begin{align}
        (\nabla f)(x) \cdot u = \norm{(\nabla f)(x)} \cdot \norm{u} \cos \theta = \norm{(\nabla f)(x)} \cos \theta.
    \end{align}
    As $\abs{\cos \theta} \leq 1$, the right hand side is maximum if $\theta = 0$.
\end{remark}

What follows is an important result.

\begin{theorem}
    Let $f:O_{n} \to \R$ be differentiable at $x \in O_{n}$ and suppose $(\nabla f)(x) \neq 0$. Then the vector $(\nabla f)(x)$ points in the direction of the steepest ascent of the function $f$ at $x$ and $\norm{(\nabla f)(x)}$ is the greatest possible rate of change.
\end{theorem}