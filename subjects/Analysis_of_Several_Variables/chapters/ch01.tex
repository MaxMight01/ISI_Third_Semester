\chapter{INTRODUCTION TO $\R^{n}$}

\section{Translation into Higher Dimensions}
\textit{July 21st.}

We begin with a definition.
\begin{definition}
    The space $\R^{n}$ is defined as $\R \times \R \times \cdots \times \R$ ($n$ times) = $\{(x_{1},x_{2},\ldots,x_{n}) : x_{i} \in \R\}$.
\end{definition}
In the context of analysis, we will talk about open sets, closed sets, sequences, compact sets, and connected sets. In contrast, algebra considers $\R^{n}$ as a vector space with the operators $+$ and $\cdot$. Combining both these aspects results in the study of analysis of several variables. In this course, we will mainly focus on dealing with functions of the form $f: \R^{n} \to \R^{m}$, and talk about their properties such as continuity, differentiability, and integrability.

\subsection{Algebraic and Analytic Structure}
We note that $\R^{n}$ is also an inner product space with the following properties:
\begin{itemize}
    \item $\ip{x,y} = \sum_{i=1}^{n} x_{i}y_{i}$ for all $x,y \in \R^{n}$.
    \item The set $\{e_{i}\}_{i=1}^{n}$ consisting of the unit vectors is an orthonormal basis for $\R^{n}$.
    \item The simplest maps from $\R^{n}$ to $\R^{m}$ are linear maps that send lines to lines.
\end{itemize}

\begin{example}
    Suppose the function $f$ is a linear map from $\R$ to $\R$. This implies that $f(x) = xf(1)$ for all $x \in \R$. Thus, $f(x) = cx$ for all $x \in \R$, where $c$ is a constant. Conversely, if $c \in \R$, then $x \mapsto cx$ is a linear map. Therefore, we conclude that $\{f: \R \to \R, \text{linear}\} \leftrightarrow \R$, with a possible bijection given by $f \mapsto f(1)$.
\end{example}
In the above example, we note that 1 is not special; we could simply fix any $\alpha \in \R\setminus\{0\}$, and notice that $f(x) = \frac{x}{\alpha} f(\alpha)$ for all $x \in \R$. Here, replacing $f(1)$ by $f(\alpha)$ is a kind of `change of variable'.
\begin{remark}
    Let $L:\R^{n} \to \R^{m}$ be a linear map. Then, $Le_{j} = \sum_{i=1}^{m} a_{ij} e_{i}$ for all $j = 1,2,\ldots,n$. We may write $L$ as $(a_{ij})_{m \times n} \in M_{m \times n}(\R)$, the set of all $m \times n$ matrices with real entries.
\end{remark}

Coming to the analysis side, there is a need for defining a distance between points in $\R^{n}$. Previously, we have seen that the \eax{norm} may be given as $\norm{x} = \sqrt{\sum_{i=1}^{n} x_{i}^{2}}$ for all $x \in \R^{n}$. We can use this norm to define our required distance function.

\begin{definition}
    The \eax{distance} function between two points $d:\R^{n} \times \R^{n} \to \R_{\geq 0}$ is defined as $d(x,y) = \norm{x-y} = \left( \sum_{i=1}^{n} (x_{i}-y_{i})^{2} \right)^{1/2}$ for all $x,y \in \R^{n}$.
\end{definition}
For $n = 1$, we note that $d(x,y) = \abs{x-y}$, from the previous analysis courses. $\R^{n}$ equipped with the function $d$ is called a \eax{metric space}. Coming to the properties of the inner product, we have
\begin{itemize}
    \item $\norm{x} = \ip{x,x}^{1/2}$ for all $x \in \R^{n}$.
    \item $\ip{x,y} = \ip{y,x}$ for all $x,y \in \R^{n}$.
    \item The function $\ip{,}$ is linear with respect to the first and second arguments.
\end{itemize}

We also have the important Cauchy-Schwarz inequality.
\begin{theorem}[\eax{Cauchy-Schwarz inequality}]
    For all $x,y \in \R^{n}$, we have $\abs{\ip{x,y}} \leq \norm{x} \norm{y}$.
\end{theorem}
\begin{proof}
    Note that 
    
    \begin{align}
        0 \leq \sum_{i=1}^{n} \sum_{j=1}^{n} (x_{i}y_{j}-x_{j}y_{i})^{2} &= 2\left(\sum_{i,j} x_{i}^{2}y_{j}^{2}- \sum_{i,j} x_{i}x_{j}y_{i}y_{j}\right) = 2 \left( \norm{x}^{2}\norm{y}^{2} - \ip{x,y}^{2} \right) \\ \implies \abs{\ip{x,y}} &\leq \norm{x}\norm{y}.
    \end{align}
\end{proof}
We note that equality occurs if and only if the first quantity in the above equation is zero, \textit{i.e.}, if and only if $x_{i}y_{j} = x_{j}y_{i}$ for all $i,j$, or $\frac{x_{i}}{y_{i}} = \frac{x_{j}}{y_{j}}$ for all $i,j$ showing that $x$ and $y$ are linearly dependent.
\begin{corollary}[\eax{Triangle inequality}]
    For all $x,y \in \R^{n}$, we have $\norm{x+y} \leq \norm{x} + \norm{y}$.
\end{corollary}
\begin{proof}
    We have
    \begin{align}
        \norm{x+y}^{2} = \ip{x+y,x+y} = \norm{x}^{2} + 2\ip{x,y} + \norm{y}^{2} \leq \norm{x}^{2} + 2\norm{x}\norm{y} + \norm{y}^{2} = (\norm{x}+\norm{y})^{2}
    \end{align}
    where the inequality follows from Cauchy-Schwarz.
\end{proof}
The following will prove to be an important result.
\begin{theorem}
    Let $L:\R^{n} \to \R^{m}$ be a linear map. Then, there exists a $M > 0$ such that $\norm{Lx} \leq M \norm{x}$ for all $x \in \R^{n}$.
\end{theorem}
\begin{proof}
    Rewriting $x$ as $x = \sum_{i=1}^{n} x_{i} e_{i}$, we have
    \begin{align}
        Lx &= \sum_{i=1}^{n} x_{i} Le_{i} \notag \\
        \implies \norm{Lx} &= \norm{\sum_{i=1}^{n} x_{i} Le_{i}} \leq \sum_{i=1}^{n} \abs{x_{i}} \norm{Le_{i}} \leq \norm{x} \sum_{i=1}^{n} \norm{Le_{i}} = \norm{x} M.
    \end{align}
    The first inequality follows from the triangle inequality, and the second from Cauchy-Schwarz. In the last step, $M$ is set to be $\sum_{i=1}^{n} \norm{Le_{i}}$, which is a constant.
\end{proof}
We also term $(\R^{n},d)$ as a Euclidean metric space. There is now a need to define open sets in $\R^{n}$ to talk more about the analysis of several variables.
\begin{definition}
    For $a \in \R^{n}$ and $r > 0$, the \eax{open ball} centred at $a$ of radius $r$ is $B_{r}(a) \defeq \{x \in \R^{n} : d(x,a) < r\}$, the set of all points in $\R^{n}$ that are at a distance less than $r$ from $a$.
\end{definition}
From the notion of open balls, we can define open sets.
\begin{definition}
    A set $S \subseteq \R^{n}$ is said to be an \eax{open set} if for all $x \in S$, there exists an $r > 0$ such that $B_{r}(x) \subseteq S$.
\end{definition}
We now bring the notion of convergence of sequences.
\begin{definition}
    Let $\{x_{m}\} \subseteq \R^{n}$ be a sequence and $x \in \R^{n}$. We say that $\{x_{m}\}$ \eax{converges} to $x$ if for every $\varepsilon > 0$, there exists a natural $N$ such that $\norm{x_{m}-x} < \varepsilon$ for all $m \geq N$.
\end{definition}

\textit{July 23rd.}

\begin{definition}
    Let $S \subseteq \R^{n}$ and $a \in \R^{n}$. We say that $a$ is a \eax{limit point} of $S$ if $S \cap (B_{r}(a)\setminus\{a\})$ is non-empty for all $r > 0$.
\end{definition}

We introduce more notation; for all $i = 1,2,\ldots,n$, the mapping $\Pi_{i}:\R^{n} \to \R$ is called the \eax{$i^{\text{th}}$ projection} where $\Pi_{i}(x) = x_{i}$. Note that $x = (x_{1},x_{2},\ldots,x_{n}) = (\Pi_{1}(x),\Pi_{2}(x),\ldots,\Pi_{n}(x))$. This notation allows us to formulate the following useful fact a little more neatly.

\begin{theorem}
    Let $\{x_{m}\} \subseteq \R^{n}$ be a sequence and $x \in \R^{n}$. Then $x_{m} \to x$ if and only if $\Pi_{i}(x_{m}) \to \Pi_{i}(x)$ for all $i = 1,2,\ldots,n$.
\end{theorem}
\begin{proof}
    Suppose $x_{m} \to x$. Then for all $\varepsilon > 0$, there exists a natural $N$ such that $\norm{x_{m}-x} < \varepsilon$ for all $N \geq N$. Restating, we have
    \begin{align}
        \sum_{i=1}^{n} (\Pi_{i}(x_{m})-\Pi_{i}(x))^{2} &< \varepsilon^{2} \text{ for all } n \geq N \\
        \implies \text{For all $i$, } \abs{\Pi_{i}(x_{m})-\Pi_{i}(x)} &< \varepsilon^{2} \text{ for all } n \geq N.
    \end{align}
    For the converse, we simply work backwards with $\varepsilon/\sqrt{n}$ as our choice of epsilon.
\end{proof}

For example, the sequence $\{(\frac{1}{n},\frac{1}{2n+3})\}_{n=1}^{\infty}$ converges to $(0,0)$. However, the sequence $\{(\frac{1}{n},n^{2})\}_{n=1}^{\infty}$ does not.

\begin{definition}
    Let $S \subseteq \R^{n}$. $a \in S$ is termed an \eax{interior point} of $S$ if for some $r > 0$, $B_{r}(a) \subseteq S$ holds. Thus, a set $S$ is open if $a$ is an interior point for all $a \in S$. The \eax{interior} of set $S$ is defined as $\intr S \defeq \{a \in S \mid a \text{ is an interior point.}\}$.

    If $a \in \intr(S^{c})$, then $a$ is termed an \eax{exterior point} of $S$. $a$ is termed a \eax{boundary point} if $B_{r}(a)$ meets both $S$ and $S^{c}$ for all $r > 0$. The set of \eax{boundary points} of $S$ is denoted as $\partial S$.
\end{definition}

We also term a set $S \subseteq \R^{n}$ as a \eax{closed set} if $\R^{n} \setminus S$ is open. The following facts will only be stated and will be left as an exercise to the reader:
\begin{itemize}
    \item A set $C \subseteq \R^{n}$ is closed if and only if for all sequences $\{x_{m}\}_{m=1}^{\infty} \subseteq C$ that converge to $x$ implies $x \in C$.
    \item The open ball $B_{r}(a)$ is an open set.
    \item The intersection of an arbitrary collection of closed sets is closed; likewise, the union of an arbitrary collection of open sets is open.
    \item The set $S \subseteq \R^{n}$ is open if and only if $S = \intr S$.
\end{itemize}
Fix $O \subseteq \R^{n}$.
\begin{itemize}
    \item $O$ is open if and only if $O \cap \partial O = \emptyset$.
    \item $O$ is closed if and only if $\partial O \subseteq O$.
\end{itemize}
For $S \subseteq \R^{n}$, we define the \eax{closure} of set $S$ as $\overline{S} = \intr S \cup \partial S$.
\begin{itemize}
    \item $S \subseteq \R^{n}$ is closed if and only if $\overline{S} = S$.
    \item Let $C_{i} \subseteq R$ be closed sets and $O_{i} \subseteq \R$ be open sets, for $i = 1,2,\ldots,n$. Then $C_{1} \times C_{2} \times \cdots \times C_{n} \subseteq \R^{n}$ is a closed set, and $O_{1} \times O_{2} \times \cdots \times O_{n} \subseteq \R^{n}$ is an open set.
    \item The \eax{$n$ dimensional unit sphere} $S^{n-1} \defeq \{x \in \R^{n} : \norm{x} = 1\}$ is closed in $\R^{n}$.
\end{itemize}

\begin{definition}
    For $S \subseteq \R^{n}$ and $a \in \R^{n}$, $a$ is termed an \eax{isolated point} if $a$ is \textit{not} a limit points; that there exists an $r > 0$ such that $S \cap (B_{r}(a) \setminus \{a\}) = \emptyset$.
\end{definition}

With the pesky definitions and translation of one dimensional concept into being defined over several variables, we come to limits and continuity.

\section{Limits and Continuity}
Recall that given $f:(a,b)\setminus\{c\} \to \R$, we say that $\lim_{x \to c}f(x) = b$ if for every $\varepsilon  0$, there exists a $\delta > 0$ such that $\abs{f(x)-b} < \varepsilon$ for all $x$ satisfying $0 < \abs{x-c} < \delta$. Note that in this definition of the limit, we have $f(x) \in B_{\varepsilon}(b)$ and $x \in B_{\delta}(c) \setminus \{c\}$; this can easily be rewritten as $f(B_{\delta}(c)\setminus\{c\}) \subseteq B_{\varepsilon}(b)$. However, for our definition we would not require $f$ to be defined on an open set. We defind it over any arbitrary set.

\begin{definition}
    Let $a \in S \subseteq \R^{n}$ be a limit point of $S$ and let $f:S\setminus\{a\} \to \R^{m}$ be a function and $b \in \R^{m}$. We say $\lim_{x \to a} f(x) = b$ if for every $\varepsilon > 0$, there exists a $\delta > 0$ such that $f((B_{\delta}(a)\setminus\{a\}) \cap S) \subseteq B_{\varepsilon}(b)$. Again, this is equivalent to saying that $\norm{f(x)-b} < \varepsilon$ for all $x \in S\setminus\{a\}$ satisfying $\norm{x-a} < \delta$.
\end{definition}

It is important to get accustomed to the definition that works with open balls.
\begin{remark}
    In the above definition, if we instead write $x - a = h$, then $\lim_{x \to a} f(x) = b$ is equivalent to saying that for every $\varepsilon > 0$, there exists a $\delta > 0$ such that $\norm{f(a+h)-b} < \varepsilon$ for all $\norm{h} < \delta$. We can further rewrite to get the usual notation of
    \begin{align}
        \lim_{\norm{h} \to 0} \norm{f(a+h)-b} = 0.
    \end{align}
    Note that the above limit is in the real numbers, making it eaiser to deal with.
\end{remark}

A notion of continuity also comes in handy.
\begin{definition}
    For $S \subseteq \R^{n}$, let $f:S \to R^{m}$ with $a \in S$. We say $f$ is \eax{continuous} at $a$ if $\lim_{x \to a} f(x) = f(a)$. In other words, for every $\varepsilon > 0$, there exists a $\delta > 0$ such that
    \begin{align}
        \norm{f(x)-f(a)} < \varepsilon \text{ for all } x  \in S \text{ satisfying } \norm{x-a} < \delta
    \end{align}
    or
    \begin{align}
        f(B_{\delta}(a) \cap S) \subseteq B_{\varepsilon}(f(a)).
    \end{align}
\end{definition}
Note that if $a$ is an isolated point of $S$, then any $f:S \to \R^{m}$ is continuous at $a$ since $f(\{a\}) \subseteq B_{\varepsilon}(f(a))$ holds true, trivially.

\begin{remark}
    Similar to the previous remark, $f$ is continuous at $a$ if and only if
    \begin{align}
        \lim_{\norm{h} \to 0} \norm{f(a+h)-f(a)} = 0.
    \end{align}
\end{remark}

Functions defined on $S \subseteq \R^{n}$ can be broken down into components; given $f :S \to R^{m}$, define $f_{j} \defeq \Pi_{j} \circ f$ for all $j = 1,2,\ldots,m$. Thus, $f$ can be rewritten as $(f_{1},f_{2},\ldots,f_{m})$. We can conclude that $f$ is continuous at $a \in S$ if and only if $f_{j}:S \to \R$ is continuous at $a$ for all $j = 1,2,\ldots,m$. The proof of this observation is left as an exercise to the reader.

\begin{theorem}
    Let $a \in \R^{n}$ be a limit point of a set $S \subseteq \R^{n}$, with $b \in \R^{m}$ and $f:S \to R^{m}$ a function. Then, the following are equivalent---
    \begin{enumerate}
        \item $\lim_{x \to a}f(x) = b$.
        \item If $\{x_{p}\} \subseteq S\setminus\{a\}$ and $x_{p} \to a$, then $f(x_{p}) \to b$.
        \item $\lim_{x \to a} \norm{f(x)-b} = 0$.
    \end{enumerate}
\end{theorem}
The proof of this theorem is left as an exercise to the reader.

\begin{definition}
    For a set $S \subseteq \R^{n}$, a function $f:S \to \R^{m}$ is termed a continuous function if $f$ is continuous at $a$ for all $a \in S$.
\end{definition}

\begin{theorem}
    Let $f:S \to \R^{m}$ be a function, where $S \subseteq \R^{n}$. The following are, then, equivalent---
    \begin{enumerate}
        \item $f$ is continuous.
        \item For all $a \in S$ and $\{x_{n}\} \subseteq S$ with $x_{n} \to a$, we have $f(x_{n}) \to f(a)$.
        \item For all open sets $O \subseteq \R^{m}$, the set $f^{-1}(O) \subseteq S$ is also open.
        \item For all closed sets $C \subseteq \R^{m}$, the set $f^{-1}(C) \subseteq S$ is also closed.
    \end{enumerate}
\end{theorem}
\begin{proof}
    For 1.~implies 3.~, let $O \subseteq \R^{m}$ be open. Pick some $a \in f^{-1}(O)$. Then, since $f(a) \in O$, there exists $r > 0$ such that $B_{r}(f(a)) \subseteq O$. Also, $f$ is continuous at $a$; for $\frac{r}{2} > 0$, there exists $\delta > 0$ such that 
    \begin{align}
        f(B_{\delta}(a)) \subseteq B_{\frac{r}{2}}(f(a)) \subseteq B_{r}(f(a)) \implies a \in B_{\delta}(a) \subseteq f^{-1}(B_{r}(f(a))) \subseteq f^{-1}(O).
    \end{align}
    Thus, $f^{-1}(O)$ is open. For 3.~implies 1.~, let $a \in S$. Fix $\varepsilon > 0$. Then the set $f^{-1}(B_{\varepsilon}(f(a)))$ is open; there exists a $\delta > 0$ such that $B_{\delta}(a) \subseteq f^{-1}(B_{\varepsilon}(f(a)))$.
\end{proof}
\noindent\textit{July 28th.}

We look at a few examples.
\begin{example}
    Let $f:\R^{2}\setminus\{(0,0)\} \to \R$ be defind as $f(x,y) = \frac{2xy}{x^{2}+y^{2}}$. We find the limit $\lim_{(x,y) \to (0,0)} f(x,y)$. Let us approach from different directions, starting with the line $L_{1}$ defined as $y = 0$ with $x > 0$. Then $\lim_{(x,y) \to (0,0);(x,y) \in L_{1}} f(x,y) = \lim_{(x,y) \to (0,0)} 0 = 0$. However, along the line $L_{2}$ defned as $\{(x,y) \mid x = y\; x,y > 0\}$, we have $\lim_{(x,y) \to (0,0);(x,y) \in L_{2}} f(x,y) = 1$. We conclude that this limit cannot exist. Going along the line $y = mx$ gives several possible values for the limit.
\end{example}
The above method is good only for showing that the limit does not exist; if the limit does exist, we need to use theory.

\begin{example}
    We compute the limit $\lim_{(x,y) \to (0,0)} \frac{x^{3}}{x^{2}+y^{2}}$. Here, we can prove that the limit exists as follows:
    \begin{align}
        \abs{\frac{x^{3}}{x^{2}+y^{2}}} \leq \abs{\frac{x^{3}}{x^{2}}} = \abs{x} \to 0 \implies \lim_{(x,y)\to(0,0)} \frac{x^{3}}{x^{2}+y^{2}} = 0.
    \end{align}
\end{example}

\begin{example}
    We solve the limit $\lim_{(x,y) \to (0,0)} \frac{\sin(x^{2}+y^{2})}{x^{2}+y^{2}}$. Simply rewriting $z = x^{2}+y^{2}$ gives us $z \to 0$ as $(x,y) \to (0,0)$, so $\lim_{z \to 0} \frac{\sin z}{z} = 1$.
\end{example}

Before the next example, we write down a few properties. Let $S \subseteq \R^{n}$, let $f,g:S \to \R$ be functions, and let $a \in \R^{n}$ be a limit point of $S$. Suppose $\lim_{x \to a} f(x) = \alpha$ and $\lim_{x \to b} g(x) = \beta$. Then,
\begin{enumerate}
    \item $\lim_{x \to a} (cf(x)+g(x)) = c \alpha + \beta$ for all $c \in \R$,
    \item $\lim_{x \to a}f(x)g(x) = \alpha\beta$,
    \item $\lim_{x \to a} \frac{f(x)}{g(x)} = \frac{\alpha}{\beta}$, provided that $\beta \neq 0$,
    \item if $f(x) \leq h(x) \leq g(x)$ for all $x \in S$ and if $\alpha = \beta$, then $\lim_{x \to a} h(x)$ exists and equals $\alpha$.
\end{enumerate}

A similar set of corresponding statements also hold true for continuous functions. Note that the function $\Pi_{i}:\R^{n} \to \R$ is also continuous.

\begin{example}
    The function $f(x,y) = \frac{\sin(x^{2}+y^{2})}{x^{2}+y^{2}}$ for $(x,y) \neq (0,0)$ and $f(x,y) = 1$ otherwise is a continuous function since it has been assigned its limit at $(x,y) = (0,0)$. However, the function $f(x,y) = \frac{2xy}{x^{2}+y^{2}}$ for $(x,y) \neq (0,0)$ and $f(x,y) = \alpha$ otherwise is continuous only at $\R^{2}\setminus\{(0,0)\}$ for all $\alpha \in \R$.
\end{example}

\begin{example}
    We look at the continuity of the function
    \begin{align}
        f(x,y) = \begin{cases}
            \frac{xy}{\sqrt{x^{2}+y^{2}}} &\text{ if } (x,y) \neq (0,0),\\
            0 &\text{ if } (x,y) = (0,0).
        \end{cases}
    \end{align}
    Then, we have
    \begin{align}
        \abs{\frac{xy}{\sqrt{x^{2}+y^{2}}}} \leq \frac{1}{2} \cdot \frac{x^{2}+y^{2}}{\sqrt{x^{2}+y^{2}}} = \frac{1}{2}\norm{(x,y)}
    \end{align}
    which shows that $\lim_{(x,y) \to (0,0)} f(x,y) = 0 = f(0,0)$. Thus, $f$ is continuous on $\R^{2}$.
\end{example}

\begin{example}
    Set $\cD = \{(x,y) \in \R^{2} \mid y \neq 0\}$. Since $\cD = (\Pi_{2}^{-1}(\{0\}))^{c}$, $\cD$ is an open set. Define $f:\cD \to \R$ by $f(x,y) = x \sin \frac{1}{y}$. Here, we simply work as
    \begin{align}
        \abs{f(x,y)} = \abs{x \sin \frac{1}{y}} \leq \abs{x} \text{ on } \cD.
    \end{align}
    Thus, the limit becomes $f(x,y)$.
\end{example}

Hereforth, $O_{n}$ denotes an open subset of $\R^{n}$.
\begin{remark}
    Let $(a,b) \in \R^{2}$ be a limit point of $O_{2}$. Suppose $\lim_{(x,y)\to(a,b)} f(x,y)$ exists and equals $\alpha \in \R$. It is natural to ask whether
    \begin{align}
        \alpha = \lim_{y \to b} \lim_{x \to a} f(x,y) = \lim_{x \to a} \lim_{y \to b} f(x,y).
    \end{align}
    For someone looking at multivariable limits for the first time, it is tempting to believe this holds true always. We leave this question unanswered for now, and come back to it later.
\end{remark}

A notion of uniform continuity may also be explored.
\begin{definition}
    A function $f:S \to \R$, for $S \subseteq \R^{n}$, is said to be a \eax{uniformly continuous} function if for every $\varepsilon > 0$ there exists $\delta > 0$ such that
    \begin{align}
        \norm{f(x)-f(y)} < \varepsilon \text{ for all } \norm{x-y} < \delta \text{ in } S.
    \end{align}
\end{definition}

We urge the reader to compute examples for uniformly continuous functions. The exercise of uniform contiuity implying continuity but not the other way around is left as an exercise to the reader.

\section{Differentiability}

As a little convention, for any $f:O_{n} \to \R^{m}$, we prefer to rewrite it as $f = (f_{1},\ldots,f_{n})$ where $f_{j} = \Pi_{j} f$. We now ask the question of derivatives; what does it mean for the derivative of a function $f:O_{n} \to \R^{m}$? What about $f'(a)$ for some $a \in O_{n}$?

For the case of $n=m=1$, we recall that $f$ is termed differentiable at $a$ if and only if $\lim_{h \to 0} \frac{f(a+h)-f(a)}{h}$ exists. If the limit is $\lambda$, then this limit exists if and only if $\lim_{h \to 0} \frac{f(a+h)-f(a)-h\lambda}{h}$, which is really a function of $h$. Thus, the function $h \mapsto \lambda h$ matters the most, that is, $L:\R \to \R$ where $Lh = \lambda h$. So, we can twist our words a little and say that $f$ is differentiable at $a$ if and only if there exists a linear map $L:\R \to \R$ such that 
\begin{align}
    \lim_{h \to 0} \frac{f(a+h) - f(a) - Lh}{h} = 0.
\end{align}
In this case, $f'(a) = L1 = \lambda$. We translate this exact idea into higher dimensions.

\begin{definition}
    Let $f:O_{n} \to \R^{m}$. We say that $f$ is \eax{differentiable} at $a \in O_{n}$ if there exists a linear map $L:\R^{n} \to \R^{m}$, that depends on $a$, such that
    \begin{align}
        \lim_{h \to 0} \frac{1}{\norm{h}}\left( f(a+h) - f(a) - Lh \right) = 0.
    \end{align}
    In this case, we write $Df(a) = L$ and call it the \text{total derivative} of $f$ at $a$. We say $f$ is differentiable on $O_{n}$ if $f$ is differentiable at $a$ for all $a \in O_{n}$.
\end{definition}
Observe that the above limit is equivalent to saying that $\lim_{h \to 0} \frac{1}{\norm{h}}\norm{f(a+h)-f(a)-Lh} = 0$. Note that $Df(a) = L$ is unique. To show this, suppose there exists another linear map $\tilde{L}:\R^{n} \to \R^{m}$ such that $\lim_{h \to 0} \frac{1}{\norm{h}} \norm{f(a+h)-f(a)-\tilde{L}h} = 0$. Let there exist $h_{0} \in \R^{n}$ such that $Lh_{0} \neq \tilde{L}h_{0}$ and $\norm{h_{0}} = 1$. Define $h:\R\to\R^{n}$ by $ht = th_{0}$. Then as $t \to 0$, $ht \to 0$. Therefore,
\begin{align}
    \norm{L(h(t))-\tilde{L}(h(t))} &\leq \norm{f(a+h)-f(a)-Lh(t)} + \norm{f(a+h)-f(a)-\tilde{L}h(t)} \\
    \implies \lim_{t \to 0} \frac{\norm{Lh(t) - \tilde{L}h(t)}}{\norm{h(t)}} &= 0 \implies \lim_{t \to 0} \frac{1}{\abs{t}} \abs{t} \cdot \norm{Lh_{0}-\tilde{L}h_{0}} = 0 \implies Lh_{0} = \tilde{L}h_{0}
\end{align}
which is a contradiction.

\begin{example}
    Let $f:\R^{n} \to \R^{m}$ be a linear map. In this case, $\frac{f(a+h)-f(a)-f(h)}{\norm{h}} \to 0$ as $\norm{h} \to 0$. Thus, $f$ is differentiable at $a$ and $Df(a) = f$ for all $a \in \R^{n}$.
\end{example}

\begin{example}
    Suppose $f:\R^{n} \to \R^{m}$ be defined as $f(x) = c$ for all $x \in \R^{n}$. Then, we simply have $Df(a) = 0$, the null linear mapping.
\end{example}

We now truly ask how to compute $Df(a)$. Observe that $Df(a):\R^{n}\to\R^{m}$ is a linear map. Then, one can represent it as a matrix $Df(a) \in M_{m \times n}(R)$.\\ \\
\textit{July 30th.}

\begin{theorem}
    Let $f:O_{n} \to \R^{m}$ be a function. Then $f$ is differentiable at $a \in O_{n}$ if and only if $f_{i}:O_{n} \to \R$ is differentiable at $a$ for all $i = 1,2,\ldots,n$. Moreover, in this case, 
    \begin{align}
        [Df(a)]_{m \times n} = \begin{bmatrix}
            [Df_{1}(a)]_{1 \times n} \\ \vdots \\ [Df_{m}(a)]_{1 \times n}
        \end{bmatrix}_{m \times n}.
    \end{align}
\end{theorem}

From the above theorem it is clear that $D\Pi_{i}f = \Pi_{i}Df$. We now provide a proof.
\begin{proof}
    For the forward implication, let $f$ be differentiable at $a \in O_{n}$. Set $L \defeq Df(a)$ and $L_{i} \defeq \Pi_{i}Df(a)$. Note that $L_{i}:\R^{n} \to \R$ since $Df(a):\R^{n}\to\R^{m}$ and $\Pi_{i}:\R^{m} \to \R$. Observe
    \begin{align}
        f(a+h) - f(a) - Df(a)h = (\tilde{f}_{1}(h),\tilde{f}_{2}(h),\ldots,\tilde{f}_{m}(h))
    \end{align}
    where $\tilde{f}_{i}(h) = f_{i}(a+h)-f_{i}(a)-L_{i}h$ for $i = 1,2,\ldots,m$. Thus, for all $i = 1,2,\ldots,m$,
    \begin{align}
        \abs{\tilde{f}_{i}(h)} \leq \left( \sum_{j=1}^{m} \abs{\tilde{f}_{j}(h)}^{2} \right)^{1/2} = \norm{f(a+h)-f(a)-Lh}.
    \end{align}
    Dividing by $\norm{h}$ and taking $h \to 0$, we have
    \begin{align}
        \lim_{h \to 0} \frac{\abs{\tilde{f}_{i}(h)}}{\norm{h}} = 0
    \end{align}
    which shows that $f_{i}$ is differentiable with $Df_{i}(a) = L_{i}$ $(=\Pi_{i}Df(a))$.

    For the converse, let $f_{i}$ be differentiable at $a$ for all $i = 1,2,\ldots,m$ and set $L_{i} = Df_{i}(a):\R^{n}\to\R$. Set $L = \begin{bmatrix}
        L_{1} \\ \vdots \\ L_{m}
    \end{bmatrix}:\R^{n} \to \R^{m}$, a linear map. Therefore,
    \begin{align}
        \frac{1}{\norm{h}} \norm{f(a+h)-f(a)-Lh} = \frac{1}{\norm{h}} \left( \sum_{j=1}^{m} \abs{\tilde{f}_{i}(h)}^{2} \right)^{1/2} \to 0.
    \end{align}
    where $\tilde{f}_{i}(h) = f_{i}(a+h) - f_{i}(a) - L_{i}h$ for all $i$.
\end{proof}

\begin{corollary}
    Let $f:O_{1} \to \R^{m}$ be a function. $f$ is, then, differentiable at $a in O_{1}$ if and only if $f_{i}$ is differentiable at $a$ for all $1 \leq i \leq m$. Moreover, in this case,
    \begin{align}
        Df(a) = \begin{bmatrix}
            f_{1}'(a) \\ \cdots \\ f_{m}'(a)
        \end{bmatrix}.
    \end{align}
\end{corollary}
This is just a special case when $n = 1$.

\begin{remark}
    Let $f:O_{n} \to \R^{m}$ be differentiable at $a$. Then $f$ is continuous at $a$.
\end{remark}
\begin{proof}
    We have
    \begin{align}
        \norm{f(x)-f(a)} &\leq \norm{f(x)-f(a)-(Df(a))(x-a)} + \norm{(Df(a))(x-a)} \notag \\
        &\leq \frac{1}{\norm{x-a}} \norm{f(x)-f(a)-(Df(a))(x-a)} \cdot \norm{x-a} + M\norm{x-a} \to 0
    \end{align}
    as $x \to a$. Note that such an $M > 0$ exists because $Df(a)$ is a linear map.
\end{proof}

\subsection{Chain Rule}

To simplify our study of derivatives in highder dimensions, we look at the so called chain rule. This will prove to be a very important tool to study the differentiability of any function of several variables.

\begin{theorem}[The \eax{chain rule}]
    Let $f:O_{n} \to O_{m}$ be differentiable at $a \in O_{n}$, and $g:O_{m} \to \R^{p}$ be differentiable at $b = f(a) \in O_{m}$. Then $g \circ f:O_{n} \to \R^{p}$ is differentiable at $a \in O_{n}$, and
    \begin{align}
        (D g \circ f)(a) = Dg(f(a)) \circ Df(a).
    \end{align}
\end{theorem}

For the proof, we will denote $A \defeq Df(a)$ and $B \defeq Dg(f(a))$, and $b = f(a)$. Moreover, we will write $r_{f}(x) \defeq f(x)-f(a)+Df(a)(x-a)$.

\begin{proof}
    There exists $r_{f}$ in the neighbourhood of $a$ and $r_{g}$ in the neighbourhood of $b$ such that $r_{f}(x) = f(x)-f(a)-A(x-a)$ and $r_{g}(y) = g(y)-g(b)-B(y-b)$. Now set
    \begin{align}
        r(x) = g(f(x)) - g(b) - BA(x-a).
    \end{align}
    We claim that $\lim_{x \to a} \frac{r(x)}{\norm{x-a}} = 0$. We know that $\lim_{x \to a} \frac{\norm{r_{f}(x)}}{\norm{x-a}} = 0 = \lim_{y \to b} \frac{\norm{r_{g}(y)}}{\norm{y-b}}$. Now,
    \begin{align}
        r(x) &= g(f(x))-g(b)-B(A(x-a)) = g(f(x)) - g(b) + B(r_{f}(x)-f(x)+f(a)) \notag\\
        &= \left( g(f(x)) - g(f(a)) - B(f(x)-f(a)) \right) + Br_{f}(x) \notag = r_{g}(f(x)) + Br_{f}(x).
    \end{align}
    We show that both terms on the right hand side, when divided by $\norm{x-a}$, tend to zero. Now,
    \begin{align}
        \frac{\norm{Br_{f}(x)}}{\norm{x-a}} \leq M\frac{\norm{r_{f}(x)}}{\norm{x-a}} \to 0.
    \end{align}
    For the remaining term, we work as follows: for $\varepsilon > 0$ fixed, there exists a $\delta > 0$ such that $\norm{r_{g}(y)} < \varepsilon\norm{y-b}$ for all $0 < \norm{y-b} < \delta$. By continuity of $f$ at $a$, there exists a $\tilde{\delta} > 0$ such that $\norm{f(x)-f(a)} < \delta$ for all $\norm{x-a} < \tilde{\delta}$. Thus, for all $0 < \norm{x-a} < \tilde{\delta}$, we have
    \begin{align}
        \norm{r_{g}(f(x))} &< \varepsilon \norm{f(x)-f(a)} < \varepsilon \norm{r_{f}(x)+A(x-a)} \\
        \implies \norm{r_{g}(f(x))} &< \varepsilon \left( \norm{r_{f}(x)} + \norm{A(x-a)} \right) < \varepsilon \left( \norm{r_{f}(x)} + \tilde{M} \norm{x-a} \right) \notag \\
        \implies \frac{\norm{r_{g}(f(x))}}{\norm{x-a}} &\to 0 \text{ as } x \to a.
    \end{align}
\end{proof}
\noindent \textit{August 4th.}

The derivative also satisfies nice properties in higher dimensions.

\begin{proposition}
    Let $f,g: O_{n} \to \R^{m}$ be differentiable at $a \in O_{n}$. Then
    \begin{enumerate}
        \item $D(\alpha f + g)(a) = \alpha Df(a) + Dg(a)$ for all $\alpha \in \R$.
        \item If $m = 1$, then $(f \times g)'(a) = f(a)g'(a) + f'(a)g(a)$.
        \item If $m = 1$, then $\left( \frac{f}{g} \right)'(a) = \frac{1}{(g(a))^{2}} (f'(a)g(a) - f(a)g'(a))$.
    \end{enumerate}
\end{proposition}
\begin{proof}
    The proof is left as an exercise to the reader.
\end{proof}

\section{Partial Derivatives}

Given $a \in O_{n}$ and $f:O_{n} \to \R$, define $\eta_{i} : (-\varepsilon,\varepsilon) \to \R$ for $i = 1,2,\ldots,n$ by
\begin{align}
    \eta_{i}(t) = f(a+te_{i}).
\end{align}
We define $\frac{\partial f}{\partial x_{i}}(a) = \frac{d\eta_{i}}{dt}(0)$, if the latter term is defined. This is called the \eax{partial derivative} of $f$ in the direction, or with respect to, $x_{i}$ at $a$. Therefore,
\begin{align}
    \frac{\partial f}{\partial x_{i}}(a) = \lim_{t \to 0} \frac{f(a+te_{i}) - f(a)}{t}.
\end{align}

\begin{remark}
    \begin{enumerate}
        \item Note that $\frac{\partial f}{\partial x_{i}}$ is easy to compute since we are essentially holding all other $x_{j}$'s with $j \neq i$ as constant and just differentiating $f$ with respect to $x_{i}$.
        \item $\frac{\partial f}{\partial x_{i}}$ is the total derivative of $f$ with the limit taken in the $x_{i}$-direction.
    \end{enumerate}
\end{remark}

We want $Df(a)$ for a function $f:O_{n} \to \R$. We show further that the idea of partial derivatives solves this issues of computing $Df(a)$.

\begin{definition}
    $f:O_{n} \to \R$ is termed a function in $C^{1}(O_{n})$ if $\frac{\partial f}{\partial x_{i}}$ for all $i = 1,2,\ldots,n$ exists and $x \mapsto \frac{\partial f}{\partial x_{i}}(x)$ is continuous on $O_{n}$.
\end{definition}

We discuss some examples.

\begin{example}
    Let $f(x,y) = x^{3}+y^{4}+\sin(xy)$ on $\R^{2}$. Taking $y$ as a constant, $x \mapsto f(x,y)$ is differentiable. Thus,
    \begin{align}
        \frac{\partial f}{\partial x} = 3x^{2} + y \cos (xy).
    \end{align}
    Similarly,
    \begin{align}
        \frac{\partial f}{\partial y} = 4y^{3} + x \cos(xy).
    \end{align}
    The above two partial derivatives are continuous functions. Thus, $f \in C^{1}(\R^{2})$.
\end{example}

\begin{example}
    Recall the function
    \begin{align}
        f(x,y) = \begin{cases}
            \frac{xy}{x^{2}+y^{2}} &\text{ if } (x,y) \neq (0,0),\\
            0 &\text{ if } (x,y) = (0,0)
        \end{cases}
    \end{align}
    which was discontinuous at the origin. At the origin, we have
    \begin{align}
        \frac{\partial f}{\partial x}(0,0) = \lim_{t \to 0} \frac{f(t,0)-f(0,0)}{t} = 0.
    \end{align}
    Similarly,
    \begin{align}
        \frac{\partial f}{\partial y}(0,0) = 0.
    \end{align}
    Thus we conclude that the partial derivatives exist at $(0,0)$. But $f$ is not continuous at $(0,0)$. Thus, we conclude that the existence of partial derivatives does \textit{not} imply the existence of the total derivative.
\end{example}


\subsection{Higher Order Partials}
Hereforth, we will denote $f_{x_{i}} \defeq \frac{\partial f}{\partial x_{i}}$. Let us assume that the partials $f_{x_{i}}$ exist for all $i = 1,2,\ldots,n$. Therefore, $f_{x_{i}} : O_{n} \to \R$ are functions. Define
\begin{align}
    \frac{\partial^{2} f}{\partial x_{j} \partial x_{i}} \defeq \frac{\partial}{\partial x_{j}} \left( \frac{\partial f}{\partial x_{i}} \right) = \frac{\partial}{\partial x_{j}} \left( f_{x_{i}} \right) \text{ for all } j = 1,2,\ldots,n.
\end{align}
This is known as the second order partial derivative. We may write this as $f_{x_{i}x_{j}} = (f_{x_{i}})_{x_{j}}$. Similarly, $\frac{\partial^{3} f}{\partial x_{i} \partial x_{j} \partial x_{k}}$ may be defined as $\frac{\partial}{\partial x_{i}} (f_{x_{j}x_{k}})$. This idea can be extended even further into higher dimensions.

\begin{example}
    Define $f(x,y) = \sin x + e^{y} + xy$. Then $f_{x} = \cos x + y$ and $f_{y} = e^{y}+x$. From here, we further have $f_{xy} = 1$ and $f_{yx} = 1$. Coincidentally, we have $f_{xy} = f_{yx}$. Thus, we question whether the order of the variables even matters.
\end{example}
The following example shows that the order of the variables does matter.
\begin{example}
    Define
    \begin{align}
        f(x,y) = \begin{cases}
            \frac{xy(x^{2}-y^{2})}{x^{2}+y^{2}} &\text{ if } (x,y) \neq (0,0),\\
            0 &\text{ if } (x,y) = (0,0).
        \end{cases}
    \end{align}
    In this case, we get $f_{xy}(0,0) = 1 \neq f_{yx}(0,0) = -1$. Partial differentiation is not commutative.
\end{example}

The following result shows that with a little more constraints, the commutativity does hold.

\begin{theorem}[\eax{Clairaut's theorem}]
    Let $(a,b) \in O_{2}$, $f:O_{2} \to \R$, and assume that $f_{xy}$ and $f_{yx}$ exist on $O_{2}$. Also suppose that $f_{xy}$ is continuous at $(a,b)$. Then $f_{xy}(a,b) = f_{yx}(a,b)$.
\end{theorem}
The result does hold in higher dimensions too, but we only show for two dimensions. The rest of the theorem and proof are left as exercises for the reader.
\begin{proof}
    Without the loss of generality, let $(a,b) = (0,0)$ and $O_{2} = B_{1}(0,0)$. Choose $h,k > 0$ such that $[0,h] \times [0,k] \subseteq B_{1}(0,0)$. Then,
    \begin{align}
        \frac{\partial^{2}f}{\partial y \partial x} &= \lim_{k \to 0} \frac{1}{k} \left( \frac{\partial f}{\partial x}(x,y+k) - \frac{\partial f}{\partial x} (x,y) \right) \\
        &= \lim_{k \to 0} \frac{1}{k} \lim_{h \to 0} \frac{1}{h} (f(x+h,y+k) - f(x,y+k) - f(x+h,y) + f(x,y)) \notag\\
        &= \lim_{k \to 0} \lim_{h \to 0} \frac{1}{hk} (f(x+h,y+k) - f(x,y+k) - f(x+h,y) + f(x,y)) \\
        \implies \frac{\partial^{2}f}{\partial y \partial x}(0,0) &= \lim_{k \to 0} \lim_{h \to 0} \frac{1}{hk} (f(h,k) - f(0,k) - f(h,0) + f(0,0)) \defeq \lim_{k \to 0} \lim_{h \to 0} \frac{1}{hk}F(h,k).
    \end{align}
    Similarly,
    \begin{align}
        \frac{\partial^{2}f}{\partial x \partial y}(0,0) = \lim_{h \to 0} \lim_{k \to 0} \frac{1}{hk}F(h,k).
    \end{align}
    Fix $k$ and $h$ for a moment. Set $f_{1}(x) = f(x,k) - f(x,0)$ for all $x \in [0,h]$. Then $f_{1}$ is continuous on $[0,h]$, since $f_{x}$ exists, and is differentiable on $(0,h)$. By the mean value theorem, there exists $c_{1} \in (0,h)$ such that $f_{1}(h) - f_{1}(0) = f_{1}'(c_{1}) \times h_{1}$
    \begin{align}
        \implies \frac{1}{h}F(h,k) = \left( \frac{\partial f}{\partial x}(c_{1},k) - \frac{\partial f}{\partial x}(c_{1},0) \right).
    \end{align}
    Next, consider $f_{2}(y) = \frac{\partial f}{\partial x} (c_{1},y)$ for all $y \in [0,k]$ which is continuous on $[0,k]$ and differentiable on $(0,k)$. Again, by the mean value theorem, there exists $c_{2} \in (0,k)$ such that $f_{2}(k) - f_{2}(0) = f_{2}'(c_{2}) \times k$
    \begin{align}
        \implies \frac{1}{hk} F(h,k) = \frac{\partial^{2} f}{\partial y \partial x} (c_{1},c_{2})
    \end{align}
    with $0 < c_{1} < h$ and $0 < c_{2} < k$. Similarly, if we had redefined $f_{1}$ and $f_{2}$, we would have received
    \begin{align}
        \frac{1}{hk} F(h,k) = \frac{\partial^{2} f}{\partial x \partial y} (\tilde{c}_{1},\tilde{c}_{2})
    \end{align}
    with $0 < \tilde{c}_{1} < h$ and $0 < \tilde{c}_{2} < k$. Thus,
    \begin{align}
        \frac{\partial^{2} f}{\partial y \partial x} (c_{1},c_{2}) = \frac{\partial^{2} f}{\partial x \partial y} (\tilde{c}_{1},\tilde{c}_{2}).
    \end{align}
    As $(h,k) \to (0,0)$, $f_{xy}(0,0) = f_{yx}(0,0)$.
\end{proof}

\noindent \textit{August 6th.}

\begin{theorem}[\eax{Schwarz theorem}]
    Let $f:O_{2} \to \R$ be a function such that $(0,0) \in O_{2}$. Also suppose that $f_{x},f_{y},f_{xy}$ exist on $O_{2}$ and $f_{xy}$ is continuous on $O_{2}$. Then $f_{yx}(0,0)$ exists and $f_{yx}(0,0) = f_{xy}(0,0)$.
\end{theorem}

\begin{proof}
    As $f_{xy}$ is continuous at $(0,0)$, for $\varepsilon > 0$, there exists $\delta > 0$ such that
    \begin{align}
        \abs{f_{xy}(s,t) - f_{xy}(0,0)} < \varepsilon \text{ for all } \sqrt{s^{2}+t^{2}} < \delta.
    \end{align}
    We already know $F(h,k) = f_{xy}(c_{1},c_{2})$ with $0 < c_{1} < h$ and $0 < c_{2} < k$. Choose $h,k$ small enough such that $\sqrt{h^{2}+k^{2}} < \delta$. Therefore, for $\sqrt{c_{1}^{2} + c_{2}^{2}} < \delta$,
    \begin{align}
        \abs{f_{xy}(c_{1},c_{2})-f_{xy}(0,0)} < \varepsilon \implies \abs{F(h,k) - f_{xy}(0,0)} < \varepsilon \text{ for } \sqrt{h^{2}+k^{2}} < \varepsilon.
    \end{align}
    From the above, we infer $-\varepsilon+f_{xy}(0,0) < F(h,k) < \varepsilon + f_{xy}(0,0)$. Rewriting the middle term,
    \begin{align}
        F(h,k) = \frac{1}{h} \left( \frac{f(h,k)-f(h,0)}{k} - \frac{f(0,k)-f(0,0)}{k} \right) \xrightarrow{k \to 0} \frac{1}{h} (f_{y}(h,0)-f_{y}(0,0))
    \end{align}
    Rebounding gives us
    \begin{align}
        \abs{\frac{1}{h}(f_{y}(h,0)-f_{y}(0,)) - f_{xy}(0,0)} \leq \varepsilon \implies \lim_{h \to 0} \frac{1}{h} (f_{y}(h,0)-f_{y}(0,0)) = f_{xy}(0,0).
    \end{align}
\end{proof}

Note that in the above theorem, $(0,0)$ was chosen for the sake of simplifying the proof. Any $(\alpha,\beta) \in O_{2}$ would have worked. We move our focus back to the total derivative.

\begin{theorem}
    Let $f:O_{n} \to \R^{m}$ be differentiable at $a \in O_{n}$. Then $\frac{\partial f_{i}}{\partial x_{j}}$ exists at $a$ for all $i = 1,2,\ldots,m$ and $j = 1,2,\ldots,n$. Moreover,
    \begin{align}
        \begin{bmatrix}
            Df(a)
        \end{bmatrix}_{m \times n} = \begin{pmatrix}
            \dfrac{\partial f_{i}}{\partial x_{j}}(a)
        \end{pmatrix}_{m \times n}.
    \end{align}
\end{theorem}

\begin{proof}
    Let $m = 1$, and fix $j \in \{1,2,\ldots,n\}$. Suppose $a = (a_{1},\ldots,a_{j},\ldots,a_{n})$. Consider the mapping $\eta_{j}:(a_{j}-\varepsilon,a_{j}+\varepsilon) \to \R^{n}$ defined as $x \mapsto (a_{1},\ldots,a_{j-1},x,a_{j+1},\ldots,a_{n})$. Since this image is in the neighbourhood of $a$, we can apply $f$ on it to get an image in $\R$; the mapping maps $x$ to $f(a_{1},\ldots,a_{j-1},x,a_{j+1},\ldots,a_{n})$.

    As $x \to a_{1}$, $x \to a_{2}$, $\ldots$, $x \to x$, $x \to a_{j+1}$, $\ldots$ differentiable on $(a_{j}-\varepsilon,a_{j}+\varepsilon)$, $\eta_{j}$ is differentiable on $a_{j}$ and $\eta_{j}'(a_{j}) = e_{j}$. Thus, $f \circ n_{j}$ is differentiable at $a_{j}$ by the chain rule.
    \begin{align}
        (Df \circ n_{j}) (a_{j}) = \frac{d}{dx} (f \circ n_{j})(a_{j}) = \lim_{h \to 0} \frac{f(\eta_{j}(a_{j}+h)) - f(a)}{h} = \lim_{h \to 0} \frac{f(a_{1},\ldots,a_{j},\ldots,a_{n})-f(a)}{h} = \frac{\partial f}{\partial x_{j}}(a).
    \end{align}
    The chain rule implies that $(Df \circ n_{j})(a_{j}) = Df(\eta_{j}(a_{j})) D\eta_{j}(a_{j}) \implies \frac{\partial f}{\partial x_{j}}(a) = Df(a) e_{j}$. Thus, we must have
    \begin{align}
        \begin{pmatrix}
            Df(a)
        \end{pmatrix} = \begin{pmatrix}
            f_{x_{1}(a)} & \cdots & f_{x_{n}}(a)
        \end{pmatrix}.
    \end{align}
    We now work the case for a general $m$; write $f$ as $(f_{1},\ldots,f_{m})$. $f$ is differentiable at $a$ implies that
    \begin{align}
        \begin{bmatrix}
            Df(a)
        \end{bmatrix} = \begin{bmatrix}
            Df_{1}(a) \\ \vdots \\ Df_{m}(a)
        \end{bmatrix} = \begin{bmatrix}
            \frac{\partial f_{1}}{\partial x_{1}}(a) & \cdots & \frac{\partial f_{1}}{\partial x_{n}}(a) \\
            \vdots & \ddots & \vdots \\
            \frac{\partial f_{m}}{\partial x_{1}}(a) & \cdots & \frac{\partial f_{m}}{\partial x_{n}}(a)
        \end{bmatrix}.
    \end{align}
\end{proof}

\begin{definition}
    Let $f:O_{n} \to \R^{m}$ be differentiable at $a$. The matrix representation $\left( \frac{\partial f_{i}}{\partial x_{j}}(a) \right)_{m \times n}$ of the total derivative $Df(a)$ is termed the \eax{Jacobian} of $f$ at $a$. We prefer to write it as $J_{f}(a)$.
\end{definition}

Since we have a matrix to deal with now, it is only natural to ask questions regarding its nature. For instance, what does the rank of the Jacobian tell us? What about its determinant?

\begin{theorem}
    Let $f:O_{n} \to \R^{m}$ be a function with $a \in O_{n}$. Suppose $f$ is a $C^{1}$ function in the neighbourhood of $a$. Then $f$ is differentiable at $a$.
\end{theorem}

There is a \textit{gap} between this theorem and the previous one; we have the extra requirement of continuity of the partial derivatives here.\\ \\
\textit{August 11th.}
\begin{example}
    Consider the function
    \begin{align}
        f(x,y) = \begin{cases}
            (x^{2}+y^{2}) \sin \frac{1}{\sqrt{x^{2}+y^{2}}} &\text{ if } (x,y) \neq (0,0),\\
            0 &\text{ if } (x,y) = (0,0).
        \end{cases}
    \end{align}
    Then $f$ is continuous at $(0,0)$. Moreover, one can show that $f$ is also differentiable at $(0,0)$ with $Df(0,0) = \begin{bmatrix}
        0 & 0
    \end{bmatrix}$. However, both $f_{x}$ and $f_{y}$ are not continuous at $(0,0)$.
\end{example}

We now provide the proof of the above theorem, setting $a = 0$ without the loss of generality.
\begin{proof}
    Let $m = 1$; the general case will be handled later. We claim that
    \begin{align}
        \lim_{h \to 0} \frac{1}{\norm{h}} \abs{f(h)-f(0) - \sum_{i=1}^{n} \frac{\partial f}{\partial x_{i}}(0)h_{i}} = 0.
    \end{align}
    For $h \in \R^{n}$, with $\norm{h}$ sufficiently small, we write $\hat{h}_{i} = (h_{1},h_{2},\ldots,h_{i},0,\ldots,0)$ with $(n-i)$ zeroes at the end, for all $i = 1,2,\ldots,n$, and $\hat{h}_{0} = (0,\ldots,0)$. Therefore,
    \begin{align}
        f(h)-f(0) &= f(\hat{h}_{n}) - f(\hat{h}_{0}) = (f(\hat{h}_{1})-f(\hat{h}_{0})) + (f(\hat{h}_{2})-f(\hat{h}_{1})) + \cdots + (f(\hat{h}_{n})-f(\hat{h}_{n-1})) \notag \\ &= \sum_{i=1}^{n} (f(\hat{h}_{i})-f(\hat{h}_{i-1})).
    \end{align}
    Define $\eta_{i}(t) = f(h_{1},\ldots,h_{i-1},t,0,\ldots,0)$ for $t \in [0,h_{i}]$. Thus, each $\eta_{i}$ is a single variable function and the chain rule tells us $\eta_{i}:[0,h_{i}] \to \R$ is a $C^{1}$-function. The mean value theorem then tell us that there exists $c_{i} \in (0,h_{i})$ such that
    \begin{align}
        \eta_{i}(h_{i})-\eta_{i}(0) &= h_{i} \eta_{i}'(c_{i}) \\
        \implies f(\hat{h}_{i})-f(\hat{h}_{i-1}) &= h_{i} \frac{\partial f}{\partial x_{i}}(h_{1},\ldots,h_{i-1},c_{i},0,\ldots,0).
    \end{align}
    Therefore,
    \begin{align}
        \frac{1}{\norm{h}} \abs{f(h)-f(0)-\sum_{i=1}^{n} f_{x_{i}}(0)h_{i}} &= \frac{1}{\norm{h}} \abs{\sum_{i=1}^{n} h_{i} \left( \frac{\partial f}{\partial x_{i}}(h_{1},\ldots,h_{i-1},c_{i},0,\ldots,0) - f_{x_{i}}(0) \right)} \notag \\
        &\leq \sum_{i=1}^{n} \frac{\abs{h_{i}}}{\norm{h}} \abs{f_{x_{i}}(h_{1},\ldots,h_{i},c_{i},0,\ldots,0) - f_{x_{i}}(0)} \xrightarrow{h \to 0} 0.
    \end{align}
    Hence, the function $f$ is differentiable at $0$.
\end{proof}

\begin{example}
    We compute the total derivative of $f(x,y,z) = (x+2y+3z,xyz,\cos x, \sin x)$. Clearly, $f$ is a $C^{1}$ function. Thus,
    \begin{align}
        J_{f}(x,y,z) = \begin{bmatrix}
            \frac{\partial f_{1}}{\partial x} & \frac{\partial f_{1}}{\partial y} & \frac{\partial f_{1}}{\partial z} \\
            \frac{\partial f_{2}}{\partial x} & \frac{\partial f_{2}}{\partial y} & \frac{\partial f_{2}}{\partial z} \\
            \frac{\partial f_{3}}{\partial x} & \frac{\partial f_{3}}{\partial y} & \frac{\partial f_{3}}{\partial z} \\
            \frac{\partial f_{4}}{\partial x} & \frac{\partial f_{4}}{\partial y} & \frac{\partial f_{4}}{\partial z}
        \end{bmatrix} = \begin{bmatrix}
            1 & 2 & 3 \\
            yz & xz & xy \\
            -\sin x & 0 & 0 \\
            \cos x & 0 & 0
        \end{bmatrix}.
    \end{align}
\end{example}

\section{Gradient and The Chain Rule}

We first extend the notion of the derivative along the coordinate directions to the derivative along \textit{any} direction.

\begin{definition}
    Let $u \in \R^{n}$ be a unit vector, that is, $\norm{u} = 1$. Also let $f:O_{n} \to \R$ be a function. The \eax{directional derivative} of $f$ at $x \in O_{n}$ in the direction of $u$ is defined as
    \begin{align}
        D_{u}f(x) = \lim_{t \to 0} \frac{f(x+tu)-f(x)}{t}, \text{ if exists.}
    \end{align}
\end{definition}

The reader may verify that $D_{e_{i}}f(x) = f_{x_{i}}(x)$. If we denote $t \mapsto f(x+tu)$ as $\eta(t)$, then we directional derivative is simply $D_{u}f(x) = \eta'(0)$. If we additionally assume that $f$ is differentiable at $x$, then the chain rule gives us
\begin{align}
    D_{u}f(x) = \eta'(0) = Df(x) \circ u.
\end{align}
Notice that $u$ can be thought of as a linear map from $\R$ to $\R^{n}$ and $Df(x)$ is a linear map from $\R^{n}$ to $\R$.

\begin{theorem}
    Let $f:O_{n} \to \R$ be differentiable at $x \in O_{n}$ and let $u$ be a unit vector in $\R^{n}$. Then the directional derivative $D_{u}f(x)$ exists and is given by
    \begin{align}
        (D_{u}f)(x) = Df(x)u.
    \end{align}
\end{theorem}

The idea of the gradient is introduced.

\begin{definition}
    Let $f:O_{n} \to \R$ be a function with $a \in O_{n}$. Also suppose $f_{x_{i}}(a)$ exists for all $i = 1,2,\ldots,n$. Then
    \begin{align}
        (\nabla f)(a) = (f_{x_{1}}(a),\ldots,f_{x_{n}}(a))
    \end{align}
    is called the \eax{gradient} of $f$ at $a$. Therefore,
    \begin{align}
        \nabla:\{f:\R^{n}\to\R \mid f_{x_{i}} \text{ exists}\} \to \R^{n}.
    \end{align}
\end{definition}

\begin{corollary}
    Let $f:O_{n} \to \R$ be differentiable at $x \in O_{n}$ and let $u$ be a unit vector. Then, $(D_{u}f)(x) = (\nabla f)(x) \cdot u$.
\end{corollary}

\begin{remark}
    Suppose $f:O_{n} \to \R$ is a function whose partial derivatives exist at $x \in O_{n}$. Then
    \begin{align}
        (\nabla f)(x) \cdot u = \norm{(\nabla f)(x)} \cdot \norm{u} \cos \theta = \norm{(\nabla f)(x)} \cos \theta.
    \end{align}
    As $\abs{\cos \theta} \leq 1$, the right hand side is maximum if $\theta = 0$.
\end{remark}

\noindent \textit{August 13th.}

What follows is an important result.

\begin{theorem}
    Let $f:O_{n} \to \R$ be differentiable at $x \in O_{n}$ and suppose $(\nabla f)(x) \neq 0$. Then the vector $(\nabla f)(x)$ points in the direction of the steepest ascent of the function $f$ at $x$ and $\norm{(\nabla f)(x)}$ is the greatest possible rate of change.
\end{theorem}

In other words, the maximum possible directional derivative of $f$ at $a$ occurs at $\nabla f(a)$.

\begin{example}
    Let $f(x,y,z) = x^{2}yz$. We find the directional derivatives of $f$ at $(1,1,0)$ in the direction of $\ip{1,1,-1}$. Note that the unit vector in this case is $u = \frac{1}{\sqrt{3}}\ip{1,1,-1}$, and that $f$ is differentiable since $f$ is a polynomial. Thus, $f_{x} = 2xyz$, $f_{y} = x^{2}z$, and $f_{z} = x^{2}y$. Therefore,
    \begin{align}
        \nabla f = \ip{2xyz, x^{2}z, x^{2}y} \implies \nabla f (1,1,0) = \ip{0,0,1}.
    \end{align}
    Finally, we get
    \begin{align}
        D_{u}f(1,1,0) = \ip{0,0,1} \cdot \frac{1}{\sqrt{3}} \ip{1,1,-1} = -\frac{1}{\sqrt{3}}.
    \end{align}
    Also, the maximum possible derivative of $f$ at $(1,1,0)$ occurs at $\ip{0,0,1}$ and it is $\ip{0,0,1} \cdot \ip{0,0,1} = 1$.
\end{example}

\begin{example}
    We look at
    \begin{align}
        f(x,y) = \begin{cases}
            \frac{x^{2}y}{x^{2}+y^{2}} &\text{ if } (x,y) \neq (0,0),\\
            0 &\text{ if } (x,y) = (0,0).
        \end{cases}
    \end{align}
    Clearly, $\abs{f(x,y) - f(0,0)} = \frac{x^{2}\abs{y}}{x^{2}+y^{2}} \leq \abs{y}$, so $f$ is continuous at $(0,0)$. For a given direction $u = \ip{u_{1},u_{2}}$, we have
    \begin{align}
        D_{u}f(0,0) = \lim_{t \to 0} \frac{f(tu_{1},tu_{2})-f(0,0)}{t} = \lim_{t \to 0} \frac{t^{3}u_{1}^{2}u_{2}}{t^{2}(u_{1}^{2}+u_{2}^{2})} \cdot \frac{1}{t} = u_{1}^{2}u_{2} < \infty.
    \end{align}
    Therefore, $D_{u}f(0,0) = u_{1}^{2}u_{2}$. Also, $f_{x}(0,0) = 0 = f_{y}(0,0)$ implying that $\nabla f(0,0) = \ip{0,0}$. This gives us
    \begin{align}
        \nabla f(0,0) \cdot u = 0 \neq u_{1}^{2}u_{2} = D_{u}f(0,0)
    \end{align}
    making $f$ \textit{not} differentiable at $(0,0)$.
\end{example}

Hereforth, given $a,b \in \R^{n}$, we denote $L_{a,b}$ to be the line segment joining $a$ to $b$. Essentially, $L_{a,b} = \{(1-t)a + tb \mid 0 \leq t \leq 1\}$.

\begin{theorem}[The \eax{mean value theorem}]
    Let $f:O_{n} \to \R$ be differentiable and $L_{a,b} \subseteq O_{n}$. Then there exists $c \in L_{a,b}$ such that
    \begin{align}
        f(b)-f(a) = \nabla f(c) \cdot (b-a).
    \end{align}
\end{theorem}

\begin{proof}
    Define $\eta:[0,1] \to L_{a,b}$ as $\eta(t) = (1-t)a + tb$. Then $f \circ \eta : [0,1] \to \R$ is differentiable and $\eta'(t) = b-a$, a column vector. We apply the one-dimensional mean value theorem on $f \circ \eta$ to get
    \begin{align}
        (f \circ \eta)(1) - (f \circ \eta)(0) = (f \circ \eta)'(t_{0})
    \end{align}
    for some $t_{0} \in (0,1)$. This implies that
    \begin{align}
        f(b) - f(a) = f'(\eta(t_{0})) \cdot \eta'(t_{0}) = \nabla f(\eta(t_{0})) \cdot (b-a) = \nabla f(c) \cdot (b-a)
    \end{align}
    where $c = \eta(t_{0}) \in L_{a,b}$.
\end{proof}

\subsection{More Partials}
Suppose we have $f:O_{n} \to O_{m}$ and $g:O_{m} \to \R^{p}$ where $a \in O_{n}$ and $b = f(a) \in O_{m}$. Assume that $f$ and $g$ are differentiable at $a$ and $b$, respectively. Then 
\begin{align}
    J_{g \circ f}(a) = J_{g}(b)J_{f}(a).
\end{align}
Comparing the $(i,j)^{\text{th}}$ entry of both sides (for $1 \leq i \leq p$ and $1 \leq j \leq n$), we get
\begin{align}
    \frac{\partial (g \circ f)_{i}}{\partial x_{j}}(a) = \sum_{k=1}^{m} \frac{\partial g_{i}}{\partial y_{k}}(b) \frac{\partial f_{k}}{\partial x_{j}}(a).
\end{align}

In a more natural way, set $y_{k} = f_{k}(x_{1},\ldots,x_{n})$ for $k = 1,\ldots,m$ and set $z_{i} = g_{i}(y_{1},\ldots,y_{m})$ for $i = 1,\ldots,p$. Then we can write
\begin{align}
    \frac{\partial z_{i}}{\partial x_{j}} = \frac{\partial z_{i}}{\partial y_{1}} \cdot \frac{\partial y_{1}}{\partial x_{j}} + \cdots + \frac{\partial z_{i}}{\partial y_{m}} \cdot \frac{\partial y_{m}}{\partial x_{j}} = \sum_{t = 1}^{m} \frac{\partial z_{i}}{\partial y_{t}} \cdot \frac{\partial y_{t}}{\partial x_{j}}.
\end{align}
This is termed the \eax{chain rule for partials}.

\begin{example}
    Suppose on $O_{1} \to O_{m} \to \R$, we have the mapping(s) $t \mapsto (x_{1}(t),\ldots,x_{m}(t)) \mapsto f(x_{1}(t),\ldots,x_{m}(t))$. If we call the first mapping $\eta(t)$, and second mapping $(f(\eta(t)))$, then
    \begin{align}
        \frac{df}{dt} = \frac{\partial f}{\partial x_{1}}\frac{dx_{1}}{dt} + \cdots + \frac{\partial f}{\partial x_{m}}\frac{dx_{m}}{dt} = \sum_{i=1}^{m} \frac{\partial f}{\partial x_{i}} \frac{dx_{i}}{dt}.
    \end{align}
\end{example}

\begin{example}
    Suppose $f(x,y,z) = xy^{2}z$ with $x = t$, $y = e^{t}$, and $z = 1+t$. Then
    \begin{align}
        \frac{df}{dt} = \frac{\partial f}{\partial x} \frac{dx}{dt} + \frac{\partial f}{\partial y} \frac{dy}{dt} + \frac{\partial f}{\partial z} \frac{dz}{dt} = y^{2}z + 2xyze^{t} + xy^ = e^{2t}((1+2t)+2t(1+t)).
    \end{align}
    Of course, in this example, it would have been preferable to substitute back in the variables in terms of $t$ and then derivating. This may not always be the case, especially in abstract computations.
\end{example}

\begin{example}
    Suppose $g(z,w) = f(x(z,w),y(z,w))$. Making all the necessary assumptions, we have
    \begin{align}
        \frac{\partial g}{\partial z} = \frac{\partial f}{\partial x} \frac{\partial x}{\partial z} + \frac{\partial f}{\partial y} \frac{\partial y}{\partial z}.
    \end{align}
\end{example}

\begin{example}
    We introduce the idea of \eax{polar coordinates}. $g(y,x)$ can be written as $f(r,\theta)$ where $r = \sqrt{x^{2}+y^{2}}$ and $\theta = \tan^{-1}(\frac{y}{x})$. The converse may also be done. Here,
    \begin{align}
        \frac{\partial g}{\partial x} = \frac{\partial g}{\partial r} \frac{\partial r}{\partial x} + \frac{\partial g}{\partial \theta} \frac{\partial \theta}{\partial x}.
    \end{align}
\end{example}

\begin{example}
    Suppose $z = z(u,v)$ with $u = x^{2}y$ and $v = 3x + 2y$. Then
    \begin{align}
        \frac{\partial z}{\partial y} = \frac{\partial z}{\partial u} \frac{\partial u}{\partial y} + \frac{\partial z}{\partial v} \frac{\partial v}{\partial y} = \frac{\partial z}{\partial u} x^{2} + \frac{\partial z}{\partial v} \cdot 2.
    \end{align}
    One can reuse the chain rule and think of double partial differentiation too as
    \begin{align}
        \frac{\partial^{2}y}{\partial y^{2}} = x^{2}\frac{\partial}{\partial y} \left( \frac{\partial z}{\partial u} \right) + 2 \frac{\partial}{\partial y} \left( \frac{\partial z}{\partial v} \right).
    \end{align}
\end{example}
\noindent \textit{August 18th.}

The Laplacian is introduced.

\begin{definition}
    For $u \in C^{2}(O_{n})$, the \eax{Laplacian} of $u$ is defined as
    \begin{align}
        \Delta u \defeq \nabla \circ \nabla u = \ip{\frac{\partial}{\partial x_{1}}, \ldots, \frac{\partial}{\partial x_{n}}} \cdot \ip{u_{x_{1}},\ldots,u_{x_{n}}} = \sum_{j=1}^{n} \frac{\partial^{2} u}{\partial x_{j}^{2}}.
    \end{align}
\end{definition}

\begin{example}
    Let $x = r \cos \theta$ and $y = r \sin \theta$ and let $u \defeq u(x,y)$. The Laplacian of $u$ is $\frac{\partial^{2}u}{\partial x^{2}} + \frac{\partial^{2}u}{\partial y^{2}}$. Now,
    \begin{align}
        \frac{\partial u}{\partial r} = \frac{\partial u}{\partial x} \frac{\partial x}{\partial r} + \frac{\partial u}{\partial y} \frac{\partial y}{\partial r} = \frac{\partial u}{\partial x} \cos \theta + \frac{\partial u}{\partial y} \sin \theta.
    \end{align}
    In a case where $u$ is considered in its polar coordinates, the Laplacian of $u$ is also written in its polar coordinate form.
\end{example}

\section{Extremum and Critical Points}

Of course, one can extend the idea of the maximum (minimum) value of a function into higher dimensions.

\begin{definition}
    Let $S_{n} \subseteq \R^{n}$ and let $a \in S_{n}$ be an interior point. We say $f:S_{n} \to \R$ attains a \eax{local maximum} at $a$ if there exists $r > 0$ such that $f(x) \leq f(a)$ for all $x \in B_{r}(a) \subseteq S_{n}$. Similarly, it attains a \eax{local minimum} at $a$ if there exists $r > 0$ such that $f(x) \geq f(a)$ for all $x \in B_{r}(a) \subseteq S_{n}$.

    $a$ is, instead, termed a \eax{saddle point} of $f$ if for all $r > 0$ satisfying $B_{r}(a) \subseteq S_{n}$, there exist $h_{1},h_{2} \in B_{r}(a)$ such that $f(h_{1}) > f(a)$ and $f(h_{2}) < f(a)$.
\end{definition}

Similarly, one has critical points.

\begin{definition}
    $a \in S_{n}$ is called a \eax{critical point} of $f:S_{n} \to \R$ if $\nabla f(a) = 0$. In other words, $f_{x_{i}}(a) = 0$ for all $i = 1,2,\ldots,n$.
\end{definition}

\begin{theorem}
    Suppose $f:O_{n} \to \R$ is differentiable at $a \in O_{n}$. If $a$ is a local extremum, then $\nabla f(a) = 0$.
\end{theorem}

\begin{proof}
    We claim that $f_{x_{i}}(a) = 0$ for each $i$. Fix $i$ and define $\varphi_{i}(t) = f(a_{1},\ldots,a_{i-1},t,a_{i+1},\ldots,a_{n})$ for $t \in (a_{i}-\varepsilon,a_{i}+\varepsilon)$. This implies that $a_{i}$ is a local extremum of $\varphi_{i}$ giving $\frac{d\varphi_{i}}{dt}(a_{i}) = 0$. Thus, $f_{x_{i}}(a) = 0$.
\end{proof}

To find these extrema, one would find the critical points and apply the second derivative test in the one variable case. In higher dimensions, we do the same; however, one needs to formulate the idea of a `second derivative test' here.

\begin{definition}
    Given $f \in C^{2}(O_{n})$ with $a \in O_{n}$. The \eax{Hessian matrix}, or Hessian, of $f$ at $a$ is defined as
    \begin{align}
        H_{f}(a) = \left[ \frac{\partial^{2} f(a)}{\partial x_{i} \partial x_{j}} \right]_{n \times n}
    \end{align}
    Note that $H_{f}$ is a symmetric matrix.
\end{definition}

\begin{example}
    Consider $f:\R^{2} \to \R$ defined as $f(x,y) = \sin^{2}x + x^{2}y + y^{2}$. Then $f_{x} = \sin 2x + 2xy$, $f_{y} = x^{2}+2y$, and $f_{xy} = 2x$. The Jacobian in this case is
    \begin{align}
        J_{f} = \begin{bmatrix}
            \sin 2x + 2xy & x^{2}+2y
        \end{bmatrix}
    \end{align}
    and the Hessian is
    \begin{align}
        H_{f} = \begin{bmatrix}
            2 \cos 2x + 2y & 2x \\ 2x & 2
        \end{bmatrix}.
    \end{align}
\end{example}

\begin{definition}
    Given $A = (a_{ij})_{n \times n} \in M_{n}(\R)$, we define $Q_{A}(x) = x^{t}Ax = \ip{Ax,x} = \sum_{i,j =1}^{n} a_{ij}x_{i}x_{j}$ for all $x \in \R^{n}$. A \eax{quadratic form} is $Q_{A}$ when $A$ is symmetric.
\end{definition}

Thus, $x^{t}H_{f}(a)x$ is a quadratic form which is also a homogenous polynomial of degree 2. One calls a symmetric matrix $A \in M_{n}(\R)$ a \eax{positive definite matrix} if $h^{t}Ah > 0$ for all $h \in \R^{n} \setminus \{0\}$ and, likewise, a \eax{negative definite matrix}. It is termed a \eax{positive semidefinite matrix} if $h^{t}Ah \geq 0$ for all $h \in \R^{n}$ and, likewise, a \eax{negative semidefinite matrix}. It is called a \eax{indefinite matrix} if it satisfies none of the above conditions.

\begin{theorem}
    Consider a symmetric matrix $A = \begin{bmatrix}
        a & b \\ b & c
    \end{bmatrix} \in M_{2}(\R)$. Then
    \begin{enumerate}
        \item $A$ is positive definite if and only if $a > 0$ and $ac - b^{2} > 0$,
        \item $A$ is negative definite if and only if $a < 0$ and $ac - b^{2} > 0$,
        \item $A$ is indefinite if and only if $ac - b^{2} < 0$.
    \end{enumerate}
\end{theorem}

\begin{lemma}
    Let $a \in O_{n}$, and suppose $A(x) = \begin{bmatrix}
        a_{11}(x) & a_{12}(x) \\ a_{21}(x) & a_{22}(x)
    \end{bmatrix} \in M_{2}(\R)$ is a symmetric matrix for all $x \in O_{n}$. Suppose $A$ is continuous at $a$, that is, the functions $a_{ij}$ are continuous at $a$ for all pairs $i,j$. If $A(a)$ is positive definite, then $A$ is positive definite in a neighbourhood of $a$.
\end{lemma}

\begin{proof}
    $A(a)$ is positive definite implies that $a_{11}(a) > 0$ and $a_{11}(a) a_{22}(a) - a_{12}(a)^{2} > 0$. As $x \mapsto a_{ij}(x)$ is continuous at $a$, there exists a neighbourhood of $a$ such that $a_{11}(x) > 0$ and $a_{11}(x)a_{22}(x) - a_{12}(x)^{2} > 0$ for all $x$ in that neighbourhood. Therefore, $A(x)$ is positive definite in a neighbourhood of $a$.
\end{proof}

Recall Taylor's polynomial and approximation; given a function $f \in C^{k}(I)$ with $a \in I \subseteq \R$, we would have
\begin{align}
    p_{a,k}(a+h) = \sum_{m=0}^{k} \frac{f^{(m)}(a)}{m!}h^{m}
\end{align}
where $a+h \in I$. In terms of $x$, it was
\begin{align}
    p_{a,k}(x) = \sum_{m=0}^{k} \frac{f^{(m)}(a)}{m!}(x-a)^{m}
\end{align}
One would also have Taylor's theorem, where if the above $f$ were a $C^{k+1}(I)$-function, then for all $x \in I$ there exists a $\zeta \equiv \zeta(x,a)$ such that
\begin{align}
    f(x) = p_{a,k}(x) + \frac{f^{(k+1)}(\zeta)}{(k+1)!}(x-a)^{k+1}.
\end{align}
This theorem extends to higher dimensions.

\begin{theorem}[\eax{Taylor's theorem}]
    Let $a \in O_{n}$ where $O_{n}$ is a convex set. Also suppose $f:O_{n} \to \R$ is a $C^{k+1}$-function. If $h \in O_{n}$ and $a+h \in O_{n}$, then
    \begin{align}
        f(a+h) = \sum_{\abs{\alpha} \leq k} \frac{\partial^{\alpha}f(a)}{\alpha !} h^{\alpha} + \gamma_{a,k}(h)
    \end{align}
    where
    \begin{align}
        \gamma_{a,k}(h) = \sum_{\abs{\alpha} = k+1} \frac{\partial^{\alpha}f(a+ch)}{\alpha !} h^{\alpha}
    \end{align}
    for some $c \in (0,1)$.
\end{theorem}

We clear up some notation. Here, $\alpha = (\alpha_{1},\ldots,\alpha_{n}) \in \N^{n}$ is termed a multi-index, with $\abs{\alpha} = \sum \alpha_{i}$. The notation $\partial^{\alpha}f(a)$ is shorthand for $\frac{\partial^{\abs{\alpha}}f(a)}{\partial x_{1}^{\alpha_{1}} \cdots \partial x_{n}^{\alpha_{n}}}$. Also, $\alpha! = \alpha_{1}! \cdots \alpha_{n}!$, and finally, $h^{\alpha} = h_{1}^{\alpha_{1}} \cdots h_{n}^{\alpha_{n}}$.

\begin{proof}
    Fix $a,a+h \in O_{n}$. Consider the mappings $t \mapsto a+th \mapsto f(a+th)$, with $\eta:[0,1] \to \R$ defined as $\eta(t) = f(a+th)$. Note that $\eta$ is also a $C^{k+1}$-function, and $\eta'(t) = \nabla f(a+th) \circ h = \sum_{i=1}^{n} h_{i}f_{x_{i}}(a+th)$. The double derivatives is
    \begin{align}
        \eta''(t) = \sum_{i=1}^{n} h_{i}f_{x_{i}}(a+th) = \nabla f_{x_{i}}(a+th) \circ h = \sum_{i,j=1}^{n} h_{i}h_{j}f_{x_{i}x_{j}}(a+th).
    \end{align}
    Call $\nabla \circ h = \sum_{i=1}^{n} h_{i} \frac{\partial}{\partial x_{i}}$. Therefore, $\eta'(t) = (\nabla \circ h) f(a+th)$ and $\eta''(t) = (\nabla \circ h)^{2}f(a+th)$; we have
    \begin{align}
        \eta^{(m)}(t) = (\nabla \circ h)^{m}f(a+th)
    \end{align}
    for all $0 \leq m \leq k+1$. Note that 
    \begin{align}
        (\nabla \circ h)^{m} = \sum_{\abs{\alpha}=m} \frac{\partial^{\alpha}}{\alpha!}h^{\alpha}.
    \end{align}
    We apply the one-dimensional Taylor's theorem to get
    \begin{align}
        \eta(1) = \eta(0) + \eta'(0) + \frac{1}{2!}\eta''(0) + \cdots + \frac{1}{k!}\eta^{(k)}(0) + \frac{1}{(k+1)!} \eta^{(k+1)} (c)
    \end{align}
    for some $c \in (0,1)$. Expanding in terms of $f$ gives the desired result.
\end{proof}

\noindent \textit{August 22nd.}

\begin{theorem}[The \eax{min-max theorem}]
    Let $f \in C^{2}(O_{2})$ with $a \in O_{2}$, and suppose $Df(a) = 0$. Consider the Hessian matrix $H_{f}(a) = \begin{bmatrix}
        f_{xx}(a) & f_{xy}(a) \\ f_{xy}(a) & f_{yy}(a)
    \end{bmatrix}$. Then
    \begin{enumerate}
        \item $f(a)$ is a local maximum if $f_{xx}(a) < 0$ and $\det H_{f}(a) > 0$,
        \item $f(a)$ is a local minimum if $f_{xx}(a) > 0$ and $\det H_{f}(a) > 0$,
        \item $f(a)$ is a saddle point if $\det H_{f}(a) < 0$.
    \end{enumerate}
\end{theorem}

\begin{proof}
    We show only for the first part; the rest follow a similar logic. Let $f_{xx}(a) > 0$ and $\det H_{f}(a) > 0$. Then $f_{xx}(x,y) > 0$ for all $(x,y)$ in a neighbourhood of $a$. Now, $\det H_{f}(x,y) = f_{xx}(x,y)f_{yy}(x,y)-f_{xy}(x,y)^{2}$ implies that $(x,y) \mapsto \det H_{f}(x,y)$ is a continuous function. Thus $H_{f}(a) > 0$ will force $H_{f}(x,y) > 0$ for all $(x,y)$ in a neighbourhood of $a$. Thus, for some $r > 0$, $f_{xx} > 0$ and $\det H_{f} > 0$ in $B_{r}(a)$. We conclude that $H_{f}$ is positive definite on $B_{r}(a)$. Thus, for all $a + h \in B_{r}(a)$, we use Taylor's polynomial to get
    \begin{align}
        f(a+h)-f(a) = \frac{1}{2}h^{t}H_{f}(a+ch)h > 0 \implies f(a+h) > f(a) \text{ for all } a+h \in B_{r}(a).
    \end{align}
    Thus, $f(a)$ is a local minimum.
\end{proof}

\begin{example}
    We find the critical points and discuss the nature of the function $f(x,y) = x^{3}-6x^{2}-8y^{2}$. We have
    \begin{align}
        f_{x} = 3x^{2}-12x,\;f_{y}=-16y,\;f_{xx}=6x-12,\;f_{yy}=-16,\;f_{xy}=0.
    \end{align}
    Setting $f_{x} = 0 = f_{y}$ gives $x = 0,4$ and $y = 0$; the critical points are $(0,0)$ and $(4,0)$. Moreover, $H_{f}(x,y) = \begin{bmatrix}
        6x-12 & 0 \\ 0 & -16
    \end{bmatrix}$. At $(0,0)$, $\det H_{f}(0,0) > 0$ and $f_{xx}(0,0) < 0$, so $f(0,0)$ is a local maximum. At $(4,0)$, $\det H_{f}(4,0) < 0$, so $f(4,0)$ is a saddle point.
\end{example}

\section{Compact Sets}

The theory of compact sets proves to be useful in various areas of analysis.

\begin{definition}
    A set $K \subseteq \R^{n}$ is termed a \eax{compact set} if every sequence from $K$ has a convergent subsequence whose limit lies in $K$.
\end{definition}

\begin{remark}
    \begin{enumerate}
        \item Compact sets are bounded sets; there exists an $M > 0$ such that $\norm{x} \leq M$ for all $x \in K$. If it were not bounded, then for every $m \in \N$, one could find $x_{m} \in K$ such that $\norm{x_{m}} > m$. The sequence $\{x_{m}\}$ does not have a convergent subsequence.
        \item Compact sets are closed sets; let $K$ be compact and let $\{x_{m}\} \subseteq K$ converge to $x$. Since $K$ is compact, simply pick the subsequence $x_{m_{k}} = x_{m}$, which will force the limit point $x$ to lie in $K$ showing that $K$ is closed.
    \end{enumerate}
\end{remark}

One then naturally asks whether every closed and bounded subset is compact.

\begin{theorem}
    The box $K \defeq \Pi_{i=1}^{n} [a_{i},b_{i}]$ is compact.
\end{theorem}
\begin{proof}
    Pick $\{x_{m}\} \subseteq K$. Look at sequence of the $i^{\text{th}}$ projection of the $x_{m}$'s as $\{\Pi_{i}(x_{m})\} \subseteq [a_{i},b_{i}]$. By the one dimensional Bolzano-Weierstrass theorem, we can find a convergent subsequence such that $\Pi_{i}(x_{m_{l}}) \to \alpha_{i}$. Pick the intersection of all these convergent subsequences, the resulting subsequences will converge to $\alpha = (\alpha_{1},\ldots,\alpha_{n}) \in K$.
\end{proof}

\begin{theorem}
    Let $K \subseteq \R^{n}$. Then $K$ is compact if and only if $K$ is closed and bounded.
\end{theorem}
\begin{proof}
    We have already shown the forward direction. For the converse implication, since $K$ is bounded, there exists $M > 0$ such that $K \subseteq [-M,M]^{n}$. Pick any $\{x_{m}\} \subseteq K$. As $\{x_{m}\}$ is also a subset of the box $[-M,M]^{n}$, a compact set, there exists $x_{m_{t}}$ such that it converges to $x \in [-M,M]^{n}$. But $K$ is closed forcing $x \in K$. Thus, $K$ is compact.
\end{proof}

\begin{theorem}
    The continuous image of a compact set is compact.
\end{theorem}
\begin{proof}
    Let $f:O_{n} \to \R$ be continuous with $K \subseteq O_{n}$, a compact set. To show that $f(K)$ is compact, pick $\{y_{m}\} \subseteq f(K)$ which results in $y_{m} = f(x_{m})$ for some $x_{m} \in K$. $K$ is compact, so there exists a convergent subsequence $x_{m_{t}} \to x \in K$. By continuity of $f$, we have $f(x_{m_{t}}) \to f(x)$. Thus, $f(K)$ is compact.
\end{proof}

\begin{theorem}[The \eax{extreme value theorem}]
    Let $f:K \to \R$, where $K$ is a compact set. Then, there exist $a,b \in K$ such that $f(a) \leq f(x) \leq f(b)$ for all $x \in K$.
\end{theorem}
\begin{proof}
    Note that $f(K) \subseteq \R$ is compact. So pick $m \defeq \inf_{x \in K} f(x)$ and $M \defeq \sup_{x \in K} f(x)$. Since $f(K)$ is compact, there exist $a,b \in K$ such that $f(a) = m$ and $f(b) = M$. Thus, for all $x \in K$, we have $m \leq f(x) \leq M$.
\end{proof}

\begin{lemma}
    Let $f:O_{n} \to \R^{n}$ be a $C^{1}$-function, with $O_{n}$ convex. Suppose $\sup_{x \in O_{n}} \abs{\frac{\partial f_{i}}{\partial x_{j}}(a)} \leq M$ for all $i,j = 1,2,\ldots,n$. Then
    \begin{align}
        \norm{f(a)-f(y)} \leq n^{2}M \norm{x-y} \text{ for all } x,y \in O_{n}.
    \end{align}
\end{lemma}
\begin{proof}
    Here, using the mean value theorem, we have
    \begin{align}
        \norm{f(x)-f(y)} \leq \sum_{i=1}^{n} \abs{f_{i}(x)-f_{i}(y)} = \sum_{i=1}^{n} \abs{\sum_{j=1}^{n}\frac{\partial f_{i}}{\partial x_{j}}(c_{i}) \cdot (x_{j}-y_{j})}
    \end{align}
    for $c_{i} \in L_{x,y}$. One then bounds as
    \begin{align}
        \norm{f(x)-f(y)} \leq M \cdot \sum_{i=1}^{n} \abs{\sum_{j=1}^{n}(x_{j}-y_{j})} \leq M \times \sum_{i=1}^{n} \sum_{j=1}^{n} \abs{x_{j}-y_{j}} = n^{2}M \norm{x-y}.
    \end{align}
\end{proof}

\begin{theorem}[The \eax{inverse function theorem}]
    Let $f:O_{n} \to \R^{n}$ be a $C^{1}$-function and $a \in O_{n}$. Suppose $f'(a)$ is invertible. Then there exist open set $V$ and $W$, with $a \in V$ and $f(a) \in W$, such that $f|_{V} : V \to W$ is a bijection and $(f|_{V})^{-1}$ is differentiable on $W$. Also, $(f^{-1}(y))' = [f'(f^{-1}(y))]^{-1}$ for all $y \in W$.
\end{theorem}
\begin{proof}
    Without the loss of generality, assume $Df(a) = I$ (This can be done easily by setting $L\defeq (Df(a))^{-1}$ implying $L \circ f$ is differentiable and $(L \circ f)'(a) = L'(f(a))f'(a) = L L^{-1} = I$). Note that there exists a closed box $U$ with $a \in U$ such that $f(x) \neq f(a)$ for all $x \in U \setminus \{a\}$. As $x \mapsto \det J_{f}(x)$ is continuous, and $\det J_{f}(a) = 1 \neq 0$, we conclude $\det J_{f}(x) \neq 0$ for all $x \in U$. By continuity of $\frac{\partial f_{i}}{\partial x_{j}}$ at $a$,
    \begin{align}
        \abs{\frac{\partial f_{i}}{\partial x_{j}}(x) - \frac{\partial f_{i}}{\partial x_{j}}(a)} < \frac{1}{2n^{2}} \text{ for all } x \in U,\; i,j = 1,2,\ldots,n.
    \end{align}
    We make a bold claim:
    \begin{align}
        \norm{f(x)-f(y)} \geq \frac{1}{2} \norm{x-y} \text{ for all } x,y \in U.
    \end{align}
    To show this claim, set $g(x) = f(x) - x$ for all $x \in U$. $g$ is then a $C^{1}$-function and $g'(x) = f'(x)-I_{n}$ shows $\frac{\partial g_{i}}{\partial x_{j}}(a) = \frac{\partial f_{i}}{\partial x_{j}}(a) - \frac{\partial f_{i}}{\partial x_{j}}(a)$, or
    \begin{align}
        \abs{\frac{\partial g_{i}}{\partial x_{j}}(x)} < \frac{1}{2n^{2}} \text{ for all } x \in U,\; i,j = 1,2,\ldots,n.
    \end{align}
    By the previous lemma, $\norm{g(x)-g(y)} \leq \frac{1}{2} \norm{x-y}$. Therefore,
    \begin{align}
        \norm{x-y} -\norm{f(x)-f(y)} \leq \norm{(x-y)-(f(x)-f(y))} \leq \frac{1}{2}\norm{x-y} \implies \norm{f(x)-f(y)} \geq \frac{1}{2} \norm{x-y}.
    \end{align}
\end{proof}