\chapter{INTRODUCTION TO $\R^{n}$}

\section{Placeholder}

We begin with a definition.
\begin{definition}
    The space $\R^{n}$ is defined as $\R \times \R \times \cdots \times \R$ ($n$ times) = $\{(x_{1},x_{2},\ldots,x_{n}) : x_{i} \in \R\}$.
\end{definition}
In the context of analysis, we will talk about open sets, closed sets, sequences, compact sets, and connected sets. In contrast, algebra considers $\R^{n}$ as a vector space with the operators $+$ and $\cdot$. Combining both these aspects results in the study of analysis of several variables. In this course, we will mainly focus on dealing with functions of the form $f: \R^{n} \to \R^{m}$, and talk about their properties such as continuity, differentiability, and integrability.

\subsection{Algebraic and Analytic Structure}
We note that $\R^{n}$ is also an inner product space with the following properties:
\begin{itemize}
    \item $\ip{x,y} = \sum_{i=1}^{n} x_{i}y_{i}$ for all $x,y \in \R^{n}$.
    \item The set $\{e_{i}\}_{i=1}^{n}$ consisting of the unit vectors is an orthonormal basis for $\R^{n}$.
    \item The simplest maps from $\R^{n}$ to $\R^{m}$ are linear maps that send lines to lines.
\end{itemize}

\begin{example}
    Suppose the function $f$ is a linear map from $\R$ to $\R$. This implies that $f(x) = xf(1)$ for all $x \in \R$. Thus, $f(x) = cx$ for all $x \in \R$, where $c$ is a constant. Conversely, if $c \in \R$, then $x \mapsto cx$ is a linear map. Therefore, we conclude that $\{f: \R \to \R, \text{linear}\} \leftrightarrow \R$, with a possible bijection given by $f \mapsto f(1)$.
\end{example}
In the above example, we note that 1 is not special; we could simply fix any $\alpha \in \R\setminus\{0\}$, and notice that $f(x) = \frac{x}{\alpha} f(\alpha)$ for all $x \in \R$. Here, replacing $f(1)$ by $f(\alpha)$ is a kind of `change of variable'.
\begin{remark}
    Let $L:\R^{n} \to \R^{m}$ be a linear map. Then, $Le_{j} = \sum_{i=1}^{m} a_{ij} e_{i}$ for all $j = 1,2,\ldots,n$. We may write $L$ as $(a_{ij})_{m \times n} \in M_{m \times n}(\R)$, the set of all $m \times n$ matrices with real entries.
\end{remark}

Coming to the analysis side, there is a need for defining a distance between points in $\R^{n}$. Previously, we have seen that the \eax{norm} may be given as $\norm{x} = \sqrt{\sum_{i=1}^{n} x_{i}^{2}}$ for all $x \in \R^{n}$. We can use this norm to define our required distance function.

\begin{definition}
    The \eax{distance} function between two points $d:\R^{n} \times \R^{n} \to \R_{\geq 0}$ is defined as $d(x,y) = \norm{x-y} = \left( \sum_{i=1}^{n} (x_{i}-y_{i})^{2} \right)^{1/2}$ for all $x,y \in \R^{n}$.
\end{definition}
For $n = 1$, we note that $d(x,y) = \abs{x-y}$, from the previous analysis courses. $\R^{n}$ equipped with the function $d$ is called a \eax{metric space}. Coming to the properties of the inner product, we have
\begin{itemize}
    \item $\norm{x} = \ip{x,x}^{1/2}$ for all $x \in \R^{n}$.
    \item $\ip{x,y} = \ip{y,x}$ for all $x,y \in \R^{n}$.
    \item The function $\ip{,}$ is linear with respect to the first and second arguments.
\end{itemize}

We also have the important Cauchy-Schwarz inequality.
\begin{theorem}[\eax{Cauchy-Schwarz inequality}]
    For all $x,y \in \R^{n}$, we have $\abs{\ip{x,y}} \leq \norm{x} \norm{y}$.
\end{theorem}
\begin{proof}
    Note that 
    
    \begin{align}
        0 \leq \sum_{i=1}^{n} \sum_{j=1}^{n} (x_{i}y_{j}-x_{j}y_{i})^{2} &= 2\left(\sum_{i,j} x_{i}^{2}y_{j}^{2}- \sum_{i,j} x_{i}x_{j}y_{i}y_{j}\right) = 2 \left( \norm{x}^{2}\norm{y}^{2} - \ip{x,y}^{2} \right) \\ \implies \abs{\ip{x,y}} &\leq \norm{x}\norm{y}.
    \end{align}
\end{proof}
We note that equality occurs if and only if the first quantity in the above equation is zero, \textit{i.e.}, if and only if $x_{i}y_{j} = x_{j}y_{i}$ for all $i,j$, or $\frac{x_{i}}{y_{i}} = \frac{x_{j}}{y_{j}}$ for all $i,j$ showing that $x$ and $y$ are linearly dependent.
\begin{corollary}[\eax{Triangle inequality}]
    For all $x,y \in \R^{n}$, we have $\norm{x+y} \leq \norm{x} + \norm{y}$.
\end{corollary}
\begin{proof}
    We have
    \begin{align}
        \norm{x+y}^{2} = \ip{x+y,x+y} = \norm{x}^{2} + 2\ip{x,y} + \norm{y}^{2} \leq \norm{x}^{2} + 2\norm{x}\norm{y} + \norm{y}^{2} = (\norm{x}+\norm{y})^{2}
    \end{align}
    where the inequality follows from Cauchy-Schwarz.
\end{proof}
The following will prove to be an important result.
\begin{theorem}
    Let $L:\R^{n} \to \R^{m}$ be a linear map. Then, there exists a $M > 0$ such that $\norm{Lx} \leq M \norm{x}$ for all $x \in \R^{n}$.
\end{theorem}
\begin{proof}
    Rewriting $x$ as $x = \sum_{i=1}^{n} x_{i} e_{i}$, we have
    \begin{align}
        Lx &= \sum_{i=1}^{n} x_{i} Le_{i} \notag \\
        \implies \norm{Lx} &= \norm{\sum_{i=1}^{n} x_{i} Le_{i}} \leq \sum_{i=1}^{n} \abs{x_{i}} \norm{Le_{i}} \leq \norm{x} \sum_{i=1}^{n} \norm{Le_{i}} = \norm{x} M.
    \end{align}
    The first inequality follows from the triangle inequality, and the second from Cauchy-Schwarz. In the last step, $M$ is set to be $\sum_{i=1}^{n} \norm{Le_{i}}$, which is a constant.
\end{proof}
We also term $(\R^{n},d)$ as a Euclidean metric space. There is now a need to define open sets in $\R^{n}$ to talk more about the analysis of several variables.
\begin{definition}
    For $a \in \R^{n}$ and $r > 0$, the \eax{open ball} centred at $a$ of radius $r$ is $B_{r}(a) \defeq \{x \in \R^{n} : d(x,a) < r\}$, the set of all points in $\R^{n}$ that are at a distance less than $r$ from $a$.
\end{definition}
From the notion of open balls, we can define open sets.
\begin{definition}
    A set $S \subseteq \R^{n}$ is said to be an \eax{open set} if for all $x \in S$, there exists an $r > 0$ such that $B_{r}(x) \subseteq S$.
\end{definition}
We now bring the notion of convergence of sequences.
\begin{definition}
    Let $\{x_{m}\} \subseteq \R^{n}$ be a sequence and $x \in \R^{n}$. We say that $\{x_{m}\}$ \eax{converges} to $x$ if for every $\varepsilon > 0$, there exists a natural $N$ such that $\norm{x_{m}-x} < \varepsilon$ for all $m \geq N$.
\end{definition}