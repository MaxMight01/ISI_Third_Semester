\chapter{INTRODUCTION TO $\R^{n}$}

\section{Translation into Higher Dimensions}
\textit{July 21st.}

We begin with a definition.
\begin{definition}
    The space $\R^{n}$ is defined as $\R \times \R \times \cdots \times \R$ ($n$ times) = $\{(x_{1},x_{2},\ldots,x_{n}) : x_{i} \in \R\}$.
\end{definition}
In the context of analysis, we will talk about open sets, closed sets, sequences, compact sets, and connected sets. In contrast, algebra considers $\R^{n}$ as a vector space with the operators $+$ and $\cdot$. Combining both these aspects results in the study of analysis of several variables. In this course, we will mainly focus on dealing with functions of the form $f: \R^{n} \to \R^{m}$, and talk about their properties such as continuity, differentiability, and integrability.

\subsection{Algebraic and Analytic Structure}
We note that $\R^{n}$ is also an inner product space with the following properties:
\begin{itemize}
    \item $\ip{x,y} = \sum_{i=1}^{n} x_{i}y_{i}$ for all $x,y \in \R^{n}$.
    \item The set $\{e_{i}\}_{i=1}^{n}$ consisting of the unit vectors is an orthonormal basis for $\R^{n}$.
    \item The simplest maps from $\R^{n}$ to $\R^{m}$ are linear maps that send lines to lines.
\end{itemize}

\begin{example}
    Suppose the function $f$ is a linear map from $\R$ to $\R$. This implies that $f(x) = xf(1)$ for all $x \in \R$. Thus, $f(x) = cx$ for all $x \in \R$, where $c$ is a constant. Conversely, if $c \in \R$, then $x \mapsto cx$ is a linear map. Therefore, we conclude that $\{f: \R \to \R, \text{linear}\} \leftrightarrow \R$, with a possible bijection given by $f \mapsto f(1)$.
\end{example}
In the above example, we note that 1 is not special; we could simply fix any $\alpha \in \R\setminus\{0\}$, and notice that $f(x) = \frac{x}{\alpha} f(\alpha)$ for all $x \in \R$. Here, replacing $f(1)$ by $f(\alpha)$ is a kind of `change of variable'.
\begin{remark}
    Let $L:\R^{n} \to \R^{m}$ be a linear map. Then, $Le_{j} = \sum_{i=1}^{m} a_{ij} e_{i}$ for all $j = 1,2,\ldots,n$. We may write $L$ as $(a_{ij})_{m \times n} \in M_{m \times n}(\R)$, the set of all $m \times n$ matrices with real entries.
\end{remark}

Coming to the analysis side, there is a need for defining a distance between points in $\R^{n}$. Previously, we have seen that the \eax{norm} may be given as $\norm{x} = \sqrt{\sum_{i=1}^{n} x_{i}^{2}}$ for all $x \in \R^{n}$. We can use this norm to define our required distance function.

\begin{definition}
    The \eax{distance} function between two points $d:\R^{n} \times \R^{n} \to \R_{\geq 0}$ is defined as $d(x,y) = \norm{x-y} = \left( \sum_{i=1}^{n} (x_{i}-y_{i})^{2} \right)^{1/2}$ for all $x,y \in \R^{n}$.
\end{definition}
For $n = 1$, we note that $d(x,y) = \abs{x-y}$, from the previous analysis courses. $\R^{n}$ equipped with the function $d$ is called a \eax{metric space}. Coming to the properties of the inner product, we have
\begin{itemize}
    \item $\norm{x} = \ip{x,x}^{1/2}$ for all $x \in \R^{n}$.
    \item $\ip{x,y} = \ip{y,x}$ for all $x,y \in \R^{n}$.
    \item The function $\ip{,}$ is linear with respect to the first and second arguments.
\end{itemize}

We also have the important Cauchy-Schwarz inequality.
\begin{theorem}[\eax{Cauchy-Schwarz inequality}]
    For all $x,y \in \R^{n}$, we have $\abs{\ip{x,y}} \leq \norm{x} \norm{y}$.
\end{theorem}
\begin{proof}
    Note that 
    
    \begin{align}
        0 \leq \sum_{i=1}^{n} \sum_{j=1}^{n} (x_{i}y_{j}-x_{j}y_{i})^{2} &= 2\left(\sum_{i,j} x_{i}^{2}y_{j}^{2}- \sum_{i,j} x_{i}x_{j}y_{i}y_{j}\right) = 2 \left( \norm{x}^{2}\norm{y}^{2} - \ip{x,y}^{2} \right) \\ \implies \abs{\ip{x,y}} &\leq \norm{x}\norm{y}.
    \end{align}
\end{proof}
We note that equality occurs if and only if the first quantity in the above equation is zero, \textit{i.e.}, if and only if $x_{i}y_{j} = x_{j}y_{i}$ for all $i,j$, or $\frac{x_{i}}{y_{i}} = \frac{x_{j}}{y_{j}}$ for all $i,j$ showing that $x$ and $y$ are linearly dependent.
\begin{corollary}[\eax{Triangle inequality}]
    For all $x,y \in \R^{n}$, we have $\norm{x+y} \leq \norm{x} + \norm{y}$.
\end{corollary}
\begin{proof}
    We have
    \begin{align}
        \norm{x+y}^{2} = \ip{x+y,x+y} = \norm{x}^{2} + 2\ip{x,y} + \norm{y}^{2} \leq \norm{x}^{2} + 2\norm{x}\norm{y} + \norm{y}^{2} = (\norm{x}+\norm{y})^{2}
    \end{align}
    where the inequality follows from Cauchy-Schwarz.
\end{proof}
The following will prove to be an important result.
\begin{theorem}
    Let $L:\R^{n} \to \R^{m}$ be a linear map. Then, there exists a $M > 0$ such that $\norm{Lx} \leq M \norm{x}$ for all $x \in \R^{n}$.
\end{theorem}
\begin{proof}
    Rewriting $x$ as $x = \sum_{i=1}^{n} x_{i} e_{i}$, we have
    \begin{align}
        Lx &= \sum_{i=1}^{n} x_{i} Le_{i} \notag \\
        \implies \norm{Lx} &= \norm{\sum_{i=1}^{n} x_{i} Le_{i}} \leq \sum_{i=1}^{n} \abs{x_{i}} \norm{Le_{i}} \leq \norm{x} \sum_{i=1}^{n} \norm{Le_{i}} = \norm{x} M.
    \end{align}
    The first inequality follows from the triangle inequality, and the second from Cauchy-Schwarz. In the last step, $M$ is set to be $\sum_{i=1}^{n} \norm{Le_{i}}$, which is a constant.
\end{proof}
We also term $(\R^{n},d)$ as a Euclidean metric space. There is now a need to define open sets in $\R^{n}$ to talk more about the analysis of several variables.
\begin{definition}
    For $a \in \R^{n}$ and $r > 0$, the \eax{open ball} centred at $a$ of radius $r$ is $B_{r}(a) \defeq \{x \in \R^{n} : d(x,a) < r\}$, the set of all points in $\R^{n}$ that are at a distance less than $r$ from $a$.
\end{definition}
From the notion of open balls, we can define open sets.
\begin{definition}
    A set $S \subseteq \R^{n}$ is said to be an \eax{open set} if for all $x \in S$, there exists an $r > 0$ such that $B_{r}(x) \subseteq S$.
\end{definition}
We now bring the notion of convergence of sequences.
\begin{definition}
    Let $\{x_{m}\} \subseteq \R^{n}$ be a sequence and $x \in \R^{n}$. We say that $\{x_{m}\}$ \eax{converges} to $x$ if for every $\varepsilon > 0$, there exists a natural $N$ such that $\norm{x_{m}-x} < \varepsilon$ for all $m \geq N$.
\end{definition}

\textit{July 23rd.}

\begin{definition}
    Let $S \subseteq \R^{n}$ and $a \in \R^{n}$. We say that $a$ is a \eax{limit point} of $S$ if $S \cap (B_{r}(a)\setminus\{a\})$ is non-empty for all $r > 0$.
\end{definition}

We introduce more notation; for all $i = 1,2,\ldots,n$, the mapping $\Pi_{i}:\R^{n} \to \R$ is called the \eax{$i^{\text{th}}$ projection} where $\Pi_{i}(x) = x_{i}$. Note that $x = (x_{1},x_{2},\ldots,x_{n}) = (\Pi_{1}(x),\Pi_{2}(x),\ldots,\Pi_{n}(x))$. This notation allows us to formulate the following useful fact a little more neatly.

\begin{theorem}
    Let $\{x_{m}\} \subseteq \R^{n}$ be a sequence and $x \in \R^{n}$. Then $x_{m} \to x$ if and only if $\Pi_{i}(x_{m}) \to \Pi_{i}(x)$ for all $i = 1,2,\ldots,n$.
\end{theorem}
\begin{proof}
    Suppose $x_{m} \to x$. Then for all $\varepsilon > 0$, there exists a natural $N$ such that $\norm{x_{m}-x} < \varepsilon$ for all $N \geq N$. Restating, we have
    \begin{align}
        \sum_{i=1}^{n} (\Pi_{i}(x_{m})-\Pi_{i}(x))^{2} &< \varepsilon^{2} \text{ for all } n \geq N \\
        \implies \text{For all $i$, } \abs{\Pi_{i}(x_{m})-\Pi_{i}(x)} &< \varepsilon^{2} \text{ for all } n \geq N.
    \end{align}
    For the converse, we simply work backwards with $\varepsilon/\sqrt{n}$ as our choice of epsilon.
\end{proof}

For example, the sequence $\{(\frac{1}{n},\frac{1}{2n+3})\}_{n=1}^{\infty}$ converges to $(0,0)$. However, the sequence $\{(\frac{1}{n},n^{2})\}_{n=1}^{\infty}$ does not.

\begin{definition}
    Let $S \subseteq \R^{n}$. $a \in S$ is termed an \eax{interior point} of $S$ if for some $r > 0$, $B_{r}(a) \subseteq S$ holds. Thus, a set $S$ is open if $a$ is an interior point for all $a \in S$. The \eax{interior} of set $S$ is defined as $\intr S \defeq \{a \in S \mid a \text{ is an interior point.}\}$.

    If $a \in \intr(S^{c})$, then $a$ is termed an \eax{exterior point} of $S$. $a$ is termed a \eax{boundary point} if $B_{r}(a)$ meets both $S$ and $S^{c}$ for all $r > 0$. The set of \eax{boundary points} of $S$ is denoted as $\partial S$.
\end{definition}

We also term a set $S \subseteq \R^{n}$ as a \eax{closed set} if $\R^{n} \setminus S$ is open. The following facts will only be stated and will be left as an exercise to the reader:
\begin{itemize}
    \item A set $C \subseteq \R^{n}$ is closed if and only if for all sequences $\{x_{m}\}_{m=1}^{\infty} \subseteq C$ that converge to $x$ implies $x \in C$.
    \item The open ball $B_{r}(a)$ is an open set.
    \item The intersection of an arbitrary collection of closed sets is closed; likewise, the union of an arbitrary collection of open sets is open.
    \item The set $S \subseteq \R^{n}$ is open if and only if $S = \intr S$.
\end{itemize}
Fix $O \subseteq \R^{n}$.
\begin{itemize}
    \item $O$ is open if and only if $O \cap \partial O = \emptyset$.
    \item $O$ is closed if and only if $\partial O \subseteq O$.
\end{itemize}
For $S \subseteq \R^{n}$, we define the \eax{closure} of set $S$ as $\overline{S} = \intr S \cup \partial S$.
\begin{itemize}
    \item $S \subseteq \R^{n}$ is closed if and only if $\overline{S} = S$.
    \item Let $C_{i} \subseteq R$ be closed sets and $O_{i} \subseteq \R$ be open sets, for $i = 1,2,\ldots,n$. Then $C_{1} \times C_{2} \times \cdots \times C_{n} \subseteq \R^{n}$ is a closed set, and $O_{1} \times O_{2} \times \cdots \times O_{n} \subseteq \R^{n}$ is an open set.
    \item The \eax{$n$ dimensional unit sphere} $S^{n-1} \defeq \{x \in \R^{n} : \norm{x} = 1\}$ is closed in $\R^{n}$.
\end{itemize}

\begin{definition}
    For $S \subseteq \R^{n}$ and $a \in \R^{n}$, $a$ is termed an \eax{isolated point} if $a$ is \textit{not} a limit points; that there exists an $r > 0$ such that $S \cap (B_{r}(a) \setminus \{a\}) = \emptyset$.
\end{definition}

With the pesky definitions and translation of one dimensional concept into being defined over several variables, we come to limits and continuity.

\section{Limits and Continuity}
Recall that given $f:(a,b)\setminus\{c\} \to \R$, we say that $\lim_{x \to c}f(x) = b$ if for every $\varepsilon  0$, there exists a $\delta > 0$ such that $\abs{f(x)-b} < \varepsilon$ for all $x$ satisfying $0 < \abs{x-c} < \delta$. Note that in this definition of the limit, we have $f(x) \in B_{\varepsilon}(b)$ and $x \in B_{\delta}(c) \setminus \{c\}$; this can easily be rewritten as $f(B_{\delta}(c)\setminus\{c\}) \subseteq B_{\varepsilon}(b)$. However, for our definition we would not require $f$ to be defined on an open set. We defind it over any arbitrary set.

\begin{definition}
    Let $a \in S \subseteq \R^{n}$ be a limit point of $S$ and let $f:S\setminus\{a\} \to \R^{m}$ be a function and $b \in \R^{m}$. We say $\lim_{x \to a} f(x) = b$ if for every $\varepsilon > 0$, there exists a $\delta > 0$ such that $f((B_{\delta}(a)\setminus\{a\}) \cap S) \subseteq B_{\varepsilon}(b)$. Again, this is equivalent to saying that $\norm{f(x)-b} < \varepsilon$ for all $x \in S\setminus\{a\}$ satisfying $\norm{x-a} < \delta$.
\end{definition}

It is important to get accustomed to the definition that works with open balls.
\begin{remark}
    In the above definition, if we instead write $x - a = h$, then $\lim_{x \to a} f(x) = b$ is equivalent to saying that for every $\varepsilon > 0$, there exists a $\delta > 0$ such that $\norm{f(a+h)-b} < \varepsilon$ for all $\norm{h} < \delta$. We can further rewrite to get the usual notation of
    \begin{align}
        \lim_{\norm{h} \to 0} \norm{f(a+h)-b} = 0.
    \end{align}
    Note that the above limit is in the real numbers, making it eaiser to deal with.
\end{remark}

A notion of continuity also comes in handy.
\begin{definition}
    For $S \subseteq \R^{n}$, let $f:S \to R^{m}$ with $a \in S$. We say $f$ is \eax{continuous} at $a$ if $\lim_{x \to a} f(x) = f(a)$. In other words, for every $\varepsilon > 0$, there exists a $\delta > 0$ such that
    \begin{align}
        \norm{f(x)-f(a)} < \varepsilon \text{ for all } x  \in S \text{ satisfying } \norm{x-a} < \delta
    \end{align}
    or
    \begin{align}
        f(B_{\delta}(a) \cap S) \subseteq B_{\varepsilon}(f(a)).
    \end{align}
\end{definition}
Note that if $a$ is an isolated point of $S$, then any $f:S \to \R^{m}$ is continuous at $a$ since $f(\{a\}) \subseteq B_{\varepsilon}(f(a))$ holds true, trivially.

\begin{remark}
    Similar to the previous remark, $f$ is continuous at $a$ if and only if
    \begin{align}
        \lim_{\norm{h} \to 0} \norm{f(a+h)-f(a)} = 0.
    \end{align}
\end{remark}

Functions defined on $S \subseteq \R^{n}$ can be broken down into components; given $f :S \to R^{m}$, define $f_{j} \defeq \Pi_{j} \circ f$ for all $j = 1,2,\ldots,m$. Thus, $f$ can be rewritten as $(f_{1},f_{2},\ldots,f_{m})$. We can conclude that $f$ is continuous at $a \in S$ if and only if $f_{j}:S \to \R$ is continuous at $a$ for all $j = 1,2,\ldots,m$. The proof of this observation is left as an exercise to the reader.

\begin{theorem}
    Let $a \in \R^{n}$ be a limit point of a set $S \subseteq \R^{n}$, with $b \in \R^{m}$ and $f:S \to R^{m}$ a function. Then, the following are equivalent---
    \begin{enumerate}
        \item $\lim_{x \to a}f(x) = b$.
        \item If $\{x_{p}\} \subseteq S\setminus\{a\}$ and $x_{p} \to a$, then $f(x_{p}) \to b$.
        \item $\lim_{x \to a} \norm{f(x)-b} = 0$.
    \end{enumerate}
\end{theorem}
The proof of this theorem is left as an exercise to the reader.

\begin{definition}
    For a set $S \subseteq \R^{n}$, a function $f:S \to \R^{m}$ is termed a continuous function if $f$ is continuous at $a$ for all $a \in S$.
\end{definition}

\begin{theorem}
    Let $f:S \to \R^{m}$ be a function, where $S \subseteq \R^{n}$. The following are, then, equivalent---
    \begin{enumerate}
        \item $f$ is continuous.
        \item For all $a \in S$ and $\{x_{n}\} \subseteq S$ with $x_{n} \to a$, we have $f(x_{n}) \to f(a)$.
        \item For all open sets $O \subseteq \R^{m}$, the set $f^{-1}(O) \subseteq S$ is also open.
        \item For all closed sets $C \subseteq \R^{m}$, the set $f^{-1}(C) \subseteq S$ is also closed.
    \end{enumerate}
\end{theorem}
\begin{proof}
    For 1.~implies 3.~, let $O \subseteq \R^{m}$ be open. Pick some $a \in f^{-1}(O)$. Then, since $f(a) \in O$, there exists $r > 0$ such that $B_{r}(f(a)) \subseteq O$. Also, $f$ is continuous at $a$; for $\frac{r}{2} > 0$, there exists $\delta > 0$ such that 
    \begin{align}
        f(B_{\delta}(a)) \subseteq B_{\frac{r}{2}}(f(a)) \subseteq B_{r}(f(a)) \implies a \in B_{\delta}(a) \subseteq f^{-1}(B_{r}(f(a))) \subseteq f^{-1}(O).
    \end{align}
    Thus, $f^{-1}(O)$ is open. For 3.~implies 1.~, let $a \in S$. Fix $\varepsilon > 0$. Then the set $f^{-1}(B_{\varepsilon}(f(a)))$ is open; there exists a $\delta > 0$ such that $B_{\delta}(a) \subseteq f^{-1}(B_{\varepsilon}(f(a)))$.
\end{proof}
\noindent\textit{July 28th.}

We look at a few examples.
\begin{example}
    Let $f:\R^{2}\setminus\{(0,0)\} \to \R$ be defind as $f(x,y) = \frac{2xy}{x^{2}+y^{2}}$. We find the limit $\lim_{(x,y) \to (0,0)} f(x,y)$. Let us approach from different directions, starting with the line $L_{1}$ defined as $y = 0$ with $x > 0$. Then $\lim_{(x,y) \to (0,0);(x,y) \in L_{1}} f(x,y) = \lim_{(x,y) \to (0,0)} 0 = 0$. However, along the line $L_{2}$ defned as $\{(x,y) \mid x = y\; x,y > 0\}$, we have $\lim_{(x,y) \to (0,0);(x,y) \in L_{2}} f(x,y) = 1$. We conclude that this limit cannot exist. Going along the line $y = mx$ gives several possible values for the limit.
\end{example}
The above method is good only for showing that the limit does not exist; if the limit does exist, we need to use theory.

\begin{example}
    We compute the limit $\lim_{(x,y) \to (0,0)} \frac{x^{3}}{x^{2}+y^{2}}$. Here, we can prove that the limit exists as follows:
    \begin{align}
        \abs{\frac{x^{3}}{x^{2}+y^{2}}} \leq \abs{\frac{x^{3}}{x^{2}}} = \abs{x} \to 0 \implies \lim_{(x,y)\to(0,0)} \frac{x^{3}}{x^{2}+y^{2}} = 0.
    \end{align}
\end{example}

\begin{example}
    We solve the limit $\lim_{(x,y) \to (0,0)} \frac{\sin(x^{2}+y^{2})}{x^{2}+y^{2}}$. Simply rewriting $z = x^{2}+y^{2}$ gives us $z \to 0$ as $(x,y) \to (0,0)$, so $\lim_{z \to 0} \frac{\sin z}{z} = 1$.
\end{example}

Before the next example, we write down a few properties. Let $S \subseteq \R^{n}$, let $f,g:S \to \R$ be functions, and let $a \in \R^{n}$ be a limit point of $S$. Suppose $\lim_{x \to a} f(x) = \alpha$ and $\lim_{x \to b} g(x) = \beta$. Then,
\begin{enumerate}
    \item $\lim_{x \to a} (cf(x)+g(x)) = c \alpha + \beta$ for all $c \in \R$,
    \item $\lim_{x \to a}f(x)g(x) = \alpha\beta$,
    \item $\lim_{x \to a} \frac{f(x)}{g(x)} = \frac{\alpha}{\beta}$, provided that $\beta \neq 0$,
    \item if $f(x) \leq h(x) \leq g(x)$ for all $x \in S$ and if $\alpha = \beta$, then $\lim_{x \to a} h(x)$ exists and equals $\alpha$.
\end{enumerate}

A similar set of corresponding statements also hold true for continuous functions. Note that the function $\Pi_{i}:\R^{n} \to \R$ is also continuous.

\begin{example}
    The function $f(x,y) = \frac{\sin(x^{2}+y^{2})}{x^{2}+y^{2}}$ for $(x,y) \neq (0,0)$ and $f(x,y) = 1$ otherwise is a continuous function since it has been assigned its limit at $(x,y) = (0,0)$. However, the function $f(x,y) = \frac{2xy}{x^{2}+y^{2}}$ for $(x,y) \neq (0,0)$ and $f(x,y) = \alpha$ otherwise is continuous only at $\R^{2}\setminus\{(0,0)\}$ for all $\alpha \in \R$.
\end{example}

\begin{example}
    We look at the continuity of the function
    \begin{align}
        f(x,y) = \begin{cases}
            \frac{xy}{\sqrt{x^{2}+y^{2}}} &\text{ if } (x,y) \neq (0,0),\\
            0 &\text{ if } (x,y) = (0,0).
        \end{cases}
    \end{align}
    Then, we have
    \begin{align}
        \abs{\frac{xy}{\sqrt{x^{2}+y^{2}}}} \leq \frac{1}{2} \cdot \frac{x^{2}+y^{2}}{\sqrt{x^{2}+y^{2}}} = \frac{1}{2}\norm{(x,y)}
    \end{align}
    which shows that $\lim_{(x,y) \to (0,0)} f(x,y) = 0 = f(0,0)$. Thus, $f$ is continuous on $\R^{2}$.
\end{example}

\begin{example}
    Set $\cD = \{(x,y) \in \R^{2} \mid y \neq 0\}$. Since $\cD = (\Pi_{2}^{-1}(\{0\}))^{c}$, $\cD$ is an open set. Define $f:\cD \to \R$ by $f(x,y) = x \sin \frac{1}{y}$. Here, we simply work as
    \begin{align}
        \abs{f(x,y)} = \abs{x \sin \frac{1}{y}} \leq \abs{x} \text{ on } \cD.
    \end{align}
    Thus, the limit becomes $f(x,y)$.
\end{example}

Hereforth, $O_{n}$ denotes an open subset of $\R^{n}$.
\begin{remark}
    Let $(a,b) \in \R^{2}$ be a limit point of $O_{2}$. Suppose $\lim_{(x,y)\to(a,b)} f(x,y)$ exists and equals $\alpha \in \R$. It is natural to ask whether
    \begin{align}
        \alpha = \lim_{y \to b} \lim_{x \to a} f(x,y) = \lim_{x \to a} \lim_{y \to b} f(x,y).
    \end{align}
    For someone looking at multivariable limits for the first time, it is tempting to believe this holds true always. We leave this question unanswered for now, and come back to it later.
\end{remark}

A notion of uniform continuity may also be explored.
\begin{definition}
    A function $f:S \to \R$, for $S \subseteq \R^{n}$, is said to be a \eax{uniformly continuous} function if for every $\varepsilon > 0$ there exists $\delta > 0$ such that
    \begin{align}
        \norm{f(x)-f(y)} < \varepsilon \text{ for all } \norm{x-y} < \delta \text{ in } S.
    \end{align}
\end{definition}

We urge the reader to compute examples for uniformly continuous functions. The exercise of uniform contiuity implying continuity but not the other way around is left as an exercise to the reader.

\section{Differentiability}

As a little convention, for any $f:O_{n} \to \R^{m}$, we prefer to rewrite it as $f = (f_{1},\ldots,f_{n})$ where $f_{j} = \Pi_{j} f$. We now ask the question of derivatives; what does it mean for the derivative of a function $f:O_{n} \to \R^{m}$? What about $f'(a)$ for some $a \in O_{n}$?

For the case of $n=m=1$, we recall that $f$ is termed differentiable at $a$ if and only if $\lim_{h \to 0} \frac{f(a+h)-f(a)}{h}$ exists. If the limit is $\lambda$, then this limit exists if and only if $\lim_{h \to 0} \frac{f(a+h)-f(a)-h\lambda}{h}$, which is really a function of $h$. Thus, the function $h \mapsto \lambda h$ matters the most, that is, $L:\R \to \R$ where $Lh = \lambda h$. So, we can twist our words a little and say that $f$ is differentiable at $a$ if and only if there exists a linear map $L:\R \to \R$ such that 
\begin{align}
    \lim_{h \to 0} \frac{f(a+h) - f(a) - Lh}{h} = 0.
\end{align}
In this case, $f'(a) = L1 = \lambda$. We translate this exact idea into higher dimensions.

\begin{definition}
    Let $f:O_{n} \to \R^{m}$. We say that $f$ is \eax{differentiable} at $a \in O_{n}$ if there exists a linear map $L:\R^{n} \to \R^{m}$, that depends on $a$, such that
    \begin{align}
        \lim_{h \to 0} \frac{1}{\norm{h}}\left( f(a+h) - f(a) - Lh \right) = 0.
    \end{align}
    In this case, we write $Df(a) = L$ and call it the \text{total derivative} of $f$ at $a$. We say $f$ is differentiable on $O_{n}$ if $f$ is differentiable at $a$ for all $a \in O_{n}$.
\end{definition}
Observe that the above limit is equivalent to saying that $\lim_{h \to 0} \frac{1}{\norm{h}}\norm{f(a+h)-f(a)-Lh} = 0$. Note that $Df(a) = L$ is unique. To show this, suppose there exists another linear map $\tilde{L}:\R^{n} \to \R^{m}$ such that $\lim_{h \to 0} \frac{1}{\norm{h}} \norm{f(a+h)-f(a)-\tilde{L}h} = 0$. Let there exist $h_{0} \in \R^{n}$ such that $Lh_{0} \neq \tilde{L}h_{0}$ and $\norm{h_{0}} = 1$. Define $h:\R\to\R^{n}$ by $ht = th_{0}$. Then as $t \to 0$, $ht \to 0$. Therefore,
\begin{align}
    \norm{L(h(t))-\tilde{L}(h(t))} &\leq \norm{f(a+h)-f(a)-Lh(t)} + \norm{f(a+h)-f(a)-\tilde{L}h(t)} \\
    \implies \lim_{t \to 0} \frac{\norm{Lh(t) - \tilde{L}h(t)}}{\norm{h(t)}} &= 0 \implies \lim_{t \to 0} \frac{1}{\abs{t}} \abs{t} \cdot \norm{Lh_{0}-\tilde{L}h_{0}} = 0 \implies Lh_{0} = \tilde{L}h_{0}
\end{align}
which is a contradiction.

\begin{example}
    Let $f:\R^{n} \to \R^{m}$ be a linear map. In this case, $\frac{f(a+h)-f(a)-f(h)}{\norm{h}} \to 0$ as $\norm{h} \to 0$. Thus, $f$ is differentiable at $a$ and $Df(a) = f$ for all $a \in \R^{n}$.
\end{example}

\begin{example}
    Suppose $f:\R^{n} \to \R^{m}$ be defined as $f(x) = c$ for all $x \in \R^{n}$. Then, we simply have $Df(a) = 0$, the null linear mapping.
\end{example}

We now truly ask how to compute $Df(a)$. Observe that $Df(a):\R^{n}\to\R^{m}$ is a linear map. Then, one can represent it as a matrix $Df(a) \in M_{m \times n}(R)$.\\ \\
\textit{July 30th.}

\begin{theorem}
    Let $f:O_{n} \to \R^{m}$ be a function. Then $f$ is differentiable at $a \in O_{n}$ if and only if $f_{i}:O_{n} \to \R$ is differentiable at $a$ for all $i = 1,2,\ldots,n$. Moreover, in this case, 
    \begin{align}
        [Df(a)]_{m \times n} = \begin{bmatrix}
            [Df_{1}(a)]_{1 \times n} \\ \vdots \\ [Df_{m}(a)]_{1 \times n}
        \end{bmatrix}_{m \times n}.
    \end{align}
\end{theorem}

From the above theorem it is clear that $D\Pi_{i}f = \Pi_{i}Df$. We now provide a proof.
\begin{proof}
    For the forward implication, let $f$ be differentiable at $a \in O_{n}$. Set $L \defeq Df(a)$ and $L_{i} \defeq \Pi_{i}Df(a)$. Note that $L_{i}:\R^{n} \to \R$ since $Df(a):\R^{n}\to\R^{m}$ and $\Pi_{i}:\R^{m} \to \R$. Observe
    \begin{align}
        f(a+h) - f(a) - Df(a)h = (\tilde{f}_{1}(h),\tilde{f}_{2}(h),\ldots,\tilde{f}_{m}(h))
    \end{align}
    where $\tilde{f}_{i}(h) = f_{i}(a+h)-f_{i}(a)-L_{i}h$ for $i = 1,2,\ldots,m$. Thus, for all $i = 1,2,\ldots,m$,
    \begin{align}
        \abs{\tilde{f}_{i}(h)} \leq \left( \sum_{j=1}^{m} \abs{\tilde{f}_{j}(h)}^{2} \right)^{1/2} = \norm{f(a+h)-f(a)-Lh}.
    \end{align}
    Dividing by $\norm{h}$ and taking $h \to 0$, we have
    \begin{align}
        \lim_{h \to 0} \frac{\abs{\tilde{f}_{i}(h)}}{\norm{h}} = 0
    \end{align}
    which shows that $f_{i}$ is differentiable with $Df_{i}(a) = L_{i}$ $(=\Pi_{i}Df(a))$.

    For the converse, let $f_{i}$ be differentiable at $a$ for all $i = 1,2,\ldots,m$ and set $L_{i} = Df_{i}(a):\R^{n}\to\R$. Set $L = \begin{bmatrix}
        L_{1} \\ \vdots \\ L_{m}
    \end{bmatrix}:\R^{n} \to \R^{m}$, a linear map. Therefore,
    \begin{align}
        \frac{1}{\norm{h}} \norm{f(a+h)-f(a)-Lh} = \frac{1}{\norm{h}} \left( \sum_{j=1}^{m} \abs{\tilde{f}_{i}(h)}^{2} \right)^{1/2} \to 0.
    \end{align}
    where $\tilde{f}_{i}(h) = f_{i}(a+h) - f_{i}(a) - L_{i}h$ for all $i$.
\end{proof}

\begin{corollary}
    Let $f:O_{1} \to \R^{m}$ be a function. $f$ is, then, differentiable at $a in O_{1}$ if and only if $f_{i}$ is differentiable at $a$ for all $1 \leq i \leq m$. Moreover, in this case,
    \begin{align}
        Df(a) = \begin{bmatrix}
            f_{1}'(a) \\ \cdots \\ f_{m}'(a)
        \end{bmatrix}.
    \end{align}
\end{corollary}
This is just a special case when $n = 1$.

\begin{remark}
    Let $f:O_{n} \to \R^{m}$ be differentiable at $a$. Then $f$ is continuous at $a$.
\end{remark}
\begin{proof}
    We have
    \begin{align}
        \norm{f(x)-f(a)} &\leq \norm{f(x)-f(a)-(Df(a))(x-a)} + \norm{(Df(a))(x-a)} \notag \\
        &\leq \frac{1}{\norm{x-a}} \norm{f(x)-f(a)-(Df(a))(x-a)} \cdot \norm{x-a} + M\norm{x-a} \to 0
    \end{align}
    as $x \to a$. Note that such an $M > 0$ exists because $Df(a)$ is a linear map.
\end{proof}

\subsection{Chain Rule}

To simplify our study of derivatives in highder dimensions, we look at the so called chain rule. This will prove to be a very important tool to study the differentiability of any function of several variables.

\begin{theorem}[The \eax{chain rule}]
    Let $f:O_{n} \to O_{m}$ be differentiable at $a \in O_{n}$, and $g:O_{m} \to \R^{p}$ be differentiable at $b = f(a) \in O_{m}$. Then $g \circ f:O_{n} \to \R^{p}$ is differentiable at $a \in O_{n}$, and
    \begin{align}
        (D g \circ f)(a) = Dg(f(a)) \circ Df(a).
    \end{align}
\end{theorem}

For the proof, we will denote $A \defeq Df(a)$ and $B \defeq Dg(f(a))$, and $b = f(a)$. Moreover, we will write $r_{f}(x) \defeq f(x)-f(a)+Df(a)(x-a)$.

\begin{proof}
    There exists $r_{f}$ in the neighbourhood of $a$ and $r_{g}$ in the neighbourhood of $b$ such that $r_{f}(x) = f(x)-f(a)-A(x-a)$ and $r_{g}(y) = g(y)-g(b)-B(y-b)$. Now set
    \begin{align}
        r(x) = g(f(x)) - g(b) - BA(x-a).
    \end{align}
    We claim that $\lim_{x \to a} \frac{r(x)}{\norm{x-a}} = 0$. We know that $\lim_{x \to a} \frac{\norm{r_{f}(x)}}{\norm{x-a}} = 0 = \lim_{y \to b} \frac{\norm{r_{g}(y)}}{\norm{y-b}}$. Now,
    \begin{align}
        r(x) &= g(f(x))-g(b)-B(A(x-a)) = g(f(x)) - g(b) + B(r_{f}(x)-f(x)+f(a)) \notag\\
        &= \left( g(f(x)) - g(f(a)) - B(f(x)-f(a)) \right) + Br_{f}(x) \notag = r_{g}(f(x)) + Br_{f}(x).
    \end{align}
    We show that both terms on the right hand side, when divided by $\norm{x-a}$, tend to zero. Now,
    \begin{align}
        \frac{\norm{Br_{f}(x)}}{\norm{x-a}} \leq M\frac{\norm{r_{f}(x)}}{\norm{x-a}} \to 0.
    \end{align}
    For the remaining term, we work as follows: for $\varepsilon > 0$ fixed, there exists a $\delta > 0$ such that $\norm{r_{g}(y)} < \varepsilon\norm{y-b}$ for all $0 < \norm{y-b} < \delta$. By continuity of $f$ at $a$, there exists a $\tilde{\delta} > 0$ such that $\norm{f(x)-f(a)} < \delta$ for all $\norm{x-a} < \tilde{\delta}$. Thus, for all $0 < \norm{x-a} < \tilde{\delta}$, we have
    \begin{align}
        \norm{r_{g}(f(x))} &< \varepsilon \norm{f(x)-f(a)} < \varepsilon \norm{r_{f}(x)+A(x-a)} \\
        \implies \norm{r_{g}(f(x))} &< \varepsilon \left( \norm{r_{f}(x)} + \norm{A(x-a)} \right) < \varepsilon \left( \norm{r_{f}(x)} + \tilde{M} \norm{x-a} \right) \notag \\
        \implies \frac{\norm{r_{g}(f(x))}}{\norm{x-a}} &\to 0 \text{ as } x \to a.
    \end{align}
\end{proof}