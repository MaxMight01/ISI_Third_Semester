\chapter{LATIN SQUARES \& DESIGNS}

\section{Latin Squares}

\begin{definition}
    A \eax{latin square} of order $n$ is an $n \times n$ array filled with $n$ distinct symbols, each occuring exactly once in each row and exactly once in each column.
\end{definition}

\eax{Euler's generals problem} in latin squares is as follows. Given $n^{2}$ military personnel, each with a rank and regiment of possible $n$ and $n$ options respectively, the problem is to arrange them in an $n \times n$ array such that any choice of a row or a column gives a group of $n$ personnel with $n$ distinct ranks. If we also require that any choice of a row or a column gives a group of $n$ personnel with $n$ distinct regiments, then it is \textit{not} possible to do so for $n = 6$.

\begin{definition}
    Two latin squares $L_{1},L_{2}$ of order $n$, on the same set of row and column indices, are said to be \eax{orthogonal latin squares} if the $n^{2}$ ordered pairs $(l_{ij}^{(1)},l_{ij}^{(2)})$ for $1 \leq i,j \leq n$ are all distinct.
\end{definition}

Euler's generals problem then becomes to find two orthogonal latin squares of order $6$.

\begin{theorem}
    For integer $n \geq 1$, the maximum number $N(n)$ of pairwise mutually orthogonal latin squares of order $n$ satisfies $N(n) \leq n-1$.
\end{theorem}

\begin{proof}
    The key is to convert this problem into a linear algebra one; Let $L_{1},\ldots,L_{t}$ be mutually orthogonal latin squares of order $n$. Let $S$ be their common symbol set. For $s \in S$ and $1 \leq i \leq t$, define $L_{i,s}$ to be the indicator matrix of the symbol $s$ in the latin square $L_{i}$. Then $L_{i,s}$ is a permutation matrix. If $J$ denotes the matrix will all entries $1$, then
    \begin{align}
        A_{i,s} = L_{i,s} - \frac{1}{n} J
    \end{align}
    has row and column sums $0$. As a small lemma, if $V$ denotes the set of matrices with row sum and column sum $0$, then $\dim(V) = (n-1)^{2}$. This is because we can choose the first $n-1$ rows and $n-1$ columns freely, and the last row and column are determined by the previous ones. Consider the Frobenius inner product on $M_{n}(\R)$ defined by $\ip{A,B} = \tr(A^{T}B)$. Note that $\ip{A_{i,s},J} = 0$. Taking two matrices $A_{i,s}$ and $A_{j,s'}$, we have
    \begin{align}
        \ip{A_{i,s},A_{j,s'}} = \ip{L_{i,s} - \frac{1}{n}J, L_{j,s'} - \frac{1}{n}J} = \ip{L_{i,s},L_{j,s'}} - \frac{1}{n}\ip{J,L_{j,s'}} = 1-1 = 0 \text{ for } i \leq j,\; s,s' \in S.
    \end{align}
    Moreover, $\sum_{s \in S} A_{i,s} = 0$ for each $i$. Thus $A_{i,s} \in V$. For a fixed $i$, we get
    \begin{align}
        \dim (\spanof_{s \in S} \{A_{i,s}\}) = n-1
    \end{align}
    by comparing inner products for both $i = j$ and $i \neq j$. So for $1 \leq i \leq t$, the $\spanof_{s \in S} \{A_{i,s}\}$ form mutually orthogonal subspaces of $V$. Therefore,
    \begin{align}
        \sum_{i=1}^{t} \dim(\spanof_{s \in S} \{A_{i,s}\}) \leq \dim V = (n-1)^{2} \implies t(n-1) \leq (n-1)^{2}.
    \end{align}
    We then have $\ip{\sum_{s \in S} \alpha_{s} A_{i,s}, \sum_{s \in S} \alpha_{s} A_{i,s'}} = n\alpha_{s'} - \sum_{s \in S} \alpha_{s} = 0$ which forces $\alpha_{s} = \alpha_{s'}$ for all $s,s' \in S$.
\end{proof}

\begin{theorem}
    If $q$ is a prime power then there exists a projective plane of order $q$.
\end{theorem}

\subsection{Projective Planes and Fields}

We move our discussion on to projective planes.

\begin{definition}
    A \eax{finite projective plane} of order $n$ is an incidence structure of points and lines satisfying
    \begin{enumerate}
        \item any two distinct points lie on exactly one line,
        \item any two distinct lines meet at exactly one points,
        \item there exist four points with no three collinear.
    \end{enumerate}
\end{definition}

\begin{lemma}
    Each line in a finite projective plane contains the same number of points $(n+1)$. Each point is on $(n+1)$ lines. The total number of points is equal to the total number of lines $n^{2}+n+1$.
\end{lemma}
\begin{proof}
    There is a one-to-one correspondence between lines passing through $p_{2}$ and points on $l$ other than $p_{1}$. The number of lines passing through $p_{2}$ equals the number of lines passing through $p_{3}$. There is a one-to-one correspondence between points on $l_{1}$, lines passing through $p$, and points on $l_{2}$. The number of points on $l_{1}$ equals the number of points on $l_{2}$, or $n+1$. The total number of points is $(n+1)^{2}-n = (n+1)n + 1$.
\end{proof}

\begin{theorem}
    There exists a projective plane of order $n$ if and only if there exists a set of $n-1$ mutually orthogonal latin squares of order $n$.
\end{theorem}


\begin{proof}
    We prove the converse implication. Choose a line $\ell_{\infty}$ and label the points on it as $p_{0},\ldots,p_{n}$. The lines through $p_{0}$, except $\ell_{\infty}$, are $R_{1},\ldots,R_{n}$. The lines through $p_{1}$, except $\ell_{\infty}$, are $C_{1},\ldots,C_{n}$. For a point $p \in P\setminus \ell_{\infty}$, $R_{i} \cap C_{j} = p$ uniquely determines via $(i,j)$. If we use $p_{2}$ to determine the symbol set $\{s_{1},\ldots,s_{n}\}$, then define latin squares as $L(P_{2})(R_{i},C_{j})$ to be the line passing through $R_{i} \cap C_{j}$ and $p_{2}$. Note that $L(p_{2})(R_{i},C_{j}) \neq L(p_{2})(R_{i'},C_{j'})$ for $(i,j) \neq (i',j')$ since $p_{2},p_{ij},p_{i'j'}$ are not collinear. So $L(p_{2})$ is a latin square. Similarly, we can define $L(p_{3}),\ldots,L(p_{n})$. To show that they are mutually orthogonal, we show that if $(i,j) \neq (i',j')$, then
    \begin{align}
        (L(p_{2})(R_{i},C_{j}), L(p_{3})(R_{i},C_{j})) \neq (L(p_{2})(R_{i'},C_{j'}), L(p_{3})(R_{i'},C_{j'})).
    \end{align}
    Case I, where $i \neq i'$ (the case $j \neq j'$ is similar). If $L(p_{2})(R_{i},C_{j}) = L(p_{2})(R_{i'},C_{j'})$, then $p_{2},R_{i} \cap C_{j},R_{i'} \cap C_{j'}$ are collinear. So $L(p_{3})(R_{i},C_{j}) \neq L(p_{3})(R_{i'},C_{j'})$. Case II, where $i = i'$ and $j = j'$. This is trivial.

    For the forward implication, let $L_{1},\ldots,L_{n-1}$ be mutually orthogonal latin squares of order $n$ on the symbol set $\{s_{1},\ldots,s_{n}\}$. Define $P' = \{(r,c) \mid r,c \in \{1,2,\ldots,n\}\}$ (as our set of points), and $L_{k,s} = \{(r,c) \mid L_{k}(r,c) = s\}$ (as our set of lines). Note that $\bigcup_{i=1}^{n} L_{k,s_{i}} = [n] \times [n]$ for each $k$. The lines $L_{k,s_{i}}$ are, thus, 'parallel' in the sense that they partition $P'$. For $1 \leq k \leq n-1$, we have a class of parallel lines. Thus we have $n^{2} - n$ lines so far. Add $n$ more lines $R_{1},\ldots,R_{n}$ where $R_{i} = \{(i,j) \mid j \in [n]\}$, and $n$ more lines $C_{1},\ldots,C_{n}$ where $C_{j} = \{(i,j) \mid i \in [n]\}$. This gives $n^{2} + n$ lines. Each class of parallel lines will be associated with a point at infinity. Let $\cF_{k}$ denote the class of parallel lines corresponding to $L_{k}$. Then $\cF_{k} \cap \cF_{k'} = \emptyset$ for $k \neq k'$. Thus we have $(n+1)$ classes $\cF_{1},\ldots,\cF_{n-1},R,C$ and $(n+1)$ points $P = P' \cup \{n+1 \text{ points}\}$. We now verify the projective plane axioms.
    
    
    Note that $L_{k,s} \cap L_{k',s'} = \{(r,c)\}$ for some unique $(r,c)$ since $L_{k}$ and $L_{k'}$ are orthogonal. Also, $R_{i} \cap C_{j} = \{(i,j)\}$ for all $i,j$. Moreover, $L_{k,s} \cap R_{i} = \{(i,c)\}$ for some unique $c$, and $L_{k,s} \cap C_{j} = \{(r,j)\}$ for some unique $r$. Finally, $R_{i} \cap R_{i'} = \emptyset$ for $i \neq i'$, and $C_{j} \cap C_{j'} = \emptyset$ for $j \neq j'$. Thus any two distinct lines meet at exactly one point.
\end{proof}


Let $\K$ be a finite field, with $\Z \hookrightarrow \K$. Note that since this field is finite, for all $a \in \K$ there exists an integer $k$ such that $a = ka$ or $(k-1)a = 0$. The \eax{characteristic} of $\K$ is the smallest $n \in \N$ such that $na = 0$ for all $a \in \K$. The characteristic is always a prime.

If $na = 0$ for $a \neq 0$, then $na a^{-1} = (a + a + \cdots + a) a^{-1} = (1 + 1 + \cdots + 1) = 0$. If we let $n = pq$, then $p(qa) = 0$ for all $a \in \K$ implies either $qa = 0$ for all $a \in \K$ or $p 1 = 0$. In either case, a factor of $n$ is a `smaller' characteristic. This process only ends if $n$ is prime.

\begin{proposition}
    The order of a finite field is a prime power.
\end{proposition}
\begin{proof}
    Let $p$ be the characteristic of $\K$ and $q$ be another prime factor of $\# \K$. Then $qa = 0$ for some $a \neq 0$. By Cauchy's theorem, $pa = 0$ for some $a \neq 0$. So $p = q$. Thus the order of $\K$ is $p^{k}$ for some $k \in \N$.
\end{proof}

\section{Designs}
\textit{October 23rd.}

\begin{theorem}[\eax{Fisher's inequality}]
    Let $A_{1},\ldots,A_{m} \subseteq \cP([n])$ be distinct subsets such that $\#(A_{i} \cap A_{j}) = k$ for all $i \neq j$. Then $m \leq n$.
\end{theorem}

\begin{proof}
    The idea is to build an incidence vector for $A_{i}$ as follows: define $v_{A_{i}} \in \R^{n}$ such that $v_{A_{i}}(j) = 1$ if $j \in A_{i}$ and $0$ otherwise. The inner product $\ip{v_{A_{i}},v_{A_{j}}} = \#(A_{i} \cap A_{j}) = k$ for $i \neq j$. We claim that the set $\{v_{A_{1}},\ldots,v_{A_{m}}\}$ is linearly independent. Suppose; then there exist $\alpha_{1},\ldots,\alpha_{m} \in \R$ not all zero such that $\sum_{i=1}^{m} \alpha_{i} v_{A_{i}} = 0$. Taking inner product with itself gives
    \begin{align}
        \ip{\sum_{i=1}^{m} \alpha_{i} v_{A_{i}},\sum_{j=1}^{m} \alpha_{j} v_{A_{j}}} = \sum_{i=1}^{m} \alpha_{i}^{2} \# A_{i} + \sum_{1 \leq i < j \leq n} 2 \alpha_{i} \alpha_{j} k = \sum_{i=1}^{m} \alpha_{i}^{2} (\# A_{i} - k) + k(\sum_{i=1}^{m} \alpha_{i})^{2} = 0
    \end{align}
    If $\# A_{i} \gneq k$ for all $i$, then $\alpha_{i} = 0$ for all $i$, a contradiction. Now suppose $\# A_{1} = k$. Then there cannot exist another $A_{i}$ such that $\# A_{i} = k$ since $A_{1} \cap A_{i} = k$ implies $A_{1} = A_{i}$. Thus $\# A_{i} > k$ for all $i \neq 1$. Then from the above equation, we have $\alpha_{i} = 0$ for all $i \neq 1$. So $\alpha_{1} v_{A_{1}} = 0$ implies $\alpha_{1} = 0$, a contradiction. Thus the set $\{v_{A_{1}},\ldots,v_{A_{m}}\}$ is linearly independent, and so $m \leq n$.
\end{proof}

We discuss this problem: suppose $A \in M_{n}(\R)$ with entries $\abs{a_{ij}} \leq 1$ for all $i,j$. What is the maximum possible value of $\det(A)$? If all the columns of $A$ are orthogonal, that would be good. Since eigenvalues have to satisfy $Ax = \lambda x$, we have $\abs{\lambda} \leq n$ and we have a simple bound
\begin{align}
    \abs{\det(A)} = \abs{\prod_{i=1}^{n} \lambda_{i}} \leq n^{n}.
\end{align}
A better bound is
\begin{align}
    \abs{\det(A)} \leq \abs{\sum_{\sigma \in S_{n}} \mathrm{sgn}(\sigma) a_{1,\sigma(1)} a_{2,\sigma(2)} \cdots a_{n,\sigma(n)}} \leq n! < n^{n}.
\end{align}
The Hadamard inequality for general matrices states that if the columns of $A$ are $c_{1},c_{2},\ldots,c_{n} \in \R^{n}$, then
\begin{align}
    \abs{\det(A)} \leq \norm{c_{1}}_{2} \norm{c_{2}}_{2} \cdots \norm{c_{n}}_{2} \leq n^{n/2} < n! < n^{n}.
\end{align}
with equality if and only if there exists a matrix $H$ with all entries $\pm 1$ such that $HH^{t} = H^{t} H = nI_{n}$. This inequality can be shown via $A^{t}A$ being positive definite.

\begin{definition}
    A matrix $H \in M_{n}(\R)$ is said to be a \eax{Hadamard matrix} if all its entries are $\pm 1$ and $\frac{1}{\sqrt{n}} H$ is orthogonal.
\end{definition}

For $n = 1$, $H = \begin{pmatrix}
    1
\end{pmatrix}$ is a Hadamard matrix. For $n = 2$, $H = \begin{pmatrix}
    1 & 1 \\
    1 & -1
\end{pmatrix}$ works. For $n$ odd, we may assume the first row is all $1$s. Then the inner product of the first row with second row $(x_{1},\ldots,x_{n})$ gives $x_{1} + x_{2} + \cdots + x_{n} = 0$, which is impossible since $n$ is odd. Thus no Hadamard matrix exists for odd $n > 1$. Note that if $H$ is a Hadamard matrix, then so is the block matrix $\begin{pmatrix}
    H & H \\
    H & -H
\end{pmatrix}$ (verify). Thus if a Hadamard matrix exists for $n$, then one exists for $2n$. This gives us Hadamard matrices for all powers of $2$. This is known as \eax{Sylvester's construction}. 

For a general Hadamard matrix, let the first row be all $1$s. Let $(x_{1},\ldots,x_{n})$ and $(y_{1},\ldots,y_{n})$ be the second and third rows respectively. Then the inner product of these two rows gives $x_{1}y_{1} + x_{2}y_{2} + \cdots + x_{n}y_{n} = 0$. The inner product with the first row gives $x_{1} + x_{2} + \cdots + x_{n} = 0$ and $y_{1} + y_{2} + \cdots + y_{n} = 0$. Let $I_{1} = \{i \mid x_{i} = 1, y_{i} = 1\}$, $I_{2} = \{i \mid x_{i} = 1, y_{i} = -1\}$, $I_{3} = \{i \mid x_{i} = -1, y_{i} = 1\}$, and $I_{4} = \{i \mid x_{i} = -1, y_{i} = -1\}$. We have
\begin{align}
    \# I_{1} + \# I_{2} + \# I_{3} + \# I_{4} &= n, \\
    \# I_{1} + \# I_{2} - \# I_{3} - \# I_{4} &= 0, \\
    \# I_{1} - \# I_{2} + \# I_{3} - \# I_{4} &= 0, \\
    \# I_{1} - \# I_{2} - \# I_{3} + \# I_{4} &= 0.
\end{align}
Solving, we get $\# I_{1} = \# I_{2} = \# I_{3} = \# I_{4} = n/4$. Thus $n$ must be divisible by $4$. This gives us the \eax{Hadamard conjecture}: for all $n$ divisible by $4$, there exists a Hadamard matrix of order $n$.

Let $q$ be a prime power congruent to $3 \bmod 4$. Recall that in the field $\F_{q}$, half of the non-zero elements are quadratic residues (or squares), and half are quadratic non-residues (or non-squares); in particular, $+1$ is a square and $-1$ is a non-square. 

The \emph{quadratic character} of $\F_{q}$ is the function $\chi$ defined by
\begin{align}
\chi(x) =
\begin{cases}
0, & \text{if } x = 0, \\
+1, & \text{if } x \text{ is a quadratic residue}, \\
-1, & \text{if } x \text{ is a quadratic non-residue}.
\end{cases}
\end{align}

Now let $A$ be the matrix whose rows and columns are indexed by elements of $\F_{q}$, and whose $(x, y)$ entry is given by $a_{xy} = \chi(y - x)$. The matrix $A$ is skew-symmetric, with zero diagonal and $\pm 1$ elsewhere, and satisfies the equation $A^{2} = J - qI$, where $J$ is the all-ones matrix and $I$ is the identity matrix. Now, if we replace the diagonal zeros of $A$ by $-1$s and border $A$ with a row and column of $+1$s, we obtain a Hadamard matrix of order $q + 1$, called a \eax{Paley matrix}.


We now consider complex Hadamard matrices of order $n$. Here, the entries come from the unit circle $S^{1}$. (The definition changes to $\frac{1}{\sqrt{n}} H$ being unitary). For $n = 3$, we already have a matrix as
\begin{align}
    \begin{pmatrix}
        1 & 1 & 1 \\
        1 & \omega & \omega^{2} \\
        1 & \omega^{2} & \omega
    \end{pmatrix}.
\end{align}
In fact, for any $n$ we can construct a Hadamard matrix with entries $a_{jk} = \exp(2\pi i (j-1)(k-1)/n)$ for $1 \leq j,k \leq n$. Let us look at the inner product of two rows $R_{j}$ and $R_{j'}$ in such a matrix:
\begin{align}
    \ip{R_{j},R_{j'}} = \sum_{k=1}^{n} a_{jk} \overline{a}_{j'k} = \sum_{k=1}^{n} \exp(2\pi i(j-j')(k-1)/n).
\end{align}
This summation is $n$ for $j = j'$. For $j \neq j'$, this summation is 0. Thus, $H = (a_{jk})_{n \times n}$ is a complex Hadamard matrix. $\frac{1}{\sqrt{n}} H_{n}$, for a $n \times n$ complex Hadamard matrix $H_{n}$, is a unitary matrix. Applying it to a vector $v \in \C^{n}$ gives a new vector $\hat{v} = \frac{1}{\sqrt{n}} H_{n} v$. This $\hat{v}$ is known as the \eax{discrete Fourier transform} of $v$. In this sense, complex Hadamard matrices are more analytic, and real Hadamard matrices are more combinatorial.

\begin{definition}
    Let $v,k,t,\lambda \in \Z_{\geq 0}$ be such that $v \geq k \geq t \geq 0$ and $\lambda \geq 1$. A \eax{$t$-$(v,k,\lambda)$ design} consists of
    \begin{enumerate}
        \item $v$ points forming the set $\cP$,
        \item a collection of distinct $k$-subsets $\cB$ of $\cP$ (called \eax{blocks}),
        \item such that every $t$-subset of $\cP$ is contained in exactly $\lambda$ blocks.
    \end{enumerate}
    In other words, $\# \cP = v$, $\# B = k$ for all $B \in \cB$, and for any $T \subseteq \cP$ with $\# T = t$, we have $\# \{B \in \cB \mid T \subseteq B\} = \lambda$.
\end{definition}

If $t = 2$, then we simply call it a $(v,k,\lambda)$ design. For the case $\lambda = 1$, a $t-(v,k,1)$ design is called a \eax{Steiner system}, denoted by $S(t,k,v)$. If $(\cB = ) b = v$, then the design is called a \eax{symmetric design}.

\begin{example}
    Suppose $q$ is a prime power. Then $S(2,q+1,q^{2}+q+1)$ is a design with $q^{2}+q+1$ points and blocks of size $q+1$. Moreover, for any two points, there is a unique block containing them. This design arises from the finite projective plane of order $q$. Moreover, this is a symmetric design since the number of blocks is equal to the number of points.
\end{example}

One can show that $b = \# \cB$ satisfies
\begin{align}
    b = \frac{\lambda \binom{v}{t}}{\binom{k}{t}}
\end{align}
in a $t-(v,k,\lambda)$ design, via double counting.